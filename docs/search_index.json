[["index.html", "MKT 4320: Marketing Analytics Lecture Materials Introduction", " MKT 4320: Marketing Analytics Lecture Materials Jeffrey Meyer 2022-01-25 Introduction This web-book will serve as your copies of the lecture slides. Because the software used in this class is R and RStudio, traditional lecture slides for students are not useful, because I want to provide the code that generated the output seen on the slides. Each topic covered is a chapter in this web-book and will begin with a motivation section, followed by the R packages and datasets used in that topic. The sections after that will provide an web version of the material on the lecture slides used in class. "],["examining-and-summarizing-data.html", "Topic 1 Examining and Summarizing Data 1.1 Motivation 1.2 R Packages and Datasets for Topic 1 1.3 Describing Data 1.4 Suggested Readings 1.5 R Code", " Topic 1 Examining and Summarizing Data 1.1 Motivation After data preparation, examining and summarizing the data provides the analyst with a feel for the data Distributions of variables Relationships between variables Missing observations Coding of variables 1.2 R Packages and Datasets for Topic 1 library(ggplot2) # Advanced graphing capabilities library(dplyr) # Easier programming library(tidyr) # Easier programming library(scales) # Control appearance of axes and legend labels library(questionr) # Easier frequency tables library(htmlTable) # Better HTML Tables library(mosaic) # Statistical functions library(sjPlot) # Easily create cross-tabs load(&quot;Topic01/airlinesat.rdata&quot;) Download airlinesat.rdata 1.3 Describing Data How we examine and summarize data depends on: Type of data Nominal Ordinal Continuous Number of variables Univariate Bivariate 1.3.1 Univariate - Graphs and Tables 1.3.1.1 Bar Chart Primarily for nominal/ordinal data Displays each categorys Frequency (usually) Centrality Dispersion Figure 1.1: Bar Chart R Code 1.3.1.2 Histogram Non-overlapping categories of equal width from continuous data Shows frequency in each category Used to examine distribution of variable Figure 1.2: Historgram 1 R Code Figure 1.3: Histogram 2 R Code 1.3.1.3 Box Plot Displays distribution of continuous data Conveys dispersion information Wider box = More dispersion Can help identify potential outliers How to interpret: Box in middle is the Interquartile Range Q3 (75th percentile) - Q1 (25th percentile) Line in middle is the median Upper/lower lines are upper/lower adjacent values Upper adjacent value is the largest observation that is smaller than Q3 + 1.5*IQR Lower adject value is the smallest observatoin that is larger than Q1 - 1.5*IQR Any observations above (below) the upper (lower) adjacent value are plotted separately, and could be outliers Figure 1.4: Box Plot R Code 1.3.1.4 Frequency Table Displays counts and percentages for categorical variables Similar to bar chart, but in table form Table 1.1: Frequency Table n % val% %cum val%cum Blue 677 63.6 63.6 63.6 63.6 Gold 143 13.4 13.4 77 77 Silver 245 23 23 100 100 Total 1065 100 100 100 100 R Code 1.3.2 Univariate - Statistics 1.3.2.1 Measures of Centrality Values of a typical or average score Mean is the sum of all observations divided by the number of observations Only appropriate for continuous data Median separates highest and lowers 50% of observations Cannot be used on categorical data Table 1.2: Measures of Centrality and Dispersion age s1 Min. : 19.00 Min. : 1.00 1st Qu.: 42.00 1st Qu.: 46.00 Median : 50.00 Median : 58.00 Mean : 50.42 Mean : 60.91 3rd Qu.: 58.00 3rd Qu.: 84.00 Max. :101.00 Max. :100.00 NAs :27 R Code 1.3.2.2 Measures of Dispersion Provide info about variability in the data Range is the highest minus the lowest observation Simple, but includes extreme values Not appropriate for categorical data Interquartile Range (IQR) is Q3 (75th percentile) - Q1 (25th percentile) Used in the Box Plot Not appropriate for categorical data Standard Deviation Given by Equation 1.1 below \\[\\begin{equation} s = \\sqrt{\\frac{\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}}{n-1}} \\tag{1.1} \\end{equation}\\] Only appropriate with continuous data Table 1.3 Measures of Centrality and Dispersion For (A) Age and (B) Airline gets me there on time (Satisfaction) (A) min Q1 median Q3 max mean sd n missing 19.00 42.00 50.00 58.00 101.00 50.42 12.27 1065.00 0.00 (B) min Q1 median Q3 max mean sd n missing 1.00 46.00 58.00 84.00 100.00 60.91 26.02 1038.00 27.00 R Code 1.3.3 Bivariate - Graphs and Tables 1.3.3.1 Scatterplots Graphically shows how two continuous variables are related If dots appear in to follow a line, variables are likely related (see Figure 1.5) If dots appear random, variables are likely not related (see Figure 1.6) Not appropriate for categorical data (see Figure 1.7) Figure 1.5: Scatterplot 1 with Fitted Line R Code Figure 1.6: Scatterplot 2 without Fitted Line R Code Figure 1.7: Scatterplot of Categorical Data R Code 1.3.3.2 Crosstabs Show if and how two categorical variables are related Common to put DV in rows and IV in columns Can ask for \\(\\chi^2\\) to test if for significant association Can also view it visually with a stacked bar chart (see Figure 1.8) Percentages represent column percentages Can also view it visually with separate bars for each category (see Figure 1.9) Bar height is percent of total Table 1.4: Crosstab flight_purpose flight_type Total Domestic International Business 33059.1 % 19538.5 % 52549.3 % Leisure 22840.9 % 31261.5 % 54050.7 % Total 558100 % 507100 % 1065100 % 2=44.619 · df=1 · =0.207 · p=0.000 R Code Figure 1.8: Stacked Bar Chart R Code Figure 1.9: Side-by-Side Bar Chart R Code 1.3.3.3 Box Plot Displays distribution of continuous data across classes of a categorical variable Figure 1.10: Box Plot by Category R Code 1.3.3.4 Bar Chart Displays mean (or some other value) of continuous data across classes of a categorical variable Figure 1.11: Bar Chart by Category R Code 1.3.4 Statistics 1.3.4.1 Correlation Provides a measure of linear association between two continuous variables Given by Equation 1.2 below \\[\\begin{equation} r = \\frac{\\sum_{i=1}^{n}{(x_i - \\bar{x})(y_i-\\bar{y})}}{(n-1)s_xs_y} \\tag{1.2} \\end{equation}\\] \\(-1 \\le r \\le 1\\) Table 1.5: Correlation Matrix age nflights e7 s11 age nflights -0.116*** e7 -0.034 -0.063* s11 0.170*** -0.106*** 0.240*** Computed correlation used pearson-method with pairwise-deletion. R Code 1.4 Suggested Readings R for Marketing Research and Analytics. 2nd Edition (2019). Chapman, Chris; McDonnel Feit, Elea BGSU Library Link:http://maurice.bgsu.edu/record=b4966554~S9 eBook through BGSU Library:https://link-springer-com.ezproxy.bgsu.edu/book/10.1007%2F978-3-030-14316-9 Chapter 3: Describing Data Chapter 4: Relationships Between Continuous Variables Chapter 5: Tables and Visualization OpenIntro Statistics. 4th Edition (2019). Diez, David; Cetinkaya-Rundel, Mine; Barr, Christopher D. Available at OpenIntro.org:https://www.openintro.org/book/os/ Summarizing Data Multivariate Data Analysis. Hair, Joseph F.; Black, William C.; Babin, Barry J.; Anderson, Rolph E. 7th Edition: Search for multivariate data analysis 7th edition hair Graphical Examination of the Data (pp. 34-40) 5th Edition: Course reserves Graphical Examination of the Data (pp. 40-46) 1.5 R Code 1.5.1 Table1.1 See Table 1.1 # Create frequency table using questionr::freq and pass result # htmlTable freq(airlinesat$status, cum=TRUE, total=TRUE) %&gt;% htmlTable() 1.5.2 Table1.2 See Table 1.2 airlinesat %&gt;% # Dataset # Select variables; Use &#39;dplyr::&#39; before &#39;select&#39; to avoid conflicts dplyr::select(age, s1) %&gt;% # Request summary statistics summary() %&gt;% # Create htmlTable htmlTable() 1.5.3 Table1.3 See Table 1.3 # Request htmlTable for summary statistics with rounding two 2 digits # for &#39;age&#39; htmlTable(txtRound(favstats(airlinesat$age),2), caption=&quot;Age&quot;) # Request htmlTable for summary statistics with rounding two 2 digits # for &#39;s1&#39; htmlTable(txtRound(favstats(airlinesat$s1),2), caption=&quot;Airline Gets me there on time (Satisfaction)&quot;) 1.5.4 Table1.4 See Table 1.4 # Use package sjPlot to easily create cross-tab # Note: sjPlot::tab_xtab not available in virtual environment tab_xtab(var.row=airlinesat$flight_purpose, # Set row variable var.col=airlinesat$flight_type, # Set column variable show.col.prc=TRUE) # Request column percentages 1.5.5 Table1.5 See Table 1.5 # Create dataframe of variables to include corrvars &lt;- airlinesat %&gt;% select(age, nflights, e7, s11) # Use package sjPlot to easily create correlation matrix # Note: sjPlot not available in virtual environment tab_corr(corrvars, na.deletion = &quot;pairwise&quot;, # Delete obs if either variable is missing corr.method = &quot;pearson&quot;, # Choose Pearson correlation coefficient show.p = TRUE, # Show asterisks for significant correlations digits = 3, # Show three decimal points triangle = &quot;lower&quot;, # Show only lower triangle fade.ns=FALSE) # Do not fade insignficant correlations # Note: For vitual environment, use package Hmisc to produce separate # tables for correlation coefficients and p-values htmlTable(txtRound(rcorr(as.matrix(corrvars))[[&quot;r&quot;]],3)) htmlTable(txtRound(rcorr(as.matrix(corrvars))[[&quot;P&quot;]],3)) 1.5.6 Figure 1.1 See Figure 1.1 airlinesat %&gt;% # Dataset # Groups dataset by variable &#39;status&#39; group_by(status) %&gt;% # Creates new variable &#39;n&#39; equal to number of obs for each status summarize(n=n()) %&gt;% # Creates new variable &#39;prop&#39; equal to proportion of same for each status mutate(prop=n/sum(n)) %&gt;% # Begins plot with &#39;status&#39; on x axis and &#39;prop&#39; on y axis ggplot(aes(x=status, y=prop)) + # Requests bar chart as the geom function, using actual value # of variable &#39;prop&#39; as the statistic to plot geom_bar(stat=&quot;identity&quot;) + # Adds data labels to the end of the bars geom_text(aes(label=sprintf(&quot;%1.1f%%&quot;, prop*100)), vjust=1.5, color=&quot;white&quot;) + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Status&quot;,y=&quot;Proportion&quot;) 1.5.7 Figure 1.2 See Figure 1.2 airlinesat %&gt;% # Dataset # Begins plot with &#39;age&#39; as variable to plot on x axis ggplot(aes(x=age)) + # Requests histogram as the geom function, with binwidth of 2, # y axis to represent density, and outline/color of bars geom_histogram(binwidth=2, aes(y=..density..), color=&quot;black&quot;, fill=&quot;tan&quot;) + # Creates normal curve overlay stat_function(fun=function(x) dnorm(x, mean=mean(airlinesat$age),sd=sd(airlinesat$age))) + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Age&quot;, y=&quot;Density&quot;) 1.5.8 Figure 1.3 See Figure 1.3 airlinesat %&gt;% # Dataset # Drops ros with missing values for variable &#39;s1&#39; drop_na(s1) %&gt;% # Begins plot with &#39;s1&#39; as variable to plot on x axis ggplot(aes(x=s1)) + # Requests histogram as the geom function, with binwidth of 2, # y axis to represent density, and outline/color of bars geom_histogram(binwidth=2, aes(y=..density..), color=&quot;black&quot;, fill=&quot;tan&quot;) + # Creates normal curve overlay stat_function(fun=function(x) dnorm(x, mean=mean(airlinesat$s1, na.rm=TRUE), sd=sd(airlinesat$s1, na.rm=TRUE))) + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Airline gets me there on time (Satisfaction)&quot;, y=&quot;Density&quot;) 1.5.9 Figure 1.4 See Figure 1.4 airlinesat %&gt;% # Dataset # Begins plot with no grouping variable and age as continuous variable ggplot(aes(x=&quot;&quot;, y=age)) + # Request boxplot as the geom function geom_boxplot() + # Adds the whiskers to the boxplot stat_boxplot(geom=&#39;errorbar&#39;) + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;&quot;, y=&quot;Age&quot;) 1.5.10 Figure 1.5 See Figure 1.5 airlinesat %&gt;% # Passes dataset to ggplot # Begins plot with x (s18) and y (s17) variables ggplot(aes(x=s18, y=s17)) + # Requests scatter plot geom_point() + # Requests linear regression fitted line without confidence interval geom_smooth(method=&quot;lm&quot;, se=FALSE) + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Employees are service-oriented (s18)&quot;, y=&quot;Employees are friendly (s17)&quot;) 1.5.11 Figure 1.6 See Figure 1.6 airlinesat %&gt;% # Passes dataset to ggplot # Begins plot with x (age) and y (s11) variables ggplot(aes(x=age, y=s11)) + # Requests scatter plot geom_point() + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Age&quot;, y=&quot;Aircraft interior is well maintained (s11)&quot;) 1.5.12 Figure 1.7 See Figure 1.7 airlinesat %&gt;% # Passes dataset to ggplot # Begins plot with x (age) and y (s11) variables ggplot(aes(x=flight_type, y=flight_purpose)) + # Requests scatter plot geom_point() + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Flight Type&quot;, y=&quot;Flight Purpose&quot;) 1.5.13 Figure 1.8 See Figure 1.8 airlinesat %&gt;% # Dataset # Groups dataset by crosstab variables group_by(flight_type, flight_purpose) %&gt;% # Creates new variable &#39;n&#39; for count of observations in each cell summarise(n=n()) %&gt;% # Creates column percentages mutate(prop=n/sum(n)) %&gt;% # Begins plot with &#39;flight_type&#39; on x, &#39;prop&#39; on y, and color of the # fill in the bars based on&#39;flight_purpose&#39; ggplot(aes(x=flight_type, y=prop, fill=flight_purpose)) + # Requests bar chart as the geom function, positioning the # location of the bars based on the fill variable geom_bar(position=&quot;fill&quot;, stat=&quot;identity&quot;) + # Labels y-axis using percentages scale_y_continuous(labels=scales::label_percent()) + # Adds data labels to middle of bars geom_text(aes(label=sprintf(&quot;%1.1f%%&quot;, prop*100)), position=position_stack(vjust=0.5), color=&quot;white&quot;) + # Changes test size to be larger theme(text=element_text(size=15)) + # Adds axis and legend labels labs(x=&quot;Flight Type&quot;, y=&quot;Percent&quot;, fill=&quot;Flight Purpose&quot;) 1.5.14 Figure 1.9 See Figure 1.9 airlinesat %&gt;% # Dataset # Groups dataset by crosstab variables group_by(flight_type, flight_purpose) %&gt;% # Creates new variable &#39;n&#39; for count of observations in each cell, but # drops grouping structure to get total percentages summarise(n=n(), .groups=&quot;drop&quot;) %&gt;% # Creates total percentages mutate(prop=n/sum(n)) %&gt;% # Begins plot with &#39;flight_type&#39; on x, &#39;prop&#39; on y, and color of the # fill in the bars based on&#39;flight_purpose&#39; ggplot(aes(x=flight_type, y=prop, fill=flight_purpose)) + # Requests bar chart as the geom function, positioning the # location of the bars to be side-by-side (dodge) geom_bar(position=&quot;dodge&quot;, stat=&quot;identity&quot;) + # Labels y-axis using percentages scale_y_continuous(labels=scales::label_percent()) + # Adds data labels to end of bars geom_text(aes(label=sprintf(&quot;%1.1f%%&quot;, prop*100)), vjust=1.5, position=position_dodge(width=.9), color=&quot;white&quot;) + # Changes test size to be larger theme(text=element_text(size=15)) + # Adds axis and legend labels labs(x=&quot;Flight Type&quot;, y=&quot;Percent&quot;, fill=&quot;Flight Purpose&quot;) 1.5.15 Figure 1.10 See Figure 1.10 airlinesat %&gt;% # Dataset # Begins plot with &#39;status&#39; as grouping variable and # &#39;age&#39; as continuous variable ggplot(aes(x=status, y=age)) + # Request boxplot as the geom function geom_boxplot() + # Adds the whiskers to the boxplot stat_boxplot(geom=&#39;errorbar&#39;) + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Status&quot;, y=&quot;Age&quot;) 1.5.16 Figure 1.11 See Figure 1.11 airlinesat %&gt;% # Dataset # Groups dataset by &#39;crosstab variables&#39;status&#39; group_by(status) %&gt;% # Creates new variable &#39;mean&#39; for mean of &#39;age&#39; by &#39;status&#39; summarise(mean=mean(age)) %&gt;% # Begins plot with &#39;status&#39; on x, &#39;mean&#39; on y ggplot(aes(x=status, y=mean)) + # Requests bar chart as the geom function, plotting the actual # value (&#39;identity&#39;), and setting fill color to match status geom_bar(stat=&quot;identity&quot;, fill=c(&quot;midnightblue&quot;,&quot;gold&quot;,&quot;gray&quot;)) + # Sets number of breaks on y-axis scale_y_continuous(n.breaks=6) + # Adds data labels to outside end of bars geom_text(aes(label=sprintf(&quot;%.2f&quot;, mean)), vjust=-.5, position=position_dodge(width=.9), color=&quot;black&quot;) + # Changes test size to be larger theme(text=element_text(size=15)) + # Adds axis and legend labels labs(x=&quot;Status&quot;, y=&quot;Mean of Age&quot;) "],["linear-regression.html", "Topic 2 Linear Regression 2.1 Motivation 2.2 R Packages and Datasets for Topic 2 2.3 Understanding Regression Analysis 2.4 Conducting Linear Regression 2.5 Linear Regression Example 2.6 Categorical IVs 2.7 Categorical IVs Example 2.8 Suggested Readings 2.9 R Code", " Topic 2 Linear Regression 2.1 Motivation Regression allows marketers to: Understand relationships between a dependent variable and one or more independent variables Determine the relative strength of different independent variables Make predictions 2.2 R Packages and Datasets for Topic 2 library(ggplot2) # Advanced graphing capabilities library(tidyr) # Easier programming library(scales) # Control appearance of axes and legend labels library(htmlTable) # Better HTML Tables library(reshape2) # Easily convert wide data to long data library(GGally) # ggplot extension; for scatterplot matrix library(summarytools) # Summary statistics library(effects) # Help with linear predictions library(cowplot) # Arrange separate plots in a grid library(ggtext) # Annotate ggplots library(lubridate) # Easily work with dates library(jtools) # Concise regression results library(dplyr) # Easier programming library(broom) # Extract values from model load(&quot;Topic02/advtsales.rdata&quot;) load(&quot;Topic02/deptstoresales.rdata&quot;) Download advtsales.rdata Download deptstoresales.rdata 2.3 Understanding Regression Analysis Regression notation: \\(y = \\alpha + \\beta_kx_k+\\varepsilon\\) where \\(y\\) is the dependent variable (DV) \\(x_k\\) is the \\(k\\)th independent variable (IV) \\(\\alpha\\) is the constant or \\(y\\)-intercept \\(\\beta_k\\) is the regression coefficient for the \\(k\\)th IV \\(\\varepsilon\\) is the error term Objective Predict DV based on knowledge of the IV(s) Method OLS creates the best fitted line by minimizing the sum of the squared residuals OLS Minimizes Equation 2.1 below: \\[\\begin{equation} \\sum_{i=1}^{n}{(y_i - \\hat{y}_i)^2} \\tag{2.1} \\end{equation}\\] Note: The best fitted regression line is not always the line that best represents the data 2.4 Conducting Linear Regression 2.4.1 Check Data Requirements Continuous DV Must be measured on an interval or ratio scale For nominal scale, use logistic regression For ordinal scale, use ordinal regression 2.4.2 Model Specification Pick IVs based on Conceptual grounding Availability of data Including irrelevant IVs Reduces parsimony May mask effects of other IVs Makes testing significance less precise Excluding relevant IVs Seriously biases results Negatively affects interpretation 2.4.3 Model Estimation Estimately is typically done using OLS All statistical packages can conduct regression R lm(dv ~ iv1 + iv2 +  + ivk) Stata GUI Statistics &gt; Linear models and related &gt; Linear Regression Stata Command regress dv iv1 iv2  ivk SPSS GUI Analyze &gt; Regression &gt; Linear SPSS Syntax regression/dependent dv/enter iv1 iv2  ivk SAS proc reg; model dv = iv1 iv2  ivk; Minitab Stat &gt; Regression &gt; Regression 2.4.4 Model Interpetation Assessing Overall Model Fit How much variation in the DV is explained by the model Individual Independent Attributes Relationship between DV and each IV \\(H_0:\\beta_k=0\\) vs. \\(H_a:\\beta_k\\ne0\\) Interpret significant relationships Relative strength of IVs 2.4.5 Model Prediction Prediction is a key use of regression Estimate DV based on assumed values of IVs \\(\\hat{y} = \\hat{\\alpha} + \\hat{\\beta_k}x_k\\) where \\(\\hat{y}\\) is predicted value of \\(y\\) for assumed values of \\(x_k\\) and Regression provided estimates of \\(\\alpha\\) and \\(\\beta_k\\) 2.5 Linear Regression Example 2.5.1 Overview Advertising and Sales data for 200 firms DV: Sales (in millions), \\(sales\\) IVs: TV Advertising (in 000s), \\(ad\\_tv\\) Radio Advertising (in 000s), \\(ad\\_radio\\) Paper Advertising (in 000s), \\(ad\\_paper\\) Model: \\(sales=\\alpha+\\beta_1ad\\_tv+\\beta_2ad\\_radio+\\beta_3ad\\_paper\\) Goal: Understand the relationship between various advertising types and sales 2.5.2 Summarize Data 2.5.2.1 Univariate Summary Statistics Table 2.1: Summary Statistics N.Valid Mean Std.Dev Min Max ad_paper 200.00 30.55 21.78 0.30 114.00 ad_radio 200.00 23.26 14.85 0.00 49.60 ad_tv 200.00 147.04 85.85 0.70 296.40 sales 200.00 14.02 5.22 1.60 27.00 R Code 2.5.2.2 Scatterplot and Correlation Matrix Figure 2.1: Scatterplot Matrix with Correlation R Code 2.5.2.3 Box Plots Figure 2.2: Box Plots R Code 2.5.3 Results 2.5.3.1 R Output 2.5.3.1.1 Regression Results (Concise) Estimated regression equation: \\(\\hat{sales}=2.939+.046ad\\_tv+.189ad\\_radio-.001ad\\_paper\\) Table 2.2: Regression Results (Concise) F(3,196) 570.2707 R² 0.8972 Adj. R² 0.8956 Est. S.E. t val. p (Intercept) 2.9389 0.3119 9.4223 0.0000 ad_tv 0.0458 0.0014 32.8086 0.0000 ad_radio 0.1885 0.0086 21.8935 0.0000 ad_paper -0.0010 0.0059 -0.1767 0.8599 Standard errors: OLS R Code 2.5.3.1.2 Standard Results Table 2.3: Regression Results (Standard) Call: lm(formula = sales ~ ad_tv + ad_radio + ad_paper, data = advtsales) Residuals: Min 1Q Median 3Q Max -8.8277 -0.8908 0.2418 1.1893 2.8292 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.938889 0.311908 9.422 &lt;2e-16 *** ad_tv 0.045765 0.001395 32.809 &lt;2e-16 *** ad_radio 0.188530 0.008611 21.893 &lt;2e-16 *** ad_paper -0.001037 0.005871 -0.177 0.86 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.686 on 196 degrees of freedom Multiple R-squared: 0.8972, Adjusted R-squared: 0.8956 F-statistic: 570.3 on 3 and 196 DF, p-value: &lt; 2.2e-16 R Code 2.5.3.2 Assessing Overall Model Fit How much variation in the DV is explained by the model Use \\(R^2\\) to assess Use \\(\\text{Adjusted }R^2\\) to compare models Conclusion: Based on the \\(R^2\\), about \\(90\\%\\) of the variance in \\(sales\\) is explained by the model 2.5.3.3 Individual Independent Variables Relationship between DV and each IV \\(H_0:\\beta_k=0\\) vs. \\(H_a:\\beta_k\\ne0\\) Interpret significant relationships With a \\(p\\text{-value}&lt;0.001\\), \\(ad\\_tv\\) has a significant effect on sales. A one unit increase in \\(ad_tv\\) is predicted to increases \\(sales\\) by \\(.0457\\) units. With a \\(p\\text{-value}&lt;0.001\\), \\(ad\\_radio\\) has a significant effect on sales. A one unit increase in \\(ad_radio\\) is predicted to increases \\(sales\\) by \\(.1885\\) units. With a \\(p\\text{-value}=.860\\), \\(ad\\_paper\\) has no significant effect on \\(sales\\). Relative strength of IVs For relative strength, use standardized \\(\\beta_k\\)s A standardized \\(\\beta_k\\) is the effect of a single standard deviation change in the IV on the DV Higher absolute values are more important Conclusion: \\(ad\\_tv\\) is the biggest driver of sales Table 2.4: Standardized Beta Coefficients Std.Beta (Intercept) 0 ad_tv 0.7531 ad_radio 0.5365 ad_paper -0.0043 R Code Visualize each IV Sometimes it helps to visually examine the IVs for interprtation Plots show predicted DV at different levels of an IV, holding the other IVs constant at the mean value Figure 2.3: Margin Plots R Code 2.5.3.4 Model Prediction For simplicity, use only \\(ad\\_tv\\) and \\(ad\\_radio\\) Table 2.5: Coefficients Table for Reduced Model Est. S.E. t val. p (Intercept) 2.9211 0.2945 9.9192 0.0000 ad_tv 0.0458 0.0014 32.9087 0.0000 ad_radio 0.1880 0.0080 23.3824 0.0000 Standard errors: OLS R Code \\(\\hat{sales}=2.9211+.0458ad\\_tv+.1880ad\\_radio\\) Predict sales for $100K television advertising and $10K radio advertising \\(\\hat{sales}=2.9211+.0458(100)+.1880(10)=9.381= \\$9,381,000\\) Visually examine prediction at different levels of \\(ad\\_tv\\) and \\(ad\\_radio\\) Figure 2.4: Prediction Plots Figure 2.1: Prediction Plots R Code 2.6 Categorical IVs 2.6.1 Overview May want to represent a qualitative variable Gender of a buyer Success/Failure Region of the country Special situations But the IVs are supposed to be continuous Use dummy variables to indicate occurrence or nonoccurence of a particular attribute Coded as 1 (usually if true) or 0 (usually if false) Dummy variables can shift the intercept, the slope, or both Intercept Shifter Dummy is only its own term in the model \\(y=\\alpha+\\beta_1x+\\beta_2D\\) Slope Shifter Dummy is only an interaction with another IV \\(y=\\alpha+\\beta_1x+\\beta_2(x\\times D)\\) Intercept and Slope Shifter Dummy is own term and an interaction with IV \\(y=\\alpha+\\beta_1x+\\beta_2D+\\beta_3(x\\times D)\\) 2.6.2 Intercept Shifter \\(D=\\begin{cases}1\\text{ if true}\\\\0\\text{ if false}\\end{cases}\\) Model: \\(y=\\alpha+\\beta_1x+\\beta_2D\\) When \\(D=0\\): \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(0)\\\\&amp; = &amp; \\alpha + \\beta_1x\\end{array}\\) When \\(D=1\\): \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(1)\\\\&amp; = &amp; (\\alpha + \\beta_2)+\\beta_1x\\end{array}\\) 2.6.3 Slope Shifter \\(D=\\begin{cases}1\\text{ if true}\\\\0\\text{ if false}\\end{cases}\\) Model: \\(y=\\alpha+\\beta_1x+\\beta_2(x\\times D)\\) When \\(D=0\\): \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(x\\times 0)\\\\&amp; = &amp; \\alpha + \\beta_1x\\end{array}\\) When \\(D=1\\): \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(x\\times 1)\\\\&amp; = &amp; \\alpha + (\\beta_1+\\beta_2)x\\end{array}\\) Unusual to see only a slope shift 2.6.4 Intercept and Slope Shifter \\(D=\\begin{cases}1\\text{ if true}\\\\0\\text{ if false}\\end{cases}\\) Model: \\(y=\\alpha+\\beta_1x+\\beta_2D+\\beta_3(x\\times D)\\) When \\(D=0\\): \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(0)+\\beta_3(x\\times 0)\\\\&amp; = &amp; \\alpha + \\beta_1x\\end{array}\\) When \\(D=1\\): \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(1)+\\beta_3(x\\times 1)\\\\&amp; = &amp; (\\alpha + \\beta_2) + (\\beta_1 + \\beta_3)x\\end{array}\\) 2.6.5 Multiple Levels What if categorical IV has multiple levels (e.g., quarters)? Choose one level to be the base Create dummy variables for the other levels Levels must be mutually exclusive Dummy variables for four levels: Level 1, \\(L_1=\\begin{cases}1\\text{ if true}\\\\0\\text{ if false}\\end{cases}\\) Level 2, \\(L_2=\\begin{cases}1\\text{ if true}\\\\0\\text{ if false}\\end{cases}\\) Level 3, \\(L_3=\\begin{cases}1\\text{ if true}\\\\0\\text{ if false}\\end{cases}\\) Level 4, \\(L_4=\\text{base level; is true when }L_1=L_2=L_3=0\\) Model: \\(y=\\alpha + \\beta_1x + \\beta_2L_1+\\beta_3L_2+\\beta_4L_3\\) When Level 1: \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(1)+\\beta_3(0)+\\beta_4(0)\\\\&amp; = &amp; (\\alpha + \\beta_2) + \\beta_1x\\end{array}\\) When Level 2: \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(0)+\\beta_3(1)+\\beta_4(0)\\\\&amp; = &amp; (\\alpha + \\beta_3) + \\beta_1x\\end{array}\\) When Level 3: \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(0)+\\beta_3(0)+\\beta_4(1)\\\\&amp; = &amp; (\\alpha + \\beta_4) + \\beta_1x\\end{array}\\) When Level 4: \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(0)+\\beta_3(0)+\\beta_4(0)\\\\&amp; = &amp; \\alpha + \\beta_1x\\end{array}\\) 2.7 Categorical IVs Example 2.7.1 Overview Sales data for 28 department store locations across 47 weeks and 69 departments DV: Department Sales, \\(sales\\) IVs: Overall Store Size, \\(size\\) Week, \\(week\\) where 1 = \\(week\\) ending 11/11/11 Predict sales by department Believe that the holiday season (or quarter 4) will be a driver of sales for some departments Generate dummy variable: \\(q4=\\begin{cases}1\\text{ if }week\\text{ in Quarter 4}\\\\0\\text{ otherwise}\\end{cases}\\) 2.7.2 Intercept Shift \\(sales = \\alpha + \\beta_1size+\\beta_2q4\\) Results: \\(sales\\) are significantly lower in Q4 (see Table 2.6 and Figure 2.5) Table 2.6: Regression Results (Intercept Shfit) for One Department F(2,1090) 126.1302 R² 0.1879 Adj. R² 0.1864 Est. S.E. t val. p (Intercept) 9993.3892 1890.0317 5.2874 0.0000 size 0.0546 0.0107 5.1210 0.0000 q4 -15153.3589 1012.8597 -14.9610 0.0000 Standard errors: OLS R Code Figure 2.5: Margin Plot for Intercept Shifter R Code 2.7.3 Slope Shift \\(sales = \\alpha + \\beta_1size+\\beta_2(size\\times q4)\\) Results: \\(sales\\) as a function of \\(size\\) are significantly lower in Q4 (see Table 2.7 and Figure 2.6) Table 2.7: Regression Results (Slope Shift) for One Department F(2,1090) 128.2440 R² 0.1905 Adj. R² 0.1890 Est. S.E. t val. p (Intercept) 6568.3795 1870.6049 3.5114 0.0005 size 0.0742 0.0107 6.9309 0.0000 size:q4 -0.0872 0.0058 -15.0986 0.0000 Standard errors: OLS R Code Figure 2.6: Margin Plot for Slope Shifter R Code 2.7.4 Intercept and Slope Shift \\(sales = \\alpha + \\beta_1size+\\beta_2q4+\\beta_3(size\\times q4)\\) Results: \\(sales\\) as a function of \\(size\\) are significantly lower in Q4 (see Table 2.8 and Figures 2.7 and 2.8) Table 2.8: Regression Results (Intercept &amp; Slope Shift) for One Department F(3,1089) 86.0379 R² 0.1916 Adj. R² 0.1894 Est. S.E. t val. p (Intercept) 7811.3235 2126.7243 3.6729 0.0003 size 0.0673 0.0121 5.5712 0.0000 q4 -5482.4234 4466.5447 -1.2274 0.2199 size:q4 -0.0567 0.0255 -2.2229 0.0264 Standard errors: OLS R Code Figure 2.7: Margin Plot for Intercept and Slope Shifter Showing \\(y\\)-intercept R Code Figure 2.8: Margin Plot for Intercept and Slope Shifter R Code 2.7.5 Intercept Shift with Multiple Levels \\(sales=\\alpha+\\beta_1size+\\beta_2q1+\\beta_3q2+\\beta_4q3\\) Q4 is set as the base level Results: \\(sales\\) are significantly lower in Q4 than in each of the other three quarters (see Table 2.9 and Figures 2.9 and 2.10) Table 2.9: Regression Results (Intercept Shift for Multiple Levels) for One Department F(4,1088) 307.2426 R² 0.5304 Adj. R² 0.5287 Est. S.E. t val. p (Intercept) -5354.0323 1539.7359 -3.4772 0.0005 size 0.0558 0.0081 6.8680 0.0000 quarter1 7268.7033 904.6239 8.0351 0.0000 quarter2 31313.5153 963.9526 32.4845 0.0000 quarter3 10436.9137 917.0085 11.3815 0.0000 Standard errors: OLS R Code Figure 2.9: Margin Plot for Multiple Levels Showing \\(y\\)-intercept R Code Figure 2.10: Margin Plot for Multiple Levels R Code 2.8 Suggested Readings R for Marketing Research and Analytics. 2nd Edition (2019). Chapman, Chris; McDonnel Feit, Elea BGSU Library Link:http://maurice.bgsu.edu/record=b4966554~S9 eBook through BGSU Library:https://link-springer-com.ezproxy.bgsu.edu/book/10.1007%2F978-3-030-14316-9 Chapter 7 OpenIntro Statistics. 4th Edition (2019). Diez, David; Cetinkaya-Rundel, Mine; Barr, Christopher D. Available at OpenIntro.org:https://www.openintro.org/book/os/ Chapter 8: Introduction to linear regression Chapter 9: Multiple and logistic regression Multivariate Data Analysis. Hair, Joseph F.; Black, William C.; Babin, Barry J.; Anderson, Rolph E. 7th Edition: Search for multivariate data analysis 7th edition hair Chapter 4: Multiple Regression Analysis 5th Edition: Course reserves Chapter 4: Multiple Regression Analysis 2.9 R Code 2.9.1 Table 2.1 See Table 2.1 # Creates dataframe with only needed variables; Use &#39;dplyr::&#39; before # &#39;select&#39; to avoid conflict with other packages lrreg1 &lt;- advtsales %&gt;% dplyr::select(-id) # Creates vector of stats to request in summarytools::descr stats &lt;- c(&quot;n.valid&quot;, &quot;mean&quot;, &quot;sd&quot;, &quot;min&quot;, &quot;max&quot;) # Use package summarytools to easily create summary statistics table # Note: summarytools::descr not available in virtual environment # Request htmlTable for summary statistics with rounding two 2 digits setHtmlTableTheme(&quot;Google&quot;) # Creates more compact table htmlTable(txtRound(descr(lrreg1, stats=stats, transpose=TRUE),2)) # Note: For virtual environment, use package mosaic::favstats to produce # separate summary statistics for each variable htmlTable(txtRound(favstats(lrreg1$ad_paper),2), caption=&quot;ad_paper&quot;) htmlTable(txtRound(favstats(lrreg1$ad_radio),2), caption=&quot;ad_radio&quot;) htmlTable(txtRound(favstats(lrreg1$ad_tv),2), caption=&quot;ad_tv&quot;) htmlTable(txtRound(favstats(lrreg1$sales),2), caption=&quot;sales&quot;) 2.9.2 Table 2.2 See Table 2.2 # Run linear model and save as &#39;results&#39; results &lt;- lm(sales ~ ad_tv + ad_radio + ad_paper, data = advtsales) # Create &#39;concise&#39; results using package &#39;jtools&#39; # NOTE: &#39;jtools&#39; not available in virtual environment; use standard results summ(results, digits=4, model.info=FALSE) 2.9.3 Table 2.3 See Table 2.3 # Run linear model and save as &#39;results&#39; results &lt;- lm(sales ~ ad_tv + ad_radio + ad_paper, data = advtsales) # Displays results summary(results) 2.9.4 Table 2.4 See Table 2.4 # Function to calculate standardized beta coefficients lm_beta &lt;- function (MOD) { b &lt;- summary(MOD)$coef[-1, 1] sx &lt;- sapply(MOD$model[-1], sd) sy &lt;- sapply(MOD$model[1], sd) beta &lt;- b * sx/sy return(beta) } # Create table setHtmlTableTheme(&quot;Google&quot;, css.table=&quot;width: 50%;&quot;) htmlTable(txtRound(as.matrix(lm_beta(results)), 4, scientific=FALSE)) 2.9.5 Table 2.5 See Table 2.5 # Run linear model and save as &#39;results&#39; resultssig &lt;- lm(sales ~ ad_tv + ad_radio, data = advtsales) # Create &#39;concise&#39; results using package &#39;jtools&#39; # NOTE: &#39;jtools&#39; not available in virtual environment; use standard results summ(resultssig, digits=4, model.info=FALSE, model.fit=FALSE) 2.9.6 Table 2.6 See Table 2.6 # Add Variables to &#39;dssales&#39; dataframe dssales &lt;- dssales %&gt;% # Add week ending date (&#39;weekdate&#39;) using package &#39;lubridate&#39; mutate(weekdate=ymd(&quot;2011-11-05&quot;) + (dssales$week-1)*7) %&gt;% # Add quarter based on &#39;weekdate&#39; using package &#39;lubridate&#39; mutate(quarter = quarter(weekdate)) %&gt;% # Create dummy variable for quarter 4 mutate(q4=ifelse(quarter==4,1,0)) # Create new data frame with only department 16 dssales.16 &lt;- dssales %&gt;% filter(dept==16) # Run model with intercept shifter only mod.is &lt;- lm(sales~size + q4, data=dssales.16) # Show results using &#39;jtools&#39; package summ(mod.is, digits=4, model.info=FALSE) 2.9.7 Table 2.7 See Table 2.7 # Run model with slope shifter only; use &#39;:&#39; between interaction terms to # exclude main effect of q4 from model mod.ss &lt;- lm(sales~size+size:q4, data=dssales.16) # Show results using &#39;jtools&#39; package summ(mod.ss, digits=4, model.info=FALSE) 2.9.8 Table 2.8 See Table 2.8 # Run model with intercept and slope shifter; use &#39;*&#39; between interaction # terms to include interaction AND main effects mod.iss &lt;- lm(sales~size*q4, data=dssales.16) # Show results using &#39;jtools&#39; package summ(mod.iss, digits=4, model.info=FALSE) 2.9.9 Table 2.9 See Table 2.9 # Make &#39;quarter&#39; a factor variable so R will use dummy variables automatically dssales.16$quarter &lt;- factor(dssales.16$quarter) # Set base level of &#39;quarter&#39; to be 4 dssales.16$quarter &lt;- relevel(dssales.16$quarter, ref=4) # Run model with multiple dummies for quarter mod.mis &lt;- lm(sales~size+quarter, data=dssales.16) # Show results using &#39;jtools&#39; package summ(mod.mis, digits=4, model.info=FALSE) 2.9.10 Figure 2.1 See Figure 2.1 # Use package GGally::ggpairs to easily create combination correlation # and scatterplot matrix ggpairs(lrreg1, # Dataset lower=list(continuous= wrap(&quot;smooth&quot;, method=&quot;lm&quot;, se=FALSE, # Add fit line color=&quot;midnightblue&quot;)), # Set dot color diag=list(continuous=&quot;blankDiag&quot;)) # Set diagonals to be blank 2.9.11 Figure 2.2 See Figure 2.2 melt(lrreg1) %&gt;% # Use package &#39;reshape2&#39; to reshape the data for facet plot # Begins plot with each variable as a factor and # value of the variable to be plotted ggplot(aes(factor(variable), value)) + # Requests boxplot as geom function geom_boxplot() + # Adds the whiskers to the boxplot stat_boxplot(geom=&#39;errorbar&#39;) + # Creates a facet/matrix layout based on the variable facet_wrap(~variable, scale=&quot;free&quot;) + # Change text size theme(text=element_text(size=15)) + # Removes axis labels labs(x=&quot;&quot;, y=&quot;&quot;) 2.9.12 Figure 2.3 See Figure 2.3 # Want to predict &#39;sales&#39; for different levels of IVs # Because we often do this for other continuous variables, we first # create a df with mean values of all continuous variables at # different levels of the factor variable (if there are any) # NOTE: variable names must be EXACTLY the same as in model m.ivs &lt;- advtsales %&gt;% group_by() %&gt;% # NOTE: No factor variables for this model summarise(ad_tv=mean(ad_tv, na.rm=TRUE), ad_radio=mean(ad_radio, na.rm=TRUE), ad_paper=mean(ad_paper, na.rm=TRUE)) # Second, create a df of values to predict with, using &#39;merge&#39; # &#39;merge&#39; simply merges data frames tv.pred &lt;- merge(data.frame(ad_tv=seq(0, 300, 25)), # Variable of interest m.ivs[,c(&quot;ad_radio&quot;, &quot;ad_paper&quot;)]) # Mean levels of other variables # Third, use model results to predict DV for values in &#39;tv.pred&#39; data frame # Predicted values and confidence intervals get appended to data frame # New variables are called: # &#39;p$fit&#39; = linear prediction from model # &#39;p$lwr&#39; = lower confidence interval for prediction # &#39;p$upr&#39; = upper confidcent interval for prediction tv.pred$p &lt;- as.data.frame( predict.lm(results, # Object with model results tv.pred, # df of IV values to predict with interval=&quot;confidence&quot;)) # Request CIs # Last, use &#39;tv.pred&#39; for margin plot; save plot p1 &lt;- tv.pred %&gt;% ggplot(aes(x=ad_tv, # Age on x-axis y=p$fit)) + # &#39;sales&#39; prediction on y-axis geom_line(size=.5, color=&quot;red4&quot;) + # Draw predicted line geom_ribbon(aes(ymin=p$lwr, # Draws the confidence interval bands ymax=p$upr), alpha=0.2, fill=&quot;red4&quot;) + # Sets transparency level # Next two commands scale x and y axes scale_x_continuous(limits=c(0,300), expand=c(.025,.025), breaks=seq(0,300,50), minor_breaks=NULL) + scale_y_continuous(limits=c(5,30), expand=c(.025,.025), breaks=seq(5,30,5), minor_breaks=NULL) + labs(x=&quot;TV Advertising&quot;, y=&quot;Linear Prediciton&quot;) # Repeat for other two variables radio.pred &lt;- merge(data.frame(ad_radio=seq(0, 50, 5)), m.ivs[,c(&quot;ad_tv&quot;, &quot;ad_paper&quot;)]) radio.pred$p &lt;- as.data.frame(predict.lm(results, radio.pred, interval=&quot;confidence&quot;)) p2 &lt;- radio.pred %&gt;% ggplot(aes(x=ad_radio, y=p$fit)) + geom_line(size=.5, color=&quot;forestgreen&quot;) + geom_ribbon(aes(ymin=p$lwr, ymax=p$upr), alpha=0.2, fill=&quot;forestgreen&quot;) + scale_x_continuous(limits=c(0,50), expand=c(.025,.025), breaks=seq(0,50,10), minor_breaks=NULL) + scale_y_continuous(limits=c(5,30), expand=c(.025,.025), breaks=seq(5,30,5), minor_breaks=NULL) + labs(x=&quot;Radio Advertising&quot;, y=&quot;Linear Prediciton&quot;) paper.pred &lt;- merge(data.frame(ad_paper=seq(0, 120, 10)), m.ivs[,c(&quot;ad_tv&quot;, &quot;ad_radio&quot;)]) paper.pred$p &lt;- as.data.frame(predict.lm(results, paper.pred, interval=&quot;confidence&quot;)) p3 &lt;- paper.pred %&gt;% ggplot(aes(x=ad_paper, y=p$fit)) + geom_line(size=.5, color=&quot;darkorange&quot;) + geom_ribbon(aes(ymin=p$lwr, ymax=p$upr), alpha=0.2, fill=&quot;darkorange&quot;) + scale_x_continuous(limits=c(0,120), expand=c(.025,.025), breaks=seq(0,120,20), minor_breaks=NULL) + scale_y_continuous(limits=c(5,30), expand=c(.025,.025), breaks=seq(5,30,5), minor_breaks=NULL) + labs(x=&quot;Paper Advertising&quot;, y=&quot;Linear Prediciton&quot;) # Arrange three plots in a grid using package &#39;cowplot&#39; plot_grid(p1,p2,p3) 2.9.13 Figure 2.4 See Figure 2.4 # Create new data for prediction with &#39;ad_tv&#39; as focus ad.tv.pred &lt;- crossing(ad_tv=seq(0,300,30), # 11 levels ad_radio=seq(0,50,10)) # 6 levels # Append linear prediction and prediction intervals to new data ad.tv.pred$pred &lt;- as.data.frame( predict.lm(resultssig, # Model to use for prediction ad.tv.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals # Create plot and save as object &#39;p1&#39; p1 &lt;- # Begins plot ggplot(aes(x=ad_tv, # levels of &#39;ad_tv&#39; for x-axis y=pred$fit, # linear prediction for y-axis group=as.factor(ad_radio), # different geoms for each level of &#39;ad_radio&#39; color=as.factor(ad_radio)), # different colors for each level of &#39;ad_radio&#39; data=ad.tv.pred) + # Draws lines and points based on predicted values geom_line() + geom_point() + # Adds confidence interaval bands around line geom_errorbar(aes(ymin=pred$lwr, ymax=pred$upr), width=5) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(0,300,50), minor_breaks=NULL) + scale_y_continuous(limits=c(0,30), expand=c(.025,.025), breaks=seq(0,30,5), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + guides(color=guide_legend(title.position=&quot;top&quot;)) + # Labels axes and legend labs(x=&quot;TV Advertising (ad_tv)&quot;, y=&quot;Linear Prediction&quot;, color=&quot;Radio Advertising (ad_radio)&quot;) # Repeat for other variable ad.rad.pred &lt;- crossing(ad_tv=seq(0,300,100), # 4 levels ad_radio=seq(0,50,5)) # 11 levels ad.rad.pred$pred &lt;- as.data.frame( predict.lm(resultssig, ad.rad.pred, interval=&quot;confidence&quot;)) p2 &lt;- ggplot(aes(x=ad_radio, y=pred$fit, group=as.factor(ad_tv), color=as.factor(ad_tv)), data=ad.rad.pred) + geom_line() + geom_point() + geom_errorbar(aes(ymin=pred$lwr, ymax=pred$upr), width=.83) + scale_x_continuous(breaks=seq(0,50,10), minor_breaks=NULL) + scale_y_continuous(limits=c(0,30), expand=c(.025,.025), breaks=seq(0,30,5), minor_breaks=NULL) + theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + guides(color=guide_legend(title.position=&quot;top&quot;)) + labs(x=&quot;Radio Advertising (ad_radio)&quot;, y=&quot;Linear Prediction&quot;, color=&quot;TV Advertising (ad_tv)&quot;) # Arrange three plots in a grid using package &#39;cowplot&#39; plot_grid(p1,p2) 2.9.14 Figure 2.5 See Figure 2.5 # Create new data for prediction with &#39;size&#39; as focus size.pred &lt;- merge(data.frame(size=seq(70000,220000,15000)), # Variable of interest data.frame(q4=0:1)) # Dummy variable # Append linear prediction and prediction intervals to new data size.pred$pred &lt;- as.data.frame( predict.lm(mod.is, # Model to use for prediction size.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals # Begins plot ggplot(aes(x=size, # levels of &#39;size&#39; for x-axis y=pred$fit, # linear prediction for y-axis color=as.factor(q4)), # different colors for each level of &#39;q4&#39; data=size.pred) + # Draws lines based on predicted values geom_line(size=1) + # Adds confidence interval bands around line geom_ribbon(aes(ymin=pred$lwr, ymax=pred$upr, fill=as.factor(q4)), alpha=0.2) + # Next two commands set colors for lines and CI fill scale_color_manual(name=&quot;Q4&quot;, values=c(&quot;darkred&quot;, &quot;darkblue&quot;)) + scale_fill_manual(name=&quot;Q4&quot;, values=c(&quot;red&quot;, &quot;blue&quot;)) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(70000,220000,30000), minor_breaks=NULL) + scale_y_continuous(limits=c(-5000,25000), expand=c(.025,.025), breaks=seq(-5000,25000,10000), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + # Labels axes and legend labs(x=&quot;Size&quot;, y=&quot;Linear Prediction&quot;) 2.9.15 Figure 2.6 See Figure 2.6 # Create new data for prediction with &#39;size&#39; as focus size.pred &lt;- merge(data.frame(size=seq(70000,220000,15000)), # Variable of interest data.frame(q4=0:1)) # Dummy variable # Append linear prediction and prediction intervals to new data size.pred$pred &lt;- as.data.frame( predict.lm(mod.ss, # Model to use for prediction size.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals # Begins plot ggplot(aes(x=size, # levels of &#39;size&#39; for x-axis y=pred$fit, # linear prediction for y-axis color=as.factor(q4)), # different colors for each level of &#39;q4&#39; data=size.pred) + # Draws lines based on predicted values geom_line(size=1) + # Adds confidence interval bands around line geom_ribbon(aes(ymin=pred$lwr, ymax=pred$upr, fill=as.factor(q4)), alpha=0.2) + # Next two commands set colors for lines and CI fill scale_color_manual(name=&quot;Q4&quot;, values=c(&quot;darkred&quot;, &quot;darkblue&quot;)) + scale_fill_manual(name=&quot;Q4&quot;, values=c(&quot;red&quot;, &quot;blue&quot;)) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(70000,220000,15000), minor_breaks=NULL) + scale_y_continuous(limits=c(-5000,25000), expand=c(.025,.025), breaks=seq(-5000,25000,10000), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + # Labels axes and legend labs(x=&quot;Size&quot;, y=&quot;Linear Prediction&quot;) 2.9.16 Figure 2.7 See Figure 2.7 # Create new data for prediction with &#39;size&#39; as focus size.pred &lt;- merge(data.frame(size=seq(0,220000,20000)), # Variable of interest data.frame(q4=0:1)) # Dummy variable # Append linear prediction and prediction intervals to new data size.pred$pred &lt;- as.data.frame( predict.lm(mod.iss, # Model to use for prediction size.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals ggplot(aes(x=size, # levels of &#39;size&#39; for x-axis y=pred$fit, # linear prediction for y-axis color=as.factor(q4)), # different colors for each level of &#39;q4&#39; data=size.pred) + # Draws lines based on predicted values geom_line(size=1) + # Next two commands set colors for lines and CI fill scale_color_manual(name=&quot;Q4&quot;, values=c(&quot;darkred&quot;, &quot;darkblue&quot;)) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(0,220000,55000), minor_breaks=NULL) + scale_y_continuous(limits=c(-10000,25000), expand=c(.025,.025), breaks=seq(-5000,25000,10000), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + # Labels axes and legend labs(x=&quot;Size&quot;, y=&quot;Linear Prediction&quot;) 2.9.17 Figure 2.8 See Figure 2.8 # Create new data for prediction with &#39;size&#39; as focus size.pred &lt;- merge(data.frame(size=seq(70000,220000,15000)), # Variable of interest data.frame(q4=0:1)) # Dummy variable # Append linear prediction and prediction intervals to new data size.pred$pred &lt;- as.data.frame( predict.lm(mod.iss, # Model to use for prediction size.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals ggplot(aes(x=size, # levels of &#39;size&#39; for x-axis y=pred$fit, # linear prediction for y-axis color=as.factor(q4)), # different colors for each level of &#39;q4&#39; data=size.pred) + # Draws lines based on predicted values geom_line(size=1) + # Adds confidence interval bands around line geom_ribbon(aes(ymin=pred$lwr, ymax=pred$upr, fill=as.factor(q4)), alpha=0.2) + # Next two commands set colors for lines and CI fill scale_color_manual(name=&quot;Q4&quot;, values=c(&quot;darkred&quot;, &quot;darkblue&quot;)) + scale_fill_manual(name=&quot;Q4&quot;, values=c(&quot;red&quot;, &quot;blue&quot;)) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(70000,220000,30000), minor_breaks=NULL) + scale_y_continuous(limits=c(-5000,25000), expand=c(.025,.025), breaks=seq(-5000,25000,10000), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + # Labels axes and legend labs(x=&quot;Size&quot;, y=&quot;Linear Prediction&quot;) 2.9.18 Figure 2.9 See Figure 2.9 # Create new data for prediction with &#39;size&#39; as focus size.pred &lt;- merge(data.frame(size=seq(0,240000,20000)), # Variable of interest data.frame(quarter=1:4)) # Dummy variable # Set &#39;quarter&#39; to be factor variable to match model size.pred$quarter &lt;- as.factor(size.pred$quarter) # Append linear prediction and prediction intervals to new data size.pred$pred &lt;- as.data.frame( predict.lm(mod.mis, # Model to use for prediction size.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals # Begins plot ggplot(aes(x=size, # levels of &#39;size&#39; for x-axis y=pred$fit, # linear prediction for y-axis color=quarter), # different colors for each level of &#39;quarter&#39; data=size.pred) + # Draws lines based on predicted values geom_line(size=1) + # Adds confidence interval bands around line geom_ribbon(aes(ymin=pred$lwr, ymax=pred$upr, fill=quarter), alpha=0.2) + # Next two commands set colors for lines and CI fill scale_color_manual(name=&quot;Quarter&quot;, values=c(&quot;darkred&quot;, &quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;darkorange&quot;)) + scale_fill_manual(name=&quot;Quarter&quot;, values=c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;)) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(0,240000,40000), minor_breaks=NULL) + scale_y_continuous(limits=c(-10000,45000), expand=c(.025,.025), breaks=seq(-10000,40000,10000), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + # Labels axes and legend labs(x=&quot;Size&quot;, y=&quot;Linear Prediction&quot;) 2.9.19 Figure 2.10 See Figure 2.10 # Create new data for prediction with &#39;size&#39; as focus size.pred &lt;- merge(data.frame(size=seq(70000,220000,15000)), # Variable of interest data.frame(quarter=1:4)) # Dummy variable # Set &#39;quarter&#39; to be factor variable to match model size.pred$quarter &lt;- as.factor(size.pred$quarter) # Append linear prediction and prediction intervals to new data size.pred$pred &lt;- as.data.frame( predict.lm(mod.mis, # Model to use for prediction size.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals # Begins plot ggplot(aes(x=size, # levels of &#39;size&#39; for x-axis y=pred$fit, # linear prediction for y-axis color=quarter), # different colors for each level of &#39;q4&#39; data=size.pred) + # Draws lines based on predicted values geom_line(size=1) + geom_point() + # Adds confidence interval bands around line scale_color_manual(name=&quot;Quarter&quot;, values=c(&quot;darkred&quot;, &quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;darkorange&quot;)) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(70000,220000,30000), minor_breaks=NULL) + scale_y_continuous(limits=c(-5000,40000), expand=c(.025,.025), breaks=seq(-5000,40000,5000), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + # Labels axes and legend labs(x=&quot;Size&quot;, y=&quot;Linear Prediction&quot;) "],["logistic-regression.html", "Topic 3 Logistic Regression 3.1 Motivation 3.2 R Packages and Datasets for Topic 3 3.3 Why not use linear regression? 3.4 Understanding Logistic Regression 3.5 Conducting Logistic Regression 3.6 Logistic Regression Example 3.7 Suggested Readings 3.8 R Code", " Topic 3 Logistic Regression 3.1 Motivation Marketers often observe binary outcomes Did a customer: purchase? subscribe? renew? respond? Using linear regression is not appropriate But logistic regression still allows us to: Understand IV/DV relationships Make predictions 3.2 R Packages and Datasets for Topic 3 library(ggplot2) # Advanced graphing capabilities library(tidyr) # Easier programming library(flextable) # Better HTML Tables library(htmlTable) # Better HTML Tables library(jtools) # Concise regression results library(dplyr) # Easier programming library(caret) # Create data partitions load(&quot;Topic03/directmktg.rdata&quot;) source(&quot;Topic03/or_table.R&quot;) source(&quot;Topic03/logreg_cm.R&quot;) source(&quot;Topic03/logreg_roc.R&quot;) source(&quot;Topic03/gainlift.R&quot;) source(&quot;Topic03/logreg_cut.R&quot;) Download directmktg.rdata 3.3 Why not use linear regression? Want to see how \\(age\\) affects \\(buy\\) \\(buy=\\begin{cases}1\\text{ if yes/true}\\\\0\\text{ if no/false}\\end{cases}\\) Examine relationship with a scatterplot What do we see? Figure 3.1: Scatterplot with binary DV (R code) Try linear regression: \\(buy=\\alpha+\\beta_1 age\\) Table 3.1: Linear Regression Results (R code) F(1,398) 251.7421 R² 0.3874 Adj. R² 0.3859 Est. S.E. t val. p (Intercept) -0.7154 0.0702 -10.1930 0.0000 age 0.0285 0.0018 15.8664 0.0000 Standard errors: OLS Good \\(R^2\\) and \\(age\\) is highly significant So whats the problem? Predict \\(buy\\) from linear regression results: \\(\\hat{buy}=-.7154+.0285age\\) Prediction line shown in plot Figure 3.2: Predicted Values from Linear Regression (R code) Add \\(age\\) categories and plot mean \\(buy\\) for each category What shape does this resemble? Can we use this shape to model the relationship? Figure 3.3: Buy for Age Groups (R code) 3.4 Understanding Logistic Regression Uses the logistic function: \\(f(z)=\\frac{e^u}{1+e^u}\\) \\(f(z)\\) is the probability of event happening \\(u\\) is a linear function, such as: \\(\\alpha+\\beta x\\) Ensures predictions are never above 1 or below 0 Figure 3.4: Logistic Function (R code) Probability of event success vs. failure \\(=\\frac{f(z)}{1-f(z)}=\\) Odds Ratio (\\(OR\\)) Suppose probability of success \\(=.01\\), then: \\(OR=\\frac{.01}{1-.01}=.0101\\) Suppose probability of success \\(=.001\\), then: \\(OR=\\frac{.001}{1-.001}=.0010\\) Suppose probability of success \\(=.99\\), then: \\(OR=\\frac{.99}{1-.99}=99\\) Suppose probability of success \\(=.999\\), then: \\(OR=\\frac{.999}{1-.999}=999\\) Suppose probability of success \\(=.5\\), then: \\(OR=\\frac{.5}{1-.5}=1\\) Substituting logistic function for \\(f(z)\\) into Odds Ratio \\(\\Rightarrow\\) \\(OR=e^u=e^{\\alpha+betax}\\) \\(\\frac{f(z)}{1-f(z)}=\\frac{\\frac{e^u}{1+e^u}}{1-\\frac{e^u}{1+e^u}}=\\frac{\\frac{e^u}{1+e^u}}{\\frac{1+e^u}{1+e^u}-\\frac{e^u}{1+e^u}}=\\frac{\\frac{e^u}{1+e^u}}{\\frac{1}{1+e^u}}=e^u\\) Can transform exponential function into linear \\(\\Rightarrow\\) \\(Logit=\\ln(OR)=\\alpha+\\beta x\\) 3.5 Conducting Logistic Regression Model Estimation Assessing Model Fit Goodness of Fit Measures Classification Matrix ROC Curve Interpreting Coefficients Gains and Lift 3.5.1 Model Estimation Best to use training data and holdout data Estimate model on training data (~75% of sample) Check prediction accuracy on holdout data (~25%) Can estimate either (1) \\(OR\\) or (2) \\(Logit\\) formulation \\(OR=e^{\\alpha+\\beta_1x_1+\\cdots+\\beta_kx_k}\\) \\(Logit=\\alpha+\\beta_1x_1+\\cdots+\\beta_kx_k\\) Independent variables: Can be one or more Can be continuous or categorical/factor 3.5.2 Assessing Model Fit 3.5.2.1 Goodness-of-Fit Measures Overall significance based on \\(-2LL\\) Lower (closer to \\(0\\)) \\(-2LL\\) indicates a better fit Compare \\(-2LL\\) of estimated model with null model McFaddens Pseudo-\\(R^2\\) Values range from 0 to 1 like linear regression Interpreted in a similar manner Amount of variation in DV explained by IVs 3.5.2.2 Classification Matrix How does the model do in predicting outcomes? Generate predicted probability of success, \\(p(\\text{SUCCESS})\\), for each observation If \\(p(\\text{SUCCESS})\\ge0.5\\), predict \\(\\text{SUCCESS}=1\\) If \\(p(\\text{SUCCESS})&lt;0.5\\), predict \\(\\text{SUCCESS}=0\\), or \\(\\text{FAILURE}\\) Check predictions against actual outcomes Examine both training and holdout data Figure 3.5: Classification Matrix Three main measures Sensitivity: Predicted success given actual success \\(p(\\hat{+}|+)=a/(a+c)\\) Specificity: Predicted failure given actual failure \\(p(\\hat{-}|-)=d/(b+d)\\) Overall correctly classified \\((a+d)/(a+b+c+d)\\) Sensitivity vs. Specificity Ideally, want both to be high, but the \\(p(\\text{SUCCESS})\\ge\\pi\\) threshold can be changed Why change \\(\\pi\\)? Avoid false positives or negatives By default: Increasing sensitivity decreases specificity Increasing specificity decreases sensitivity Overall correctly classified Compare with Proportional Chance Criterion (\\(PCC\\)) \\(PCC\\) is the average probability of classification based on group sizes \\(PCC=p^2+(1-p)^2\\) where \\(p\\) is the proportion of sample in the \\(\\text{SUCCESS}\\) group Overall correctly classified \\(&gt;PCC\\) considered good fit when examining holdout data 3.5.2.3 ROC Curve Plot sensitivity by \\(1-\\) specificity as \\(\\pi\\) goes from \\(0\\) to \\(1\\) More area under curve means better model Area under Curve Discrimination AUC = .5 None .5 &lt; AUC &lt; .7 Poor .7  AUC &lt; .8 Acceptable .8  AUC &lt; .9 Excellent AUC  .9 Outstanding Figure 3.6: Sample ROC Curve 3.5.3 Interpreting Coefficients Relationship between DV and each IV \\(H_0: \\beta_k=0\\) vs. \\(H_a: \\beta_k\\ne0\\) Interpret significant relationships Interpretation depends on \\(OR\\) or \\(Logit\\) estimation Direction of relationship: \\(Logit\\) estimation: \\(\\beta_k&gt;0\\) for positive, \\(\\beta_k&lt;0\\) for negative \\(OR\\) estimation: \\(\\beta_k&gt;1\\) for positive, \\(\\beta_k&lt;1\\) for negative Magnitude of change: \\(Logit\\) estimation: coefficients are not particularly useful \\(OR\\) estimation: Percentage change in odds Compare probabilities between groups 3.5.4 Gain and Lift Evaluate performance of classification Example: Suppose \\(10\\%\\) of \\(2000\\) customers will accept offer For \\(100\\) random customers, expect \\(10\\) accepted offers Model predicts some customers more likely to accept Instead of contacting \\(100\\) random customersContact \\(100\\) most likely to accept based on model Continue doing this in groups of \\(100\\) (or \\(200\\), etc.) Gain and lift provide measures of how much better the model performs vs. no model/random Process Predict $p() for each observation and sort descending Split into 10 (deciles) or 20 (demi-deciles) ordered groups Calculate \\(\\%\\) observations and \\(\\%\\) successes for each group 3.5.4.1 Gain Cumulative successes up to that group divided by total successes across all groups Plot on \\(y\\)-axis, with cumulative percent of observations on \\(x\\)-axis Figure 3.7: Typical Gain Chart Shape 3.5.4.2 Lift Ratio of cumulative success up to that group divided by expected success from no model Plot on \\(y\\)-axis, with cumulative percent of observations on \\(x\\)-axis Figure 3.8: Typical Lift Chart Shape 3.5.5 Sensitivity/Specificity Plots Sensitivity, Specificity, and Accuracy depend on the cutoff value for predicting SUCCESS/ FAILURE While 0.5 is the most common threshold, it might not be the best threshold for prediction Sensitivity/Specificity Plots can show the analyst how each changes with different cutoff values The analyst can try to balance the three depending on the purpose of project Figure 3.9: Sample Sensitivy/Specificity Plot 3.6 Logistic Regression Example 3.6.1 Overview Purchase data for direct marketing campaign 400 observations of individual responses DV: Purchase made, \\(buy\\) (factor: Yes, No) IVs: Age, \\(age\\) Estimated Salary ($000s), \\(salary\\) Gender, \\(gender\\) (factor: Male, Female) Predict likelihood of purchase 3.6.2 Estimation Results Logit formulation results Table 3.2: Logistic Regression Results (Logit Formulation) (R code) ²(3) 182.2574 Pseudo-R² (Cragg-Uhler) 0.6231 Pseudo-R² (McFadden) 0.4638 AIC 218.6842 BIC 233.5126 Est. S.E. z val. p (Intercept) -13.1661 1.6217 -8.1187 0.0000 age 0.2502 0.0321 7.7961 0.0000 salary 0.0406 0.0067 6.0265 0.0000 genderFemale -0.4069 0.3498 -1.1631 0.2448 Standard errors: MLE Model p-value = 0.0000 Odds Ratio Coefficients Table 3.3: Logistic Regression Odds Ratio Coefficients (R code) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-ff86b1a0{}.cl-ff7ea0dc{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ff7ea0dd{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ff7ea0de{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ff7eeeb6{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ff7eeeb7{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ff7eeeb8{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ff7eeeb9{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ff7eeeba{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ff7eeebb{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} ParameterOR Estp2.5%97.5%(Intercept)0.00000.00000.00000.0000age1.28430.00001.20601.3677salary1.04150.00001.02781.0553genderFemale0.66570.24480.33541.3215 3.6.3 Overall Model Fit Based on the likelihood ratio \\(\\chi^2\\) test with a \\(p\\text{-value}&lt;.0001\\), the overall model is significant (see Table 3.2) McFaddens Pseudo-\\(R^2\\) of \\(.464\\) means that the model explains about \\(46\\%\\) of the variation between buyers/non-buyers (see Table 3.2) Classification Matrix for the Training Sample shows: High sensitivity (\\(72.2\\%\\)) High specificity (\\(91.2\\%\\)) Correctly classified (\\(84.4\\%\\)) &gt; PCC (\\(54.0\\%\\)) Table 3.4: Classification Matrix for Training Sample (R code) Confusion Matrix and Statistics Reference Prediction No Yes No 176 30 Yes 17 78 Accuracy : 0.8439 95% CI : (0.7978, 0.883) No Information Rate : 0.6412 P-Value [Acc &gt; NIR] : 4.55e-15 Kappa : 0.6514 Mcnemar&#39;s Test P-Value : 0.08005 Sensitivity : 0.7222 Specificity : 0.9119 Pos Pred Value : 0.8211 Neg Pred Value : 0.8544 Prevalence : 0.3588 Detection Rate : 0.2591 Detection Prevalence : 0.3156 Balanced Accuracy : 0.8171 &#39;Positive&#39; Class : Yes PCC = 53.99% Classification Matrix for the Test/Holdout Sample shows: High sensitivity (\\(77.1\\%\\)) High specificity (\\(90.6\\%\\)) Correctly classified (\\(85.9\\%\\)) &gt; PCC (\\(54.3\\%\\)) Table 3.5: Classification Matrix for Test/Holdout Data (R code) Confusion Matrix and Statistics Reference Prediction No Yes No 58 8 Yes 6 27 Accuracy : 0.8586 95% CI : (0.7741, 0.9205) No Information Rate : 0.6465 P-Value [Acc &gt; NIR] : 2.004e-06 Kappa : 0.6866 Mcnemar&#39;s Test P-Value : 0.7893 Sensitivity : 0.7714 Specificity : 0.9062 Pos Pred Value : 0.8182 Neg Pred Value : 0.8788 Prevalence : 0.3535 Detection Rate : 0.2727 Detection Prevalence : 0.3333 Balanced Accuracy : 0.8388 &#39;Positive&#39; Class : Yes PCC = 54.29% ROC Curve for Training Sample Area \\(&gt;.90\\) suggests an outstanding model fit Figure 3.10: ROC Curve for Training Sample (R code) ROC Curve for Holdout Sample Area \\(&gt;.90\\) suggests an outstanding model fit Figure 3.11: ROC Curve for Test/Holdout Sample (R code) 3.6.4 Interpreting Coefficients \\(age\\) is positive (\\(OR&gt;1\\)) and significant (\\(p&lt;.001\\)) \\(1\\) year increase in \\(age\\) increases odds of buying by a factor of \\(1.28\\) (or odds of buying increase by \\(25\\%\\)) \\(salary\\) is positive (\\(OR&gt;1\\)) and significant (\\(p&lt;.001\\)) \\(\\$1000\\) increase in \\(salary\\) increases odds of buying by a factor of \\(1.04\\) (or odds of buying increase by \\(4\\%\\)) \\(gender\\) is negative (\\(OR&lt;1\\)), but not significant (\\(p=.245\\)) Had it been significant Being female decreases odds of buying by a factor of \\(.67\\) (or odds of buying decrease by \\(33\\%\\)) Can visually examine how \\(\\Pr(buy)\\) changes with a variable for continuous IVs Factor variables can be separate lines (see Figure 3.12 or separate plots see 3.13) Figure 3.12: Margin Plot for Age (Separate Lines for Gender) (R code) Figure 3.13: Margin Plot for Age (Separate Plots for Gender) (R code) Can create different lines for specific levels of another continuous IV See 3.14 for \\(age\\) with different levels of \\(salary\\) See 3.15 for \\(salary\\) with different levels of \\(age\\) Figure 3.14: Margin Plot for Age at Different Levels of Salary (R code) Figure 3.15: Margin Plot for Salary at Different Levels of Age (R code) 3.6.5 Gain Can examine gain for both the training and holdout samples But using holdout is more informative Contacting the top \\(25\\%\\) of predicted buyers yields nearly \\(60\\%\\) of actual buyers Table 3.6: Gain Table for Training and Test/Holdout Samples (R code) # A tibble: 20 x 3 `% Sample` Holdout Training &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.05 0.114 0.130 2 0.1 0.2 0.25 3 0.15 0.343 0.370 4 0.2 0.486 0.481 5 0.25 0.6 0.593 6 0.3 0.743 0.713 7 0.35 0.771 0.778 8 0.4 0.829 0.843 9 0.45 0.914 0.907 10 0.5 0.971 0.935 11 0.55 1 0.981 12 0.6 1 0.991 13 0.65 1 0.991 14 0.7 1 0.991 15 0.75 1 1 16 0.8 1 1 17 0.85 1 1 18 0.9 1 1 19 0.95 1 1 20 1 1 1 Figure 3.16: Gain Chart for Training and Test/Holdout Samples (R code) 3.6.6 Lift Can examine gain for both the training and holdout samples But using holdout is more informative Contacting the top \\(25\\%\\) of predicted buyers provides lift of nearly 2.5 Table 3.7: Lift Table for Training and Test/Holdout Samples (R code) # A tibble: 20 x 3 `% Sample` Holdout Training &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.05 2.83 2.60 2 0.1 2.2 2.51 3 0.15 2.42 2.48 4 0.2 2.53 2.42 5 0.25 2.48 2.38 6 0.3 2.54 2.38 7 0.35 2.25 2.23 8 0.4 2.10 2.11 9 0.45 2.06 2.02 10 0.5 1.96 1.88 11 0.55 1.83 1.79 12 0.6 1.68 1.66 13 0.65 1.55 1.53 14 0.7 1.43 1.42 15 0.75 1.34 1.34 16 0.8 1.25 1.25 17 0.85 1.18 1.18 18 0.9 1.11 1.11 19 0.95 1.05 1.06 20 1 1 1 Figure 3.17: Lift Chart for Training and Test/Holdout Samples (R code) 3.6.7 Sensitivity/Specificity Plots Examine to see if different thresholds are warranted Looking at the plots for both the training sample (see Figure 3.18) and the test/holdout sample (see Figure 3.19), it might be worthwhile to try a cutoff threshold between 0.35 and 0.40 Doing so will balance specificity and sensitivity without hurting accuracy Figure 3.18: Sensitivity/Specificity Plot for Training Sample (R code) Figure 3.19: Sensitivity/Specificity Plot for Test/Holdout Sample (R code) 3.7 Suggested Readings R for Marketing Research and Analytics. 2nd Edition (2019). Chapman, Chris; McDonnel Feit, Elea BGSU Library Link:http://maurice.bgsu.edu/record=b4966554~S9 eBook through BGSU Library:https://link-springer-com.ezproxy.bgsu.edu/book/10.1007%2F978-3-030-14316-9 Chapter 9.2: Linear Models for Binary Outcomes: Logistic Regression OpenIntro Statistics. 4th Edition (2019). Diez, David; Cetinkaya-Rundel, Mine; Barr, Christopher D. Available at OpenIntro.org:https://www.openintro.org/book/os/ Chapter 9: Multiple and logistic regression Multivariate Data Analysis. Hair, Joseph F.; Black, William C.; Babin, Barry J.; Anderson, Rolph E. 7th Edition: Search for multivariate data analysis 7th edition hair Chapter 6: Logistic Regression with a Binary Dependent Variable 5th Edition: Course reserves Chapter 5: Multiple Discriminant Analysis and Logistic Regression (pp. 276-281; 314-321) 3.8 R Code Figure 3.1 directmktg %&gt;% mutate(buy01=as.numeric(buy)-1) %&gt;% # Change &#39;buy&#39; to 0-1 ggplot(aes(x=age, y=buy01)) + geom_point(size=2) + labs(x=&quot;Age&quot;, y=&quot;Buy&quot;) Figure 3.2 directmktg %&gt;% select(age) %&gt;% # Select only the age variable mutate(yhat=predict.lm(model,.)) %&gt;% # Predict y from model # Next line creates variable to highlight negative predictions mutate(neg=as.factor(ifelse(yhat&lt;0,&quot;Yes&quot;,&quot;No&quot;))) %&gt;% ggplot(aes(x=age, y=yhat, color=neg)) + geom_point(size=3) + scale_color_manual(values=c(&quot;Yes&quot;=&quot;red&quot;, # Manually set point colors &quot;No&quot;=&quot;black&quot;), guide=&quot;none&quot;) + labs(x=&quot;Age&quot;, y=&quot;Linear Prediction&quot;) Figure 3.3 # Create data frame grouped by age dmgrp &lt;- directmktg %&gt;% # &#39;cut&#39; breaks a continuous variable into groups of each width # &#39;as.numeric&#39; keeps the new variable as integer (vs. factor) mutate(agegrp = as.numeric(cut(age, 9))) %&gt;% group_by(agegrp) %&gt;% summarise(age=mean(age), buy=mean(as.numeric(buy)-1)) # Run logistic model to create prediction to make s-curve binmod &lt;- glm(buy~age, directmktg, family=&quot;binomial&quot;) # Create dataframe with predicted values dmpred &lt;- directmktg %&gt;% select(age, buy) %&gt;% mutate(yhat=predict(binmod, type=&quot;response&quot;), buy=as.numeric(buy)-1) # Create combined plot; each geom with separate data ggplot() + geom_point(data=directmktg, aes(x=age, y=(as.numeric(buy)-1)), size=3, color=&quot;red&quot;) + geom_line(data=dmgrp, aes(x=age, y=buy), size=1.5, color=&quot;midnightblue&quot;) + geom_line(data=dmpred, aes(x=age, y=yhat), size=1.5, color=&quot;darkorange&quot;) + theme(text=element_text(size=15)) + labs(x=&quot;Age&quot;, y=&quot;Buy&quot;) Figure 3.4 # Create simulated data frame based on logistic function u=seq(-7,7,.05) fz=exp(u)/(1+exp(u)) ufz=data.frame(u=u, fz=fz) # Plot function ufz %&gt;% ggplot(aes(x=u, y=fz)) + geom_line(color=&quot;darkorange&quot;, size=1.5) + theme(text=element_text(size=15), panel.grid.major.x = element_blank()) + scale_x_continuous(breaks=0, minor_breaks=NULL) + scale_y_continuous(breaks=seq(0,1,1), minor_breaks=NULL) + labs(x=&quot;u&quot;, y=&quot;f(z)&quot;) Figure 3.10 # Use the &#39;logreg_roc.R&#39; user-defined script # It was loaded above with the packages # Requires package &#39;pROC&#39; and &#39;ggplot2 logreg_roc(model, # Object with model results train) # Data to use (i.e., training vs. testing) Figure 3.11 # Use the &#39;logreg_roc.R&#39; user-defined script # It was loaded above with the packages # Requires package &#39;pROC&#39; and &#39;ggplot2 logreg_roc(model, # Object with model results test) # Data to use (i.e., training vs. testing) Figure 3.12 # Want to predict for different levels of IVs # Because we often do this for other continuous variables, we first # create a df with mean values of all continuous IVs # at different levels of the factor variable (if there are any) # NOTE: variable names must be EXACTLY the same as in model m.ivs &lt;- train %&gt;% group_by(gender) %&gt;% summarise(age=mean(age, na.rm=TRUE), salary=mean(salary, na.rm=TRUE)) # Create new data for prediction with &#39;age&#39; as focus # This will be used for the next two figures age.pred &lt;- merge(data.frame(age=seq(18,60,2)), m.ivs[ ,c(&quot;salary&quot;, &quot;gender&quot;)]) # Create data frame with predicted values and confidence bands train.pred &lt;- # &#39;cbind&#39; combines objects by columns cbind(age.pred, predict(model, # Model to predict values with age.pred, # New data to use for IVs type=&quot;link&quot;, # Return log-odds predictions se=TRUE)) %&gt;% # Get std.err. for CIs mutate(pred=plogis(fit), # Calculate pr(buy) upr=plogis(fit+(qnorm(0.975)*se.fit)), # Upper CI lwr=plogis(fit-(qnorm(0.975)*se.fit))) # Lower CI # Create plot with gender on same plot ggplot(aes(x=age, y=pred), data=train.pred) + geom_line(aes(color=gender), size=1) + geom_ribbon(aes(ymin=lwr, ymax=upr, color=gender, fill=gender), alpha=.2) + theme(legend.position=&quot;bottom&quot;, plot.caption=element_text(size=8)) + scale_color_manual(values=c(&quot;red4&quot;, &quot;navy&quot;)) + scale_fill_manual(values=c(&quot;pink&quot;, &quot;cyan&quot;)) + labs(x=&quot;Age&quot;, y=&quot;Pr(Buy)&quot;, color=&quot;Gender&quot;, fill=&quot;Gender&quot;, caption=&quot;Calculated at mean value of salary&quot;) Figure 3.13 # Create plot with gender on same plot ggplot(aes(x=age, y=pred), data=train.pred) + geom_line(aes(color=gender), size=1) + geom_ribbon(aes(ymin=lwr, ymax=upr, color=gender, fill=gender), alpha=.2) + facet_grid(~gender) + theme(legend.position=&quot;none&quot;, plot.caption=element_text(size=8)) + scale_color_manual(values=c(&quot;red4&quot;, &quot;navy&quot;)) + scale_fill_manual(values=c(&quot;pink&quot;, &quot;cyan&quot;)) + labs(x=&quot;Age&quot;, y=&quot;Pr(Buy)&quot;, caption=&quot;Calculated at mean value of salary&quot;) Figure 3.14 # Want to predict for different levels of IVs # Because we often do this for other continuous variables, we first # create a df with values of all continuous IVs # at different levels of the factor variable (if there are any) # NOTE: variable names must be EXACTLY the same as in model msd.ivs &lt;- train %&gt;% group_by(gender) %&gt;% # Group on factor variable # Calculate values for different levels of IVs summarise(age_mn15=mean(age, na.rm=TRUE)-1.5*sd(age, na.rm=TRUE), age_m=mean(age, na.rm=TRUE), age_mp15=mean(age, na.rm=TRUE)+1.5*sd(age, na.rm=TRUE), salary_mn15=mean(salary, na.rm=TRUE)-1.5*sd(salary, na.rm=TRUE), salary_m=mean(salary, na.rm=TRUE), salary_mp15=mean(salary, na.rm=TRUE)+1.5*sd(salary, na.rm=TRUE)) %&gt;% # Convert data from wide to long pivot_longer(cols=!gender, names_to=c(&quot;.value&quot;,&quot;measure&quot;), names_sep=&quot;_&quot;) age.pred &lt;- merge(data.frame(age=seq(18, 60, 2)), # Variable of interest msd.ivs[,c(&quot;gender&quot;, &quot;salary&quot;, &quot;measure&quot;)]) # Create data frame with predicted values and confidence bands train.pred &lt;- # &#39;cbind&#39; combines objects by columns cbind(age.pred, predict(model, # Model to predict values with age.pred, # New data to use for IVs type=&quot;link&quot;, # Return log-odds predictions se=TRUE)) %&gt;% # Get std.err. for CIs mutate(pred=plogis(fit), # Calculate pr(buy) upr=plogis(fit+(qnorm(0.975)*se.fit)), # Upper CI lwr=plogis(fit-(qnorm(0.975)*se.fit)), # Lower CI gender=as.factor(gender), # Factor for plot salary=as.factor(measure)) # Factor for plot ggplot(aes(x=age, y=pred), data=train.pred) + geom_line(aes(color=salary), size=1) + geom_ribbon(aes(ymin=lwr, ymax=upr, color=salary, fill=salary), alpha=.2) + facet_grid(.~gender) + # Create separate plots for gender theme(legend.position=&quot;bottom&quot;) + scale_color_manual(&quot;Salary&quot;, values=c(&quot;red4&quot;, &quot;navy&quot;, &quot;forestgreen&quot;), labels=c(&quot;Mean&quot;, &quot;Mean-1.5SD&quot;, &quot;Mean+1.5SD&quot;)) + scale_fill_manual(&quot;Salary&quot;, values=c(&quot;pink&quot;, &quot;cyan&quot;, &quot;lawngreen&quot;), labels=c(&quot;Mean&quot;, &quot;Mean-1.5SD&quot;, &quot;Mean+1.5SD&quot;)) + labs(x=&quot;Age&quot;, y=&quot;Pr(Buy)&quot;) Figure 3.15 # Use df &#39;msd.ivs&#39; from above sal.pred &lt;- merge(data.frame(salary=seq(15, 150, 5)), # Variable of interest msd.ivs[,c(&quot;gender&quot;, &quot;age&quot;, &quot;measure&quot;)]) # Create data frame with predicted values and confidence bands train.pred &lt;- # &#39;cbind&#39; combines objects by columns cbind(sal.pred, predict(model, # Model to predict values with sal.pred, # New data to use for IVs type=&quot;link&quot;, # Return log-odds predictions se=TRUE)) %&gt;% # Get std.err. for CIs mutate(pred=plogis(fit), # Calculate pr(buy) upr=plogis(fit+(qnorm(0.975)*se.fit)), # Upper CI lwr=plogis(fit-(qnorm(0.975)*se.fit)), # Lower CI gender=as.factor(gender), # Factor for plot age=as.factor(measure)) # Factor for plot ggplot(aes(x=salary, y=pred), data=train.pred) + geom_line(aes(color=age), size=1) + geom_ribbon(aes(ymin=lwr, ymax=upr, color=age, fill=age), alpha=.2) + facet_grid(.~gender) + # Create separate plots for gender theme(legend.position=&quot;bottom&quot;) + scale_color_manual(&quot;Age&quot;, values=c(&quot;red4&quot;, &quot;navy&quot;, &quot;forestgreen&quot;), labels=c(&quot;Mean&quot;, &quot;Mean-1.5SD&quot;, &quot;Mean+1.5SD&quot;)) + scale_fill_manual(&quot;Age&quot;, values=c(&quot;pink&quot;, &quot;cyan&quot;, &quot;lawngreen&quot;), labels=c(&quot;Mean&quot;, &quot;Mean-1.5SD&quot;, &quot;Mean+1.5SD&quot;)) + labs(x=&quot;Salary&quot;, y=&quot;Pr(Buy)&quot;) Figure 3.16 # Plot was already returned in the previous call to &#39;gainlift&#39; glresults$gainplot Figure 3.17 # Plot was already returned in the previous call to &#39;gainlift&#39; glresults$gainlift Figure 3.18 # Use the &#39;logreg_cut.R&#39; user-defined script # It was loaded above with the packages # Requires packages &#39;ggplot2&#39; logreg_cut(model, train, &quot;Yes&quot;) Figure 3.19 # Use the &#39;logreg_cut.R&#39; user-defined script # It was loaded above with the packages # Requires packages &#39;ggplot2&#39; logreg_cut(model, test, &quot;Yes&quot;) Table 3.1 model &lt;- directmktg %&gt;% mutate(buy=as.numeric(buy)-1) %&gt;% lm(buy ~ age, .) # NOTE: &#39;summ&#39; uses the &#39;jtools&#39; package summ(model, model.info=FALSE, digits=4) # For virtual environment, use &#39;summary&#39; from Base R, # but manually calculate McFadden&#39;s Pseudo-Rsq summary(model) Mrsq &lt;- 1-model$deviance/model$null.deviance cat(&quot;McFadden&#39;s Pseudo-Rsquared = &quot;, round(Mrsq, digits=4) Table 3.2 # Use &#39;caret&#39; package to create training and test/holdout samples # This will create two separate dataframes: train and test set.seed(4320) inTrain &lt;- createDataPartition(y=directmktg$buy, p=.75, list=FALSE) train &lt;- directmktg[inTrain,] test &lt;- directmktg[-inTrain,] # Estimate the model on the training data model &lt;- glm(buy ~ age + salary + gender, train, family=&quot;binomial&quot;) # NOTE: &#39;summ&#39; uses the &#39;jtools&#39; package summ(model, model.info=FALSE, digits=4) # For virtual environment, use &#39;summary&#39; from Base R Table 3.3 # Use the &#39;or_table.R&#39; user-defined script # It was loaded above with the packages flextable(or_table(model)) Table 3.4 # Use the &#39;logreg_cm.R&#39; user-defined script # It was loaded above with the packages # Requires package &#39;caret&#39; logreg_cm(model, # Object with model results train, # Data to use (i.e., training vs. testing) &quot;Yes&quot;) # Factor level for &quot;True&quot; Table 3.5 # Use the &#39;logreg_cm.R&#39; user-defined script # It was loaded above with the packages # Requires package &#39;caret&#39; logreg_cm(model, # Object with model results test, # Data to use (i.e., training vs. testing) &quot;Yes&quot;) # Factor level for &quot;True&quot; Table 3.6 # Use the &#39;gainlift.R&#39; user-defined script # It was loaded above with the packages # Requires packages &#39;ggplot2&#39;, &#39;dplyr&#39;, and &#39;tidyr&#39; # Returns a list of four things: # gainplot, liftplot, gaintable, lifttable glresults &lt;- gainlift(model, train, test, &quot;Yes&quot;) glresults$gaintable Table 3.7 # Table was already returned in the previous call to &#39;gainlift&#39; glresults$lifttable "],["targeting-and-retaining-customers.html", "Topic 4 Targeting and Retaining Customers 4.1 R Packages and Datasets for Topic 4 4.2 Targeting Customers 4.3 Retaining Customers 4.4 Targeting Customers (Linear Regression) Example 4.5 Targeting Customers (Logistic Regression) 4.6 Suggested Readings 4.7 R Code", " Topic 4 Targeting and Retaining Customers 4.1 R Packages and Datasets for Topic 4 library(cowplot) # Arrange plots in grid library(ggplot2) # Advanced graphing capabilities library(tidyr) # Easier programming library(GGally) # Scatterplot matrix library(flextable) # Better HTML Tables library(htmlTable) # Better HTML Tables library(jtools) # Concise regression results library(dplyr) # Easier programming library(caret) # Create data partitions load(&quot;Topic04/bankmktg.rdata&quot;) load(&quot;Topic04/telecom.rdata&quot;) 4.2 Targeting Customers One-to-One Marketing Time consuming Costly Mass Marketing Customer needs not being met Target Marketing Market to those likely to 4.2.1 Goal Target customers with the highest likelihood of a favorable outcome using explanatory variables Outcome variable could be: Purchase Sales Costs Profitability CLV Explanatory variables could be: Demographics Behaviors Usage Lifestyles The outcome variable will dictate the type of analysis we can perform Continuous outcome variables have a meaningful magnitude Use linear regression Categorical outcome variables do not have a meaningful magnitude Use logistic regression 4.3 Retaining Customers Importance of retention: Reducing defections \\(5\\%\\) boosts profits \\(25\\%\\) to \\(85\\%\\).  Frederick F. Reichheld and W. Earl Sasser, Jr. 4.3.1 Goal Identify factors (i.e., independent variables) that increase the likelihood of retention (or decrease the likelihood of churn) Retention (or Churn) is the outcome or dependent variable DV = Binary, so Method = Logistic Regression 4.4 Targeting Customers (Linear Regression) Example 4.4.1 Overview Customer revenue, usage, and demographics for a cell phone provider DV: Mean monthly revenue (prior 6 months), avg6rev IVs: Mean monthly minutes (prior 6 months), avg6mou Mean monthly customer care calls, cc Mean monthly directory assistance calls, da Mean monthly overage minutes, ovrmou Household income (dollars), income Own home (Yes; No), own 4.4.2 Summarize Data Useful to examine data prior to specifying the model Summary Statistics Table 4.1: Summary Statistics (R code) Variable N Mean Std. Dev. Min Pctl. 25 Pctl. 75 Max avg6rev 2382 57.361 45.108 2 33 68 726 avg6mou 2382 466.913 487.389 0 143 615.75 5321 cc 2382 1.494 3.767 0 0 1 62.667 da 2382 0.803 2.098 0 0 0.743 47.52 ovrmou 2382 41.11 105.045 0 0 36.5 2239.25 income 2382 63.54 35.537 10.006 38.414 82.608 179.572 own 2382  No 709 29.8%  Yes 1673 70.2% Scatterplot Matrix (with Correlations) Figure 4.1: Scatterplot Matrix with Correlations (R code) 4.4.3 Model Specification Goal: Determine what behaviors and demographics are associated with high revenue customers IVs are expected to be ones that are related to revenue Model: \\(avg6rev=\\alpha+\\beta_1avg6mou+\\beta_2cc+\\beta_3da+\\beta_4ovrmou+\\beta_5income+\\beta_6own\\) 4.4.4 Model Interpretation 4.4.4.1 Results Table 4.2: Linear Regression Results (R code) Call: lm(formula = avg6rev ~ avg6mou + cc + da + ovrmou + income + own, data = telecom) Residuals: Min 1Q Median 3Q Max -152.446 -10.334 -1.500 7.361 288.252 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 31.421222 1.404375 22.374 &lt; 2e-16 *** avg6mou 0.048503 0.001378 35.209 &lt; 2e-16 *** cc -1.262511 0.149997 -8.417 &lt; 2e-16 *** da 1.924595 0.270331 7.119 1.43e-12 *** ovrmou 0.177919 0.005825 30.545 &lt; 2e-16 *** income -0.012768 0.014377 -0.888 0.37456 ownYes -4.086334 1.117271 -3.657 0.00026 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 24.74 on 2375 degrees of freedom Multiple R-squared: 0.6999, Adjusted R-squared: 0.6991 F-statistic: 923.2 on 6 and 2375 DF, p-value: &lt; 2.2e-16 \\(avg6rev=31.421+.049avg6mou-1.263cc+1.925da+.178ovrmou-.013income-4.086own\\) 4.4.4.2 Testing Overall Model Significance Relationship between DV and combined effects of IVs \\(H_0: \\text{all }\\beta_k=0\\) vs. \\(H_a: \\text{at least one }\\beta_k\\ne0\\) Use F-statistic to test Conclusion: With a F-statistic of \\(923.2\\) and a \\(p&lt;.001\\), we conclude that at least one \\(\\beta_k\\) is significant 4.4.4.3 Assessing overall model fit How much variation in the DV is explained by the model Use \\(R^2\\) to assess Use Adjusted \\(R^2\\) to compare models Conclusion: Based on the \\(R^2\\), about \\(70\\%\\) of the variance in avg6rev is explained by the model 4.4.4.4 Interpret Individual IVs Relationship between DV and each IV \\(H_0: \\beta_k=0\\) vs. \\(H_a: \\beta_k\\ne0\\) Interpret significant relationships avg6mou With \\(p&lt;.001\\), avg6mou has a significant effect on avg6rev. A one unit increase in avg6mou is predicted to increase avg6rev by \\(.049\\) units. cc With \\(p&lt;.001\\), cc has a significant effect on avg6rev. A one unit increase in cc is predicted to decrease avg6rev by \\(1.263\\) units. da With \\(p&lt;.001\\), da has a significant effect on avg6rev. A one unit increase in da is predicted to increase avg6rev by \\(1.925\\) units. ovrmou With \\(p&lt;.001\\), ovrmou has a significant effect on avg6rev. A one unit increase in ovrmou is predicted to increase avg6rev by \\(.178\\) units. own With \\(p&lt;.001\\), own has a significant effect on avg6rev. Those that own their home have \\(4.086\\) units less revenue per month than those that do not. A standardized \\(\\beta\\) is the effect of a single standard deviation change in the IV on the DV Higher absolute values are more important Conclusion: avg6mou is the biggest driver of avg6rev Table 4.3: Standardized Beta Coefficients (R code) Std.Beta (Intercept) 0 avg6mou 0.5241 cc -0.1054 da 0.0895 ovrmou 0.4143 income -0.0101 ownYes -0.0414 Sometimes helps to visually examine the IVs for interpretation Plots can show predicted DV at different levels of IVs Figure 4.2: Margin Plots for Significant IVs (R code) Figure 4.3: Margin Plots for Insignificant IV (R code) Examine deciles of predicted values by the IVs Split sample into 10 groups based on predicted DV Look at mean values of IVs for each decile Table 4.4: IVs by Predicted Deciles (R code) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-0413a336{}.cl-040b925e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-040b925f{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-040cca16{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-040cca17{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-040cca18{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} yhat.decavg6revavg6mouccdaovrmouincomeown1140.726891,441.113453.39495803.23101892234.016806762.128810.5420168279.76891889.084032.75490201.6354411770.810224168.835080.5924370365.66387659.974792.31792720.9421638645.981092466.315420.6344538453.73950482.932771.12184870.6104306724.861344565.677940.5924370550.24790382.634451.02661060.5494222715.829131664.910300.7016807645.14706296.781511.04621850.396207989.970588266.704660.6596639740.24790223.659660.84173670.263098745.385154158.926950.7352941836.64706149.878150.64425770.244380251.882352958.294290.7184874933.2343197.426780.58437940.121161091.909344561.432600.85774061028.4058648.949791.20781030.043493720.784518862.190470.9874477 4.4.5 Conclusion Recall our Goal: Determine what behaviors and demographics are associated with high revenue customers What did we learn? We can identify our highest revenue customers by examining avg6mou, cc, da, ovrmou and own Our highest revenue customers consumer over \\(1000\\) minutes per month and have over \\(200\\) overage minutes per month More directory assistance calls and fewer customer care calls are associated with higher revenue 4.5 Targeting Customers (Logistic Regression) 4.5.1 Overview Bank marketing data for customers of a bank DV: Open term deposit account, response IVs: Age, age Average Yearly Balance, balance Housing Loan (Yes, No), housing Personal Loan (Yes, No), loan Married (Yes, No), married Predict current customers likely to buy Use training (75%) and holdout (25%) samples 4.5.2 Estimation Results Table 4.5: Logistic Regression Estimation Results on Training Sample (R code) Call: glm(formula = response ~ age + balance + housing + loan + married, family = &quot;binomial&quot;, data = train) Deviance Residuals: Min 1Q Median 3Q Max -0.8039 -0.4760 -0.4084 -0.3311 2.7149 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.42502380 0.28108855 -8.627 &lt; 2e-16 *** age 0.02063143 0.00658090 3.135 0.00172 ** balance -0.00001342 0.00002360 -0.569 0.56962 housingYes -0.57512112 0.13736995 -4.187 0.0000283 *** loanYes -0.67663138 0.22339652 -3.029 0.00245 ** marriedYes -0.57891627 0.14211504 -4.074 0.0000463 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 1629.3 on 2652 degrees of freedom Residual deviance: 1580.0 on 2647 degrees of freedom AIC: 1592 Number of Fisher Scoring iterations: 5 Model p-value = 0.0000 McFadden&#39;s Pseudo-Rsquared = 0.0303 .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-04306656{}.cl-04287c84{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-0428a380{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-0428a381{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-0428f164{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0428f165{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0428f166{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0428f167{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0428f168{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0428f169{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} ParameterOR Estp2.5%97.5%(Intercept)0.08850.00000.05100.1535age1.02080.00171.00781.0341balance1.00000.56960.99991.0000housingYes0.56260.00000.42980.7365loanYes0.50830.00250.32810.7876marriedYes0.56050.00000.42420.7405 4.5.3 Overall Model Fit Based on the likelihood ratio test with p-value &lt; .0001, the overall model is significant McFaddens Pseudo-\\(R^2\\) of .030 means that the model explains only about 3% of variation between buyers/non-buyers Classification Matrix for Training Sample Whats the problem? Table 4.6: Classification Matrix for Training Sample (R code) Confusion Matrix and Statistics Reference Prediction No Yes No 2409 244 Yes 0 0 Accuracy : 0.908 95% CI : (0.8964, 0.9188) No Information Rate : 0.908 P-Value [Acc &gt; NIR] : 0.517 Kappa : 0 Mcnemar&#39;s Test P-Value : &lt;2e-16 Sensitivity : 0.00000 Specificity : 1.00000 Pos Pred Value : NaN Neg Pred Value : 0.90803 Prevalence : 0.09197 Detection Rate : 0.00000 Detection Prevalence : 0.00000 Balanced Accuracy : 0.50000 &#39;Positive&#39; Class : Yes PCC = 83.30% Sensitivity/Specificity Plot Figure 4.4: Sensitivity/Specificity Plot for Training Sample (R code) Classificaiton Matrix for Holdout Sample Table 4.7: Classification Matrix for Training Sample with 0.1 Cutoff (R code) Confusion Matrix and Statistics Reference Prediction No Yes No 1540 107 Yes 869 137 Accuracy : 0.6321 95% CI : (0.6134, 0.6505) No Information Rate : 0.908 P-Value [Acc &gt; NIR] : 1 Kappa : 0.0835 Mcnemar&#39;s Test P-Value : &lt;2e-16 Sensitivity : 0.56148 Specificity : 0.63927 Pos Pred Value : 0.13618 Neg Pred Value : 0.93503 Prevalence : 0.09197 Detection Rate : 0.05164 Detection Prevalence : 0.37919 Balanced Accuracy : 0.60037 &#39;Positive&#39; Class : Yes PCC = 83.30% Classification Matrix for Holdout Sample Results very similar for holdout sample Table 4.8: Classification Matrix for Holdout Sample with 0.1 Cutoff (R code) Confusion Matrix and Statistics Reference Prediction No Yes No 527 37 Yes 275 44 Accuracy : 0.6467 95% CI : (0.6141, 0.6782) No Information Rate : 0.9083 P-Value [Acc &gt; NIR] : 1 Kappa : 0.0863 Mcnemar&#39;s Test P-Value : &lt;2e-16 Sensitivity : 0.54321 Specificity : 0.65711 Pos Pred Value : 0.13793 Neg Pred Value : 0.93440 Prevalence : 0.09173 Detection Rate : 0.04983 Detection Prevalence : 0.36127 Balanced Accuracy : 0.60016 &#39;Positive&#39; Class : Yes PCC = 83.34% ROC Curve for Holdout Sample Area between \\(.5\\) and \\(.7\\) suggests poor model fit Figure 4.5: ROC Curve for Test/Holdout Sample (R code) 4.5.4 Interpreting Coefficients age is positive (\\(OR&gt;1\\)) and significant (\\(p=.0017\\)) \\(1\\) year increase in age increases odds of buying by a factor of \\(1.021\\) (or odds of buying increase by \\(2.1\\%\\)) married is negative (\\(OR&lt;1\\)) and significant (\\(p&lt;.0001\\)) Being married decreases odds of buying by factor of \\(.56\\) (or odds of buying decrease by \\(44\\%\\)) housing is negative (\\(OR&lt;1\\)) and significant (\\(p&lt;.0001\\)) Having a home loan decreases odds of buying by factor of \\(.56\\) (or odds of buying decrease by \\(44\\%\\)) loan is negative (\\(OR&lt;1\\)) and significant (\\(p=.0025\\)) Having a personal loan decreases odds of buying by factor of \\(.50\\) (or odds of buying decrease by \\(50\\%\\)) 4.5.5 Interpreting Coefficients Visually Figure 4.6: Margin Plots for Age at Different Factor Levels (R code) 4.5.6 Gain Chart Contacting top \\(20\\%\\) of predicted buyers yields about \\(40\\%\\) of actual buyers Contacting top \\(30\\%\\) of predicted buyers yields about \\(52\\%\\) of actual buyers Figure 4.7: Gain Chart (R code) 4.5.7 Lift Chart Contacting top \\(20\\%\\) of predicted buyers provides a lift of about \\(2\\) Figure 4.8: Lift Chart (R code) 4.5.8 Conclusion Recall our goal: Predict current customers likely to buy What did we learn? We can identify those more likely to buy by examining age, married, housing, and loan By targeting those customers that are more likely to purchase, we can better spend our limited resources 4.6 Suggested Readings Marketing Data Science (2015). Miller, Thomas W. BGSU Library Link:http://maurice.bgsu.edu:2083/record=b41416968~S0 eBook through BGSU Library:https://learning.oreilly.com/library/view/marketing-data-science/9780133887662/?ar=Note: Might need to create an account; select Not Listed. Click here from the Select your institution drop down box. Use your BGSU email to create the account. Chapter 3: Targeting Current Customers 4.7 R Code Figure 4.1 # Need to detach package &#39;cowplot&#39; to prevent an error detach(&quot;package:cowplot&quot;, unload = TRUE) ggpairs(telecom[,1:6], # Dataset lower=list(continuous= wrap(&quot;smooth&quot;, method=&quot;lm&quot;, se=FALSE, # Add fit line color=&quot;midnightblue&quot;)), # Set dot color diag=list(continuous=&quot;blankDiag&quot;)) # Set diagonals to be blank # Reload &#39;cowplot&#39; library(cowplot) Figure 4.2 # Want to predict for different levels of IVs # Because we often do this for other continuous variables, we first # create a df with mean values of all continuous IVs # at different levels of the factor variable (if there are any) # NOTE: variable names must be EXACTLY the same as in model m.ivs &lt;- telecom %&gt;% group_by(own) %&gt;% summarise(avg6mou=mean(avg6mou, na.rm=TRUE), cc=mean(cc, na.rm=TRUE), da=mean(da, na.rm=TRUE), ovrmou=mean(ovrmou, na.rm=TRUE), income=mean(income, na.rm=TRUE)) # Create dataframe for prediction at different levels of &#39;avg6mou&#39; a6m.pred &lt;- merge(data.frame(avg6mou=seq(0,1500,100)), # variable of interest m.ivs[,c(&quot;cc&quot;, &quot;da&quot;, &quot;ovrmou&quot;, &quot;income&quot;, &quot;own&quot;)]) # Use model results to predict dv for values in &#39;a6m.pred&#39; data frame # Predicted values and CIs get appended to data frame # New variables are called: # &#39;p$fit&#39; = linear prediction from model # &#39;p$lwr&#39; = lower confidence interval for prediction # &#39;p$upr&#39; = upper confidcent interval for prediction a6m.pred$p &lt;- as.data.frame( predict.lm(target, a6m.pred, interval=&quot;confidence&quot;)) # Use ggplot to create margin plot p1 &lt;- ggplot(aes(x=avg6mou, y=p$fit, color=own), data=a6m.pred) + geom_line(size=1) + geom_ribbon(aes(ymin=p$lwr, ymax=p$upr, fill=own), alpha=0.2) + labs(x=&quot;Mean Monthly Minutes&quot;, y=&quot;Linear Prediction&quot;, fill=&quot;Own&quot;, color=&quot;Own&quot;) + scale_y_continuous(limits=c(30,120), breaks=seq(30,120,15), minor_breaks=NULL) + theme(legend.position=&quot;none&quot;) # Repeat for variable &#39;cc&#39; cc.pred &lt;- merge(data.frame(cc=seq(0,10,1)), # variable of interest m.ivs[,c(&quot;avg6mou&quot;, &quot;da&quot;, &quot;ovrmou&quot;, &quot;income&quot;, &quot;own&quot;)]) cc.pred$p &lt;- as.data.frame(predict.lm(target, cc.pred, interval=&quot;confidence&quot;)) p2 &lt;- ggplot(aes(x=cc, y=p$fit, color=own), data=cc.pred) + geom_line(size=1) + geom_ribbon(aes(ymin=p$lwr, ymax=p$upr, fill=own), alpha=0.2) + labs(x=&quot;Mean Monthly Customer Care Calls&quot;, y=&quot;Linear Prediction&quot;) + scale_y_continuous(limits=c(30,120), breaks=seq(30,120,15), minor_breaks=NULL) + theme(legend.position=&quot;none&quot;) # Repeat for variable &#39;da&#39; da.pred &lt;- merge(data.frame(da=seq(0,10,1)), # variable of interest m.ivs[,c(&quot;cc&quot;, &quot;avg6mou&quot;, &quot;ovrmou&quot;, &quot;income&quot;, &quot;own&quot;)]) da.pred$p &lt;- as.data.frame(predict.lm(target, da.pred, interval=&quot;confidence&quot;)) p3 &lt;- ggplot(aes(x=da, y=p$fit, color=own), data=da.pred) + geom_line(size=1) + geom_ribbon(aes(ymin=p$lwr, ymax=p$upr, fill=own), alpha=0.2) + labs(x=&quot;Mean Monthly Directory Assistance Calls&quot;, y=&quot;Linear Prediction&quot;) + scale_y_continuous(limits=c(30,120), breaks=seq(30,120,15), minor_breaks=NULL) + theme(legend.position=&quot;none&quot;) # Repeat for variable &#39;ovrmou&#39; om.pred &lt;- merge(data.frame(ovrmou=seq(0,300,25)), # variable of interest m.ivs[,c(&quot;cc&quot;, &quot;da&quot;, &quot;avg6mou&quot;, &quot;income&quot;, &quot;own&quot;)]) om.pred$p &lt;- as.data.frame(predict.lm(target, om.pred, interval=&quot;confidence&quot;)) p4 &lt;- ggplot(aes(x=ovrmou, y=p$fit, color=own), data=om.pred) + geom_line(size=1) + geom_ribbon(aes(ymin=p$lwr, ymax=p$upr, fill=own), alpha=0.2) + labs(x=&quot;Mean Monthly Overage Minutes&quot;, y=&quot;Linear Prediction&quot;) + scale_y_continuous(limits=c(30,120), breaks=seq(30,120,15), minor_breaks=NULL) + theme(legend.position=&quot;none&quot;) # Save Plots to a grid object using &#39;cowplot&#39; pgrid &lt;- plot_grid(p1, p2, p3, p4, nrow=2) # Extract legend from first plot legend &lt;- get_legend(p1 + theme(legend.position=&quot;bottom&quot;)) # Combine plot object and legend in a single column plot_grid(pgrid, legend, ncol=1, rel_heights=c(1,.1)) Figure 4.3 # Create dataframe for prediction at different levels of &#39;avg6mou&#39; inc.pred &lt;- merge(data.frame(income=seq(10,200,10)), # variable of interest m.ivs[,c(&quot;cc&quot;, &quot;da&quot;, &quot;ovrmou&quot;, &quot;avg6mou&quot;, &quot;own&quot;)]) # Use model results to predict dv for values in &#39;a6m.pred&#39; data frame # Predicted values and CIs get appended to data frame # New variables are called: # &#39;p$fit&#39; = linear prediction from model # &#39;p$lwr&#39; = lower confidence interval for prediction # &#39;p$upr&#39; = upper confidcent interval for prediction inc.pred$p &lt;- as.data.frame( predict.lm(target, inc.pred, interval=&quot;confidence&quot;)) # Use ggplot to create margin plot ggplot(aes(x=income, y=p$fit, color=own), data=inc.pred) + geom_line(size=1) + geom_ribbon(aes(ymin=p$lwr, ymax=p$upr, fill=own), alpha=0.2) + labs(x=&quot;Income (000s)&quot;, y=&quot;Linear Prediction&quot;, fill=&quot;Own&quot;, color=&quot;Own&quot;) + scale_y_continuous(limits=c(30,120), breaks=seq(30,120,15), minor_breaks=NULL) + theme(legend.position=&quot;bottom&quot;) Figure 4.6 # Create df will mean values of continuous IVs at levels of factor IVs m.ivs &lt;- train %&gt;% group_by(housing, loan, married) %&gt;% summarise(age=mean(age, na.rm=TRUE), balance=mean(balance, na.rm=TRUE)) # Create new data for prediction with &#39;age&#39; as focus age.pred &lt;- merge(data.frame(age=seq(15,85,5)), m.ivs[ ,c(&quot;housing&quot;, &quot;loan&quot;, &quot;married&quot;, &quot;balance&quot;)]) # Create data frame with predicted values and confidence bands train.pred &lt;- cbind(age.pred, # &#39;cbind&#39; combines objects by columns predict(tgt.log, # Model to predict values with age.pred, # New data to use for IVs type=&quot;link&quot;, # Return log-odds predictions se=TRUE)) %&gt;% # Get std.err. for CIs mutate(pred=plogis(fit), # Calculate pr(response) upr=plogis(fit+(qnorm(0.975)*se.fit)), # Upper CI lwr=plogis(fit-(qnorm(0.975)*se.fit))) # Lower CI # Create plot grid with separate lines for &#39;housing&#39; ggplot(aes(x=age, y=pred), data=train.pred) + geom_line(aes(color=housing), size=1) + geom_ribbon(aes(ymin=lwr, ymax=upr, color=housing, fill=housing), alpha=.2) + scale_y_continuous(&quot;Pr(Response)&quot;,limits=c(0,.5)) + theme(legend.position=&quot;bottom&quot;) + # &#39;facet-grid&#39; creates separate plots in a grid of factor variable levels facet_grid(rows=vars(married), cols=vars(loan), labeller=labeller(married=as_labeller(c(&quot;No&quot;=&quot;Married: No&quot;, &quot;Yes&quot;=&quot;Married: Yes&quot;)), loan=as_labeller(c(&quot;No&quot;=&quot;Personal Loan: No&quot;, &quot;Yes&quot;=&quot;Personal Loan: Yes&quot;))), switch=&quot;y&quot;) + labs(x=&quot;Age&quot;, color=&quot;Housing Loan&quot;, fill=&quot;Housing Loan&quot;) Figure 4.7 # Load user-defined object &#39;gainlift&#39; source(&quot;Topic03/gainlift.R&quot;) # Call the function and assign to object named &#39;glresults&#39; glresults &lt;- gainlift(tgt.log, # Name of the glm results object train, # Name of the training data frame test, # Name of the testing data frame &quot;Yes&quot;) # Level that represents success/true # Output gainplot glresults$gainplot Figure 4.8 # Output liftplot glresults$liftplot Table 4.1 summary(telecom) Table 4.2 target &lt;- lm(avg6rev ~ avg6mou + cc + da + ovrmou + income + own, data=telecom) summary(target) Table ?? # Use user-defined function &#39;lm_beta.R&#39; source(&quot;Topic02/lm_beta.R&quot;) htmlTable(lm_beta(target, digits=4)) Table 4.4 telecom %&gt;% cbind(., yhat=fitted(target)) %&gt;% # Append fitted values to data mutate(yhat.dec=11-ntile(yhat, 10), # Create deciles, but reverse order own=as.numeric(own)-1) %&gt;% # Covert own to 1=yes, 0=no group_by(yhat.dec) %&gt;% # Group by decile summarise(across(avg6rev:own, ~mean(.x, na.rm=TRUE))) %&gt;% # Calculate mean of each IV flextable() # Nice table Table 4.5 # Use &#39;caret&#39; package to create training and test/holdout samples library(caret) # This will create two separate dataframes: train and test set.seed(9999) inTrain &lt;- createDataPartition(y=bankmktg$response, p=.75, list=FALSE) train &lt;- bankmktg[inTrain,] test &lt;- bankmktg[-inTrain,] # Estimate using training sample tgt.log &lt;- glm(response ~ age + balance + default + housing + educ + loan + married, data=train, family=&quot;binomial&quot;) summary(tgt.log) # Get p-value p &lt;- with(tgt.log, pchisq(null.deviance - deviance, df.null-df.residual, lower.tail=FALSE)) cat(&quot;Model p-value =&quot;,sprintf(&quot;%.4f&quot;,p),&quot;\\ &quot;) # Calculate McFadden&#39;s R-sq Mrsq &lt;- 1-tgt.log$deviance/tgt.log$null.deviance cat(&quot;McFadden&#39;s Pseudo-Rsquared = &quot;, round(Mrsq, digits=4)) # Use the &#39;or_table.R&#39; user-defined script to get Odds Ratio estimates source(&quot;Topic03/or_table.R&quot;) flextable(or_table(tgt.log)) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
