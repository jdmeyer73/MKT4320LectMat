[["index.html", "MKT 4320: Marketing Analytics Lecture Materials Introduction", " MKT 4320: Marketing Analytics Lecture Materials Jeffrey Meyer 2024-01-02 Introduction This web-book will serve as your copies of the lecture slides. Because the software used in this class is R and RStudio, traditional lecture slides for students are not useful, because I want to provide the code that generated the output seen on the slides. Each topic covered is a “chapter” in this web-book and will begin with a motivation section, followed by the R packages and datasets used in that topic. The sections after that will provide an web version of the material on the lecture slides used in class. "],["examining-and-summarizing-data.html", "Topic 1 Examining and Summarizing Data 1.1 Motivation 1.2 R Packages and Datasets for Topic 1 1.3 Describing Data 1.4 Suggested Readings 1.5 R Code", " Topic 1 Examining and Summarizing Data 1.1 Motivation After data preparation, examining and summarizing the data provides the analyst with a “feel” for the data Distributions of variables Relationships between variables Missing observations Coding of variables 1.2 R Packages and Datasets for Topic 1 library(ggplot2) # Advanced graphing capabilities library(dplyr) # Easier programming library(tidyr) # Easier programming library(scales) # Control appearance of axes and legend labels library(questionr) # Easier frequency tables library(htmlTable) # Better HTML Tables library(mosaic) # Statistical functions library(sjPlot) # Easily create cross-tabs library(MKT4320BGSU) data(airlinesat) 1.3 Describing Data How we examine and summarize data depends on: Type of data Nominal Ordinal Continuous Number of variables Univariate Bivariate 1.3.1 Univariate - Graphs and Tables 1.3.1.1 Bar Chart Primarily for nominal/ordinal data Displays each category’s… Frequency (usually) Centrality Dispersion Figure 1.1: Bar Chart R Code 1.3.1.2 Histogram Non-overlapping categories of equal width from continuous data Shows frequency in each category Used to examine distribution of variable Figure 1.2: Historgram 1 R Code Figure 1.3: Histogram 2 R Code 1.3.1.3 Box Plot Displays distribution of continuous data Conveys dispersion information Wider box = More dispersion Can help identify potential outliers How to interpret: Box in middle is the Interquartile Range Q3 (75th percentile) - Q1 (25th percentile) Line in middle is the median Upper/lower lines are upper/lower adjacent values Upper adjacent value is the largest observation that is smaller than Q3 + 1.5*IQR Lower adject value is the smallest observatoin that is larger than Q1 - 1.5*IQR Any observations above (below) the upper (lower) adjacent value are plotted separately, and could be outliers Figure 1.4: Box Plot R Code 1.3.1.4 Frequency Table Displays counts and percentages for categorical variables Similar to bar chart, but in table form Table 1.1: Frequency Table n % val% %cum val%cum Blue 677 63.6 63.6 63.6 63.6 Gold 143 13.4 13.4 77 77 Silver 245 23 23 100 100 Total 1065 100 100 100 100 R Code 1.3.2 Univariate - Statistics 1.3.2.1 Measures of Centrality Values of a “typical” or “average” score Mean is the sum of all observations divided by the number of observations Only appropriate for continuous data Median separates highest and lowers 50% of observations Cannot be used on categorical data Table 1.2: Measures of Centrality and Dispersion age s1 Min. : 19.00 Min. : 1.00 1st Qu.: 42.00 1st Qu.: 46.00 Median : 50.00 Median : 58.00 Mean : 50.42 Mean : 60.91 3rd Qu.: 58.00 3rd Qu.: 84.00 Max. :101.00 Max. :100.00 NA’s :27 R Code 1.3.2.2 Measures of Dispersion Provide info about variability in the data Range is the highest minus the lowest observation Simple, but includes extreme values Not appropriate for categorical data Interquartile Range (IQR) is Q3 (75th percentile) - Q1 (25th percentile) Used in the Box Plot Not appropriate for categorical data Standard Deviation Given by Equation 1.1 below \\[\\begin{equation} s = \\sqrt{\\frac{\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}}{n-1}} \\tag{1.1} \\end{equation}\\] Only appropriate with continuous data Table 1.3 Measures of Centrality and Dispersion For (A) ‘Age’ and (B) ‘Airline gets me there on time (Satisfaction)’ (A) min Q1 median Q3 max mean sd n missing 19 42 50 58 101 50.4197183098592 12.2746371060814 1065 0 (B) min Q1 median Q3 max mean sd n missing 1 46 58 84 100 60.9113680154143 26.0222411706285 1038 27 R Code 1.3.3 Bivariate - Graphs and Tables 1.3.3.1 Scatterplots Graphically shows how two continuous variables are related If dots appear in to follow a line, variables are likely related (see Figure 1.5) If dots appear random, variables are likely not related (see Figure 1.6) Not appropriate for categorical data (see Figure 1.7) Figure 1.5: Scatterplot 1 with Fitted Line R Code Figure 1.6: Scatterplot 2 without Fitted Line R Code Figure 1.7: Scatterplot of Categorical Data R Code 1.3.3.2 Crosstabs Show if and how two categorical variables are related Common to put “DV” in rows and “IV” in columns Can ask for \\(\\chi^2\\) to test if for significant association Can also view it visually with a stacked bar chart (see Figure 1.8) Percentages represent column percentages Can also view it visually with separate bars for each category (see Figure 1.9) Bar height is percent of total Table 1.4: Crosstab flight_purpose flight_type Total Domestic International Business 33059.1 % 19538.5 % 52549.3 % Leisure 22840.9 % 31261.5 % 54050.7 % Total 558100 % 507100 % 1065100 % χ2=44.619 · df=1 · φ=0.207 · p=0.000 R Code Figure 1.8: Stacked Bar Chart R Code Figure 1.9: Side-by-Side Bar Chart R Code 1.3.3.3 Box Plot Displays distribution of continuous data across classes of a categorical variable Figure 1.10: Box Plot by Category R Code 1.3.3.4 Bar Chart Displays mean (or some other value) of continuous data across classes of a categorical variable Figure 1.11: Bar Chart by Category R Code 1.3.4 Statistics 1.3.4.1 Correlation Provides a measure of linear association between two continuous variables Given by Equation 1.2 below \\[\\begin{equation} r = \\frac{\\sum_{i=1}^{n}{(x_i - \\bar{x})(y_i-\\bar{y})}}{(n-1)s_xs_y} \\tag{1.2} \\end{equation}\\] \\(-1 \\le r \\le 1\\) Table 1.5: Correlation Matrix   age nflights e7 s11 age         nflights -0.116***       e7 -0.034 -0.063*     s11 0.170*** -0.106*** 0.240***   Computed correlation used pearson-method with pairwise-deletion. R Code 1.4 Suggested Readings R for Marketing Research and Analytics. 2nd Edition (2019). Chapman, Chris; McDonnel Feit, Elea BGSU Library Link:http://maurice.bgsu.edu/record=b4966554~S9 eBook through BGSU Library:https://link-springer-com.ezproxy.bgsu.edu/book/10.1007%2F978-3-030-14316-9 Chapter 3: Describing Data Chapter 4: Relationships Between Continuous Variables Chapter 5: Tables and Visualization OpenIntro Statistics. 4th Edition (2019). Diez, David; Cetinkaya-Rundel, Mine; Barr, Christopher D. Available at OpenIntro.org:https://www.openintro.org/book/os/ Summarizing Data Multivariate Data Analysis. Hair, Joseph F.; Black, William C.; Babin, Barry J.; Anderson, Rolph E. 7th Edition: Search for “multivariate data analysis 7th edition hair” Graphical Examination of the Data (pp. 34-40) 5th Edition: Course reserves Graphical Examination of the Data (pp. 40-46) 1.5 R Code 1.5.1 Table1.1 See Table 1.1 # Create frequency table using questionr::freq and pass result # htmlTable freq(airlinesat$status, cum=TRUE, total=TRUE) %&gt;% htmlTable() 1.5.2 Table1.2 See Table 1.2 airlinesat %&gt;% # Dataset # Select variables; Use &#39;dplyr::&#39; before &#39;select&#39; to avoid conflicts dplyr::select(age, s1) %&gt;% # Request summary statistics summary() %&gt;% # Create htmlTable htmlTable() 1.5.3 Table1.3 See Table 1.3 # Request htmlTable for summary statistics with rounding two 2 digits # for &#39;age&#39; htmlTable(txtRound(favstats(airlinesat$age),2), caption=&quot;Age&quot;) # Request htmlTable for summary statistics with rounding two 2 digits # for &#39;s1&#39; htmlTable(txtRound(favstats(airlinesat$s1),2), caption=&quot;Airline Gets me there on time (Satisfaction)&quot;) 1.5.4 Table1.4 See Table 1.4 # Use package sjPlot to easily create cross-tab # Note: sjPlot::tab_xtab not available in virtual environment tab_xtab(var.row=airlinesat$flight_purpose, # Set row variable var.col=airlinesat$flight_type, # Set column variable show.col.prc=TRUE) # Request column percentages 1.5.5 Table1.5 See Table 1.5 # Create dataframe of variables to include corrvars &lt;- airlinesat %&gt;% select(age, nflights, e7, s11) # Use package sjPlot to easily create correlation matrix # Note: sjPlot not available in virtual environment tab_corr(corrvars, na.deletion = &quot;pairwise&quot;, # Delete obs if either variable is missing corr.method = &quot;pearson&quot;, # Choose Pearson correlation coefficient show.p = TRUE, # Show asterisks for significant correlations digits = 3, # Show three decimal points triangle = &quot;lower&quot;, # Show only lower triangle fade.ns=FALSE) # Do not fade insignficant correlations # Note: For vitual environment, use package Hmisc to produce separate # tables for correlation coefficients and p-values htmlTable(txtRound(rcorr(as.matrix(corrvars))[[&quot;r&quot;]],3)) htmlTable(txtRound(rcorr(as.matrix(corrvars))[[&quot;P&quot;]],3)) 1.5.6 Figure 1.1 See Figure 1.1 airlinesat %&gt;% # Dataset # Groups dataset by variable &#39;status&#39; group_by(status) %&gt;% # Creates new variable &#39;n&#39; equal to number of obs for each status summarize(n=n()) %&gt;% # Creates new variable &#39;prop&#39; equal to proportion of same for each status mutate(prop=n/sum(n)) %&gt;% # Begins plot with &#39;status&#39; on x axis and &#39;prop&#39; on y axis ggplot(aes(x=status, y=prop)) + # Requests bar chart as the geom function, using actual value # of variable &#39;prop&#39; as the statistic to plot geom_bar(stat=&quot;identity&quot;) + # Adds data labels to the end of the bars geom_text(aes(label=sprintf(&quot;%1.1f%%&quot;, prop*100)), vjust=1.5, color=&quot;white&quot;) + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Status&quot;,y=&quot;Proportion&quot;) 1.5.7 Figure 1.2 See Figure 1.2 airlinesat %&gt;% # Dataset # Begins plot with &#39;age&#39; as variable to plot on x axis ggplot(aes(x=age)) + # Requests histogram as the geom function, with binwidth of 2, # y axis to represent density, and outline/color of bars geom_histogram(binwidth=2, aes(y=..density..), color=&quot;black&quot;, fill=&quot;tan&quot;) + # Creates normal curve overlay stat_function(fun=function(x) dnorm(x, mean=mean(airlinesat$age),sd=sd(airlinesat$age))) + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Age&quot;, y=&quot;Density&quot;) 1.5.8 Figure 1.3 See Figure 1.3 airlinesat %&gt;% # Dataset # Drops ros with missing values for variable &#39;s1&#39; drop_na(s1) %&gt;% # Begins plot with &#39;s1&#39; as variable to plot on x axis ggplot(aes(x=s1)) + # Requests histogram as the geom function, with binwidth of 2, # y axis to represent density, and outline/color of bars geom_histogram(binwidth=2, aes(y=..density..), color=&quot;black&quot;, fill=&quot;tan&quot;) + # Creates normal curve overlay stat_function(fun=function(x) dnorm(x, mean=mean(airlinesat$s1, na.rm=TRUE), sd=sd(airlinesat$s1, na.rm=TRUE))) + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Airline gets me there on time (Satisfaction)&quot;, y=&quot;Density&quot;) 1.5.9 Figure 1.4 See Figure 1.4 airlinesat %&gt;% # Dataset # Begins plot with no grouping variable and age as continuous variable ggplot(aes(x=&quot;&quot;, y=age)) + # Request boxplot as the geom function geom_boxplot() + # Adds the whiskers to the boxplot stat_boxplot(geom=&#39;errorbar&#39;) + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;&quot;, y=&quot;Age&quot;) 1.5.10 Figure 1.5 See Figure 1.5 airlinesat %&gt;% # Passes dataset to ggplot # Begins plot with x (s18) and y (s17) variables ggplot(aes(x=s18, y=s17)) + # Requests scatter plot geom_point() + # Requests linear regression fitted line without confidence interval geom_smooth(method=&quot;lm&quot;, se=FALSE) + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Employees are service-oriented (s18)&quot;, y=&quot;Employees are friendly (s17)&quot;) 1.5.11 Figure 1.6 See Figure 1.6 airlinesat %&gt;% # Passes dataset to ggplot # Begins plot with x (age) and y (s11) variables ggplot(aes(x=age, y=s11)) + # Requests scatter plot geom_point() + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Age&quot;, y=&quot;Aircraft interior is well maintained (s11)&quot;) 1.5.12 Figure 1.7 See Figure 1.7 airlinesat %&gt;% # Passes dataset to ggplot # Begins plot with x (age) and y (s11) variables ggplot(aes(x=flight_type, y=flight_purpose)) + # Requests scatter plot geom_point() + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Flight Type&quot;, y=&quot;Flight Purpose&quot;) 1.5.13 Figure 1.8 See Figure 1.8 airlinesat %&gt;% # Dataset # Groups dataset by crosstab variables group_by(flight_type, flight_purpose) %&gt;% # Creates new variable &#39;n&#39; for count of observations in each cell summarise(n=n()) %&gt;% # Creates column percentages mutate(prop=n/sum(n)) %&gt;% # Begins plot with &#39;flight_type&#39; on x, &#39;prop&#39; on y, and color of the # fill in the bars based on&#39;flight_purpose&#39; ggplot(aes(x=flight_type, y=prop, fill=flight_purpose)) + # Requests bar chart as the geom function, positioning the # location of the bars based on the fill variable geom_bar(position=&quot;fill&quot;, stat=&quot;identity&quot;) + # Labels y-axis using percentages scale_y_continuous(labels=scales::label_percent()) + # Adds data labels to middle of bars geom_text(aes(label=sprintf(&quot;%1.1f%%&quot;, prop*100)), position=position_stack(vjust=0.5), color=&quot;white&quot;) + # Changes test size to be larger theme(text=element_text(size=15)) + # Adds axis and legend labels labs(x=&quot;Flight Type&quot;, y=&quot;Percent&quot;, fill=&quot;Flight Purpose&quot;) 1.5.14 Figure 1.9 See Figure 1.9 airlinesat %&gt;% # Dataset # Groups dataset by crosstab variables group_by(flight_type, flight_purpose) %&gt;% # Creates new variable &#39;n&#39; for count of observations in each cell, but # drops grouping structure to get total percentages summarise(n=n(), .groups=&quot;drop&quot;) %&gt;% # Creates total percentages mutate(prop=n/sum(n)) %&gt;% # Begins plot with &#39;flight_type&#39; on x, &#39;prop&#39; on y, and color of the # fill in the bars based on&#39;flight_purpose&#39; ggplot(aes(x=flight_type, y=prop, fill=flight_purpose)) + # Requests bar chart as the geom function, positioning the # location of the bars to be side-by-side (dodge) geom_bar(position=&quot;dodge&quot;, stat=&quot;identity&quot;) + # Labels y-axis using percentages scale_y_continuous(labels=scales::label_percent()) + # Adds data labels to end of bars geom_text(aes(label=sprintf(&quot;%1.1f%%&quot;, prop*100)), vjust=1.5, position=position_dodge(width=.9), color=&quot;white&quot;) + # Changes test size to be larger theme(text=element_text(size=15)) + # Adds axis and legend labels labs(x=&quot;Flight Type&quot;, y=&quot;Percent&quot;, fill=&quot;Flight Purpose&quot;) 1.5.15 Figure 1.10 See Figure 1.10 airlinesat %&gt;% # Dataset # Begins plot with &#39;status&#39; as grouping variable and # &#39;age&#39; as continuous variable ggplot(aes(x=status, y=age)) + # Request boxplot as the geom function geom_boxplot() + # Adds the whiskers to the boxplot stat_boxplot(geom=&#39;errorbar&#39;) + # Changes text size to be larger theme(text=element_text(size=15)) + # Adds axis labels labs(x=&quot;Status&quot;, y=&quot;Age&quot;) 1.5.16 Figure 1.11 See Figure 1.11 airlinesat %&gt;% # Dataset # Groups dataset by &#39;crosstab variables&#39;status&#39; group_by(status) %&gt;% # Creates new variable &#39;mean&#39; for mean of &#39;age&#39; by &#39;status&#39; summarise(mean=mean(age)) %&gt;% # Begins plot with &#39;status&#39; on x, &#39;mean&#39; on y ggplot(aes(x=status, y=mean)) + # Requests bar chart as the geom function, plotting the actual # value (&#39;identity&#39;), and setting fill color to match status geom_bar(stat=&quot;identity&quot;, fill=c(&quot;midnightblue&quot;,&quot;gold&quot;,&quot;gray&quot;)) + # Sets number of breaks on y-axis scale_y_continuous(n.breaks=6) + # Adds data labels to outside end of bars geom_text(aes(label=sprintf(&quot;%.2f&quot;, mean)), vjust=-.5, position=position_dodge(width=.9), color=&quot;black&quot;) + # Changes test size to be larger theme(text=element_text(size=15)) + # Adds axis and legend labels labs(x=&quot;Status&quot;, y=&quot;Mean of Age&quot;) "],["linear-regression.html", "Topic 2 Linear Regression 2.1 Motivation 2.2 R Packages and Datasets for Topic 2 2.3 Understanding Regression Analysis 2.4 Conducting Linear Regression 2.5 Linear Regression Example 2.6 Categorical IVs 2.7 Categorical IVs Example 2.8 Suggested Readings 2.9 R Code", " Topic 2 Linear Regression 2.1 Motivation Regression allows marketers to: Understand relationships between a dependent variable and one or more independent variables Determine the relative strength of different independent variables Make predictions 2.2 R Packages and Datasets for Topic 2 The following packages are used for this topic and are loaded when needed. library(ggplot2) # Advanced graphing capabilities library(tidyr) # Easier programming library(htmlTable) # Better HTML Tables library(reshape2) # Easily convert wide data to long data library(summarytools) # Summary statistics library(effects) # Help with linear predictions library(cowplot) # Arrange separate plots in a grid library(ggtext) # Annotate ggplots library(lubridate) # Easily work with dates library(jtools) # Concise regression results library(dplyr) # Easier programming library(GGally) # Scatterplot Matrix library(MKT4320BGSU) The following datasets are used for this topic and are loaded now. load(&quot;Topic02/advtsales.rdata&quot;) load(&quot;Topic02/deptstoresales.rdata&quot;) Download advtsales.rdata Download deptstoresales.rdata 2.3 Understanding Regression Analysis Regression notation: \\(y = \\alpha + \\beta_kx_k+\\varepsilon\\)        where \\(y\\) is the dependent variable (DV) \\(x_k\\) is the \\(k\\)th independent variable (IV) \\(\\alpha\\) is the constant or \\(y\\)-intercept \\(\\beta_k\\) is the regression coefficient for the \\(k\\)th IV \\(\\varepsilon\\) is the error term Objective Predict DV based on knowledge of the IV(s) Method OLS creates the “best” fitted line by minimizing the sum of the squared residuals OLS Minimizes Equation 2.1 below: \\[\\begin{equation} \\sum_{i=1}^{n}{(y_i - \\hat{y}_i)^2} \\tag{2.1} \\end{equation}\\] Note: The “best” fitted regression line is not always the line that best represents the data 2.4 Conducting Linear Regression 2.4.1 Check Data Requirements Continuous DV Must be measured on an interval or ratio scale For nominal scale, use logistic regression For ordinal scale, use ordinal regression 2.4.2 Model Specification Pick IVs based on… Conceptual grounding Availability of data Including irrelevant IVs… Reduces parsimony May mask effects of other IVs Makes testing significance less precise Excluding relevant IVs… Seriously biases results Negatively affects interpretation 2.4.3 Model Estimation Estimately is typically done using OLS All statistical packages can conduct regression R lm(dv ~ iv1 + iv2 + … + ivk) Stata GUI Statistics &gt; Linear models and related &gt; Linear Regression Stata Command regress dv iv1 iv2 … ivk SPSS GUI Analyze &gt; Regression &gt; Linear SPSS Syntax regression/dependent dv/enter iv1 iv2 … ivk SAS proc reg; model dv = iv1 iv2 … ivk; Minitab Stat &gt; Regression &gt; Regression 2.4.4 Model Interpetation Assessing Overall Model Fit How much variation in the DV is explained by the model Individual Independent Attributes Relationship between DV and each IV \\(H_0:\\beta_k=0\\) vs. \\(H_a:\\beta_k\\ne0\\) Interpret significant relationships Relative strength of IVs 2.4.5 Model Prediction Prediction is a key use of regression Estimate DV based on assumed values of IVs \\(\\hat{y} = \\hat{\\alpha} + \\hat{\\beta_k}x_k\\)        where \\(\\hat{y}\\) is predicted value of \\(y\\) for assumed values of \\(x_k\\)        and Regression provided estimates of \\(\\alpha\\) and \\(\\beta_k\\) 2.5 Linear Regression Example 2.5.1 Overview Advertising and Sales data for 200 firms DV: Sales (in millions), \\(sales\\) IVs: TV Advertising (in 000s), \\(ad\\_tv\\) Radio Advertising (in 000s), \\(ad\\_radio\\) Paper Advertising (in 000s), \\(ad\\_paper\\) Model: \\(sales=\\alpha+\\beta_1ad\\_tv+\\beta_2ad\\_radio+\\beta_3ad\\_paper\\) Goal: Understand the relationship between various advertising types and sales 2.5.2 Summarize Data 2.5.2.1 Univariate Summary Statistics Table 2.1: Summary Statistics N.Valid Mean Std.Dev Min Max ad_paper 200 31 21.7786209215222 0.300000011920929 114 ad_radio 200 23 14.8468091920268 0 49.5999984741211 ad_tv 200 147 85.8542367175323 0.699999988079071 296.399993896484 sales 200 14 5.21745660345847 1.60000002384186 27 R Code 2.5.2.2 Scatterplot and Correlation Matrix Figure 2.1: Scatterplot Matrix with Correlation R Code 2.5.2.3 Box Plots Figure 2.2: Box Plots R Code 2.5.3 Results 2.5.3.1 R Output 2.5.3.1.1 Regression Results (Concise) Estimated regression equation: \\(\\hat{sales}=2.939+.046ad\\_tv+.189ad\\_radio-.001ad\\_paper\\) Table 2.2: Regression Results (Concise) F(3,196) 570.2707 R² 0.8972 Adj. R² 0.8956 Est. S.E. t val. p (Intercept) 2.9389 0.3119 9.4223 0.0000 ad_tv 0.0458 0.0014 32.8086 0.0000 ad_radio 0.1885 0.0086 21.8935 0.0000 ad_paper -0.0010 0.0059 -0.1767 0.8599 Standard errors: OLS R Code 2.5.3.1.2 Standard Results Table 2.3: Regression Results (Standard) Call: lm(formula = sales ~ ad_tv + ad_radio + ad_paper, data = advtsales) Residuals: Min 1Q Median 3Q Max -8.8277 -0.8908 0.2418 1.1893 2.8292 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.938889 0.311908 9.422 &lt;2e-16 *** ad_tv 0.045765 0.001395 32.809 &lt;2e-16 *** ad_radio 0.188530 0.008611 21.893 &lt;2e-16 *** ad_paper -0.001037 0.005871 -0.177 0.86 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.686 on 196 degrees of freedom Multiple R-squared: 0.8972, Adjusted R-squared: 0.8956 F-statistic: 570.3 on 3 and 196 DF, p-value: &lt; 2.2e-16 R Code 2.5.3.2 Assessing Overall Model Fit How much variation in the DV is explained by the model Use \\(R^2\\) to assess Use \\(\\text{Adjusted }R^2\\) to compare models Conclusion: Based on the \\(R^2\\), about \\(90\\%\\) of the variance in \\(sales\\) is explained by the model 2.5.3.3 Individual Independent Variables Relationship between DV and each IV \\(H_0:\\beta_k=0\\) vs. \\(H_a:\\beta_k\\ne0\\) Interpret significant relationships With a \\(p\\text{-value}&lt;0.001\\), \\(ad\\_tv\\) has a significant effect on sales. A one unit increase in \\(ad_tv\\) is predicted to increases \\(sales\\) by \\(.0457\\) units. With a \\(p\\text{-value}&lt;0.001\\), \\(ad\\_radio\\) has a significant effect on sales. A one unit increase in \\(ad_radio\\) is predicted to increases \\(sales\\) by \\(.1885\\) units. With a \\(p\\text{-value}=.860\\), \\(ad\\_paper\\) has no significant effect on \\(sales\\). Relative strength of IVs For relative strength, use standardized \\(\\beta_k\\)s A standardized \\(\\beta_k\\) is the effect of a single standard deviation change in the IV on the DV Higher absolute values are more important Conclusion: \\(ad\\_tv\\) is the biggest driver of sales Table 2.4: Standardized Beta Coefficients Std.Beta (Intercept) 0 ad_tv 0.7531 ad_radio 0.5365 ad_paper -0.0043 R Code Visualize each IV Sometimes it helps to visually examine the IVs for interprtation Plots show predicted DV at different levels of an IV, holding the other IVs constant at the mean value Figure 2.3: Margin Plots R Code 2.5.3.4 Model Prediction For simplicity, use only \\(ad\\_tv\\) and \\(ad\\_radio\\) Table 2.5: Coefficients Table for Reduced Model Est. S.E. t val. p (Intercept) 2.9211 0.2945 9.9192 0.0000 ad_tv 0.0458 0.0014 32.9087 0.0000 ad_radio 0.1880 0.0080 23.3824 0.0000 Standard errors: OLS R Code \\(\\hat{sales}=2.9211+.0458ad\\_tv+.1880ad\\_radio\\) Predict sales for $100K television advertising and $10K radio advertising \\(\\hat{sales}=2.9211+.0458(100)+.1880(10)=9.381= \\$9,381,000\\) Visually examine prediction at different levels of \\(ad\\_tv\\) and \\(ad\\_radio\\) Figure 2.4: Prediction Plots Figure 2.1: Prediction Plots R Code 2.6 Categorical IVs 2.6.1 Overview May want to represent a qualitative variable… Gender of a buyer Success/Failure Region of the country Special situations …But the IVs are supposed to be continuous Use “dummy” variables to indicate occurrence or nonoccurence of a particular attribute Coded as 1 (usually if true) or 0 (usually if false) Dummy variables can shift the intercept, the slope, or both Intercept Shifter Dummy is only its own term in the model \\(y=\\alpha+\\beta_1x+\\beta_2D\\) Slope Shifter Dummy is only an interaction with another IV \\(y=\\alpha+\\beta_1x+\\beta_2(x\\times D)\\) Intercept and Slope Shifter Dummy is own term and an interaction with IV \\(y=\\alpha+\\beta_1x+\\beta_2D+\\beta_3(x\\times D)\\) 2.6.2 Intercept Shifter \\(D=\\begin{cases}1\\text{ if true}\\\\0\\text{ if false}\\end{cases}\\) Model: \\(y=\\alpha+\\beta_1x+\\beta_2D\\) When \\(D=0\\): \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(0)\\\\&amp; = &amp; \\alpha + \\beta_1x\\end{array}\\) When \\(D=1\\): \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(1)\\\\&amp; = &amp; (\\alpha + \\beta_2)+\\beta_1x\\end{array}\\) 2.6.3 Slope Shifter \\(D=\\begin{cases}1\\text{ if true}\\\\0\\text{ if false}\\end{cases}\\) Model: \\(y=\\alpha+\\beta_1x+\\beta_2(x\\times D)\\) When \\(D=0\\): \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(x\\times 0)\\\\&amp; = &amp; \\alpha + \\beta_1x\\end{array}\\) When \\(D=1\\): \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(x\\times 1)\\\\&amp; = &amp; \\alpha + (\\beta_1+\\beta_2)x\\end{array}\\) Unusual to see only a slope shift 2.6.4 Intercept and Slope Shifter \\(D=\\begin{cases}1\\text{ if true}\\\\0\\text{ if false}\\end{cases}\\) Model: \\(y=\\alpha+\\beta_1x+\\beta_2D+\\beta_3(x\\times D)\\) When \\(D=0\\): \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(0)+\\beta_3(x\\times 0)\\\\&amp; = &amp; \\alpha + \\beta_1x\\end{array}\\) When \\(D=1\\): \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(1)+\\beta_3(x\\times 1)\\\\&amp; = &amp; (\\alpha + \\beta_2) + (\\beta_1 + \\beta_3)x\\end{array}\\) 2.6.5 Multiple Levels What if categorical IV has multiple levels (e.g., quarters)? Choose one level to be the base Create dummy variables for the other levels Levels must be mutually exclusive Dummy variables for four levels: Level 1, \\(L_1=\\begin{cases}1\\text{ if true}\\\\0\\text{ if false}\\end{cases}\\) Level 2, \\(L_2=\\begin{cases}1\\text{ if true}\\\\0\\text{ if false}\\end{cases}\\) Level 3, \\(L_3=\\begin{cases}1\\text{ if true}\\\\0\\text{ if false}\\end{cases}\\) Level 4, \\(L_4=\\text{base level; is true when }L_1=L_2=L_3=0\\) Model: \\(y=\\alpha + \\beta_1x + \\beta_2L_1+\\beta_3L_2+\\beta_4L_3\\) When Level 1: \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(1)+\\beta_3(0)+\\beta_4(0)\\\\&amp; = &amp; (\\alpha + \\beta_2) + \\beta_1x\\end{array}\\) When Level 2: \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(0)+\\beta_3(1)+\\beta_4(0)\\\\&amp; = &amp; (\\alpha + \\beta_3) + \\beta_1x\\end{array}\\) When Level 3: \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(0)+\\beta_3(0)+\\beta_4(1)\\\\&amp; = &amp; (\\alpha + \\beta_4) + \\beta_1x\\end{array}\\) When Level 4: \\(\\begin{array}{rcl}y &amp; = &amp; \\alpha+\\beta_1x+\\beta_2(0)+\\beta_3(0)+\\beta_4(0)\\\\&amp; = &amp; \\alpha + \\beta_1x\\end{array}\\) 2.7 Categorical IVs Example 2.7.1 Overview Sales data for 28 department store locations across 47 weeks and 69 departments DV: Department Sales, \\(sales\\) IVs: Overall Store Size, \\(size\\) Week, \\(week\\) where 1 = \\(week\\) ending 11/11/11 Predict sales by department Believe that the “holiday” season (or quarter 4) will be a driver of sales for some departments Generate dummy variable: \\(q4=\\begin{cases}1\\text{ if }week\\text{ in Quarter 4}\\\\0\\text{ otherwise}\\end{cases}\\) 2.7.2 Intercept Shift \\(sales = \\alpha + \\beta_1size+\\beta_2q4\\) Results: \\(sales\\) are significantly lower in Q4 (see Table 2.6 and Figure 2.5) Table 2.6: Regression Results (Intercept Shfit) for One Department F(2,1090) 126.1302 R² 0.1879 Adj. R² 0.1864 Est. S.E. t val. p (Intercept) 9993.3892 1890.0317 5.2874 0.0000 size 0.0546 0.0107 5.1210 0.0000 q4 -15153.3589 1012.8597 -14.9610 0.0000 Standard errors: OLS R Code Figure 2.5: Margin Plot for Intercept Shifter R Code 2.7.3 Slope Shift \\(sales = \\alpha + \\beta_1size+\\beta_2(size\\times q4)\\) Results: \\(sales\\) as a function of \\(size\\) are significantly lower in Q4 (see Table 2.7 and Figure 2.6) Table 2.7: Regression Results (Slope Shift) for One Department F(2,1090) 128.2440 R² 0.1905 Adj. R² 0.1890 Est. S.E. t val. p (Intercept) 6568.3795 1870.6049 3.5114 0.0005 size 0.0742 0.0107 6.9309 0.0000 size:q4 -0.0872 0.0058 -15.0986 0.0000 Standard errors: OLS R Code Figure 2.6: Margin Plot for Slope Shifter R Code 2.7.4 Intercept and Slope Shift \\(sales = \\alpha + \\beta_1size+\\beta_2q4+\\beta_3(size\\times q4)\\) Results: \\(sales\\) as a function of \\(size\\) are significantly lower in Q4 (see Table 2.8 and Figures 2.7 and 2.8) Table 2.8: Regression Results (Intercept &amp; Slope Shift) for One Department F(3,1089) 86.0379 R² 0.1916 Adj. R² 0.1894 Est. S.E. t val. p (Intercept) 7811.3235 2126.7243 3.6729 0.0003 size 0.0673 0.0121 5.5712 0.0000 q4 -5482.4234 4466.5447 -1.2274 0.2199 size:q4 -0.0567 0.0255 -2.2229 0.0264 Standard errors: OLS R Code Figure 2.7: Margin Plot for Intercept and Slope Shifter Showing \\(y\\)-intercept R Code Figure 2.8: Margin Plot for Intercept and Slope Shifter R Code 2.7.5 Intercept Shift with Multiple Levels \\(sales=\\alpha+\\beta_1size+\\beta_2q1+\\beta_3q2+\\beta_4q3\\) Q4 is set as the base level Results: \\(sales\\) are significantly lower in Q4 than in each of the other three quarters (see Table 2.9 and Figures 2.9 and 2.10) Table 2.9: Regression Results (Intercept Shift for Multiple Levels) for One Department F(4,1088) 307.2426 R² 0.5304 Adj. R² 0.5287 Est. S.E. t val. p (Intercept) -5354.0323 1539.7359 -3.4772 0.0005 size 0.0558 0.0081 6.8680 0.0000 quarter1 7268.7033 904.6239 8.0351 0.0000 quarter2 31313.5153 963.9526 32.4845 0.0000 quarter3 10436.9137 917.0085 11.3815 0.0000 Standard errors: OLS R Code Figure 2.9: Margin Plot for Multiple Levels Showing \\(y\\)-intercept R Code Figure 2.10: Margin Plot for Multiple Levels R Code 2.8 Suggested Readings R for Marketing Research and Analytics. 2nd Edition (2019). Chapman, Chris; McDonnel Feit, Elea BGSU Library Link:http://maurice.bgsu.edu/record=b4966554~S9 eBook through BGSU Library:https://link-springer-com.ezproxy.bgsu.edu/book/10.1007%2F978-3-030-14316-9 Chapter 7 OpenIntro Statistics. 4th Edition (2019). Diez, David; Cetinkaya-Rundel, Mine; Barr, Christopher D. Available at OpenIntro.org:https://www.openintro.org/book/os/ Chapter 8: Introduction to linear regression Chapter 9: Multiple and logistic regression Multivariate Data Analysis. Hair, Joseph F.; Black, William C.; Babin, Barry J.; Anderson, Rolph E. 7th Edition: Search for “multivariate data analysis 7th edition hair” Chapter 4: Multiple Regression Analysis 5th Edition: Course reserves Chapter 4: Multiple Regression Analysis 2.9 R Code 2.9.1 Table 2.1 See Table 2.1 # Creates dataframe with only needed variables; Use &#39;dplyr::&#39; before # &#39;select&#39; to avoid conflict with other packages lrreg1 &lt;- advtsales %&gt;% dplyr::select(-id) # Creates vector of stats to request in summarytools::descr stats &lt;- c(&quot;n.valid&quot;, &quot;mean&quot;, &quot;sd&quot;, &quot;min&quot;, &quot;max&quot;) # Use package summarytools to easily create summary statistics table # Note: summarytools::descr not available in virtual environment # Request htmlTable for summary statistics with rounding two 2 digits setHtmlTableTheme(&quot;Google&quot;) # Creates more compact table htmlTable(txtRound(descr(lrreg1, stats=stats, transpose=TRUE),2)) # Note: For virtual environment, use package mosaic::favstats to produce # separate summary statistics for each variable htmlTable(txtRound(favstats(lrreg1$ad_paper),2), caption=&quot;ad_paper&quot;) htmlTable(txtRound(favstats(lrreg1$ad_radio),2), caption=&quot;ad_radio&quot;) htmlTable(txtRound(favstats(lrreg1$ad_tv),2), caption=&quot;ad_tv&quot;) htmlTable(txtRound(favstats(lrreg1$sales),2), caption=&quot;sales&quot;) 2.9.2 Table 2.2 See Table 2.2 # Run linear model and save as &#39;results&#39; results &lt;- lm(sales ~ ad_tv + ad_radio + ad_paper, data = advtsales) # Create &#39;concise&#39; results using package &#39;jtools&#39; # NOTE: &#39;jtools&#39; not available in virtual environment; use standard results summ(results, digits=4, model.info=FALSE) 2.9.3 Table 2.3 See Table 2.3 # Run linear model and save as &#39;results&#39; results &lt;- lm(sales ~ ad_tv + ad_radio + ad_paper, data = advtsales) # Displays results summary(results) 2.9.4 Table 2.4 See Table 2.4 # Function to calculate standardized beta coefficients lm_beta &lt;- function (MOD) { b &lt;- summary(MOD)$coef[-1, 1] sx &lt;- sapply(MOD$model[-1], sd) sy &lt;- sapply(MOD$model[1], sd) beta &lt;- b * sx/sy return(beta) } # Create table setHtmlTableTheme(&quot;Google&quot;, css.table=&quot;width: 50%;&quot;) htmlTable(txtRound(as.matrix(lm_beta(results)), 4, scientific=FALSE)) 2.9.5 Table 2.5 See Table 2.5 # Run linear model and save as &#39;results&#39; resultssig &lt;- lm(sales ~ ad_tv + ad_radio, data = advtsales) # Create &#39;concise&#39; results using package &#39;jtools&#39; # NOTE: &#39;jtools&#39; not available in virtual environment; use standard results summ(resultssig, digits=4, model.info=FALSE, model.fit=FALSE) 2.9.6 Table 2.6 See Table 2.6 # Add Variables to &#39;dssales&#39; dataframe dssales &lt;- dssales %&gt;% # Add week ending date (&#39;weekdate&#39;) using package &#39;lubridate&#39; mutate(weekdate=ymd(&quot;2011-11-05&quot;) + (dssales$week-1)*7) %&gt;% # Add quarter based on &#39;weekdate&#39; using package &#39;lubridate&#39; mutate(quarter = quarter(weekdate)) %&gt;% # Create dummy variable for quarter 4 mutate(q4=ifelse(quarter==4,1,0)) # Create new data frame with only department 16 dssales.16 &lt;- dssales %&gt;% filter(dept==16) # Run model with intercept shifter only mod.is &lt;- lm(sales~size + q4, data=dssales.16) # Show results using &#39;jtools&#39; package summ(mod.is, digits=4, model.info=FALSE) 2.9.7 Table 2.7 See Table 2.7 # Run model with slope shifter only; use &#39;:&#39; between interaction terms to # exclude main effect of q4 from model mod.ss &lt;- lm(sales~size+size:q4, data=dssales.16) # Show results using &#39;jtools&#39; package summ(mod.ss, digits=4, model.info=FALSE) 2.9.8 Table 2.8 See Table 2.8 # Run model with intercept and slope shifter; use &#39;*&#39; between interaction # terms to include interaction AND main effects mod.iss &lt;- lm(sales~size*q4, data=dssales.16) # Show results using &#39;jtools&#39; package summ(mod.iss, digits=4, model.info=FALSE) 2.9.9 Table 2.9 See Table 2.9 # Make &#39;quarter&#39; a factor variable so R will use dummy variables automatically dssales.16$quarter &lt;- factor(dssales.16$quarter) # Set base level of &#39;quarter&#39; to be 4 dssales.16$quarter &lt;- relevel(dssales.16$quarter, ref=4) # Run model with multiple dummies for quarter mod.mis &lt;- lm(sales~size+quarter, data=dssales.16) # Show results using &#39;jtools&#39; package summ(mod.mis, digits=4, model.info=FALSE) 2.9.10 Figure 2.1 See Figure 2.1 # Use package GGally::ggpairs to easily create combination correlation # and scatterplot matrix ggpairs(lrreg1, # Dataset lower=list(continuous= wrap(&quot;smooth&quot;, method=&quot;lm&quot;, se=FALSE, # Add fit line color=&quot;midnightblue&quot;)), # Set dot color diag=list(continuous=&quot;blankDiag&quot;)) # Set diagonals to be blank 2.9.11 Figure 2.2 See Figure 2.2 melt(lrreg1) %&gt;% # Use package &#39;reshape2&#39; to reshape the data for facet plot # Begins plot with each variable as a factor and # value of the variable to be plotted ggplot(aes(factor(variable), value)) + # Requests boxplot as geom function geom_boxplot() + # Adds the whiskers to the boxplot stat_boxplot(geom=&#39;errorbar&#39;) + # Creates a facet/matrix layout based on the variable facet_wrap(~variable, scale=&quot;free&quot;) + # Change text size theme(text=element_text(size=15)) + # Removes axis labels labs(x=&quot;&quot;, y=&quot;&quot;) 2.9.12 Figure 2.3 See Figure 2.3 # Use &#39;effect&#39; package library(effects) # Want to predict &#39;sales&#39; for different levels of &#39;ad_tv&#39; tv.pred &lt;- data.frame(predictorEffect(&quot;ad_tv&quot;, # Focal variable results)) # Use &#39;tv.pred&#39; for margin plot, assign to &#39;p1&#39; p1 &lt;- tv.pred %&gt;% ggplot(aes(x=ad_tv, # ad_tv on x-axis y=fit)) + # &#39;sales&#39; prediction on y-axis geom_line(size=1) + # Draw predicted line geom_ribbon(aes(ymin=lower, # Draws the confidence interval bands ymax=upper), alpha=0.2) + # Sets transparency level # Next two commands scale x and y axes scale_x_continuous(limits=c(0,300), expand=c(.025,.025), breaks=seq(0,300,50), minor_breaks=NULL) + scale_y_continuous(limits=c(5,25), expand=c(.025,.025), breaks=seq(5,25,5), minor_breaks=NULL) + labs(x=&quot;TV Advertising&quot;, y=&quot;Linear Prediction&quot;) # Repeat for other two variables rd.pred &lt;- data.frame(predictorEffect(&quot;ad_radio&quot;, results)) p2 &lt;- rd.pred %&gt;% ggplot(aes(x=ad_radio, y=fit)) + geom_line(size=1) + geom_ribbon(aes(ymin=lower, ymax=upper), alpha=0.2) + scale_x_continuous(limits=c(0,50), expand=c(.025,.025), breaks=seq(0,50,10), minor_breaks=NULL) + scale_y_continuous(limits=c(5,25), expand=c(.025,.025), breaks=seq(5,25,5), minor_breaks=NULL) + labs(x=&quot;Radio Advertising&quot;, y=&quot;Linear Prediction&quot;) np.pred &lt;- data.frame(predictorEffect(&quot;ad_paper&quot;, results, focal.levels=seq(0,100,2))) p3 &lt;- np.pred %&gt;% ggplot(aes(x=ad_paper, y=fit)) + geom_line(size=1) + geom_ribbon(aes(ymin=lower, ymax=upper), alpha=0.2) + scale_x_continuous(limits=c(0,100), expand=c(.025,.025), breaks=seq(0,100,20), minor_breaks=NULL) + scale_y_continuous(limits=c(5,25), expand=c(.025,.025), breaks=seq(5,25,5), minor_breaks=NULL) + labs(x=&quot;Newspaper Advertising&quot;, y=&quot;Linear Prediction&quot;) # Arrange three plots in a grid using package &#39;cowplot&#39; plot_grid(p1,p2,p3) 2.9.13 Figure 2.4 See Figure 2.4 # Create new data for prediction with &#39;ad_tv&#39; as focus ad.tv.pred &lt;- crossing(ad_tv=seq(0,300,30), # 11 levels ad_radio=seq(0,50,10)) # 6 levels # Append linear prediction and prediction intervals to new data ad.tv.pred$pred &lt;- as.data.frame( predict.lm(resultssig, # Model to use for prediction ad.tv.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals # Create plot and save as object &#39;p1&#39; p1 &lt;- # Begins plot ggplot(aes(x=ad_tv, # levels of &#39;ad_tv&#39; for x-axis y=pred$fit, # linear prediction for y-axis group=as.factor(ad_radio), # different geoms for each level of &#39;ad_radio&#39; color=as.factor(ad_radio)), # different colors for each level of &#39;ad_radio&#39; data=ad.tv.pred) + # Draws lines and points based on predicted values geom_line() + geom_point() + # Adds confidence interaval bands around line geom_errorbar(aes(ymin=pred$lwr, ymax=pred$upr), width=5) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(0,300,50), minor_breaks=NULL) + scale_y_continuous(limits=c(0,30), expand=c(.025,.025), breaks=seq(0,30,5), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + guides(color=guide_legend(title.position=&quot;top&quot;)) + # Labels axes and legend labs(x=&quot;TV Advertising (ad_tv)&quot;, y=&quot;Linear Prediction&quot;, color=&quot;Radio Advertising (ad_radio)&quot;) # Repeat for other variable ad.rad.pred &lt;- crossing(ad_tv=seq(0,300,100), # 4 levels ad_radio=seq(0,50,5)) # 11 levels ad.rad.pred$pred &lt;- as.data.frame( predict.lm(resultssig, ad.rad.pred, interval=&quot;confidence&quot;)) p2 &lt;- ggplot(aes(x=ad_radio, y=pred$fit, group=as.factor(ad_tv), color=as.factor(ad_tv)), data=ad.rad.pred) + geom_line() + geom_point() + geom_errorbar(aes(ymin=pred$lwr, ymax=pred$upr), width=.83) + scale_x_continuous(breaks=seq(0,50,10), minor_breaks=NULL) + scale_y_continuous(limits=c(0,30), expand=c(.025,.025), breaks=seq(0,30,5), minor_breaks=NULL) + theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + guides(color=guide_legend(title.position=&quot;top&quot;)) + labs(x=&quot;Radio Advertising (ad_radio)&quot;, y=&quot;Linear Prediction&quot;, color=&quot;TV Advertising (ad_tv)&quot;) # Arrange three plots in a grid using package &#39;cowplot&#39; plot_grid(p1,p2) 2.9.14 Figure 2.5 See Figure 2.5 # Create new data for prediction with &#39;size&#39; as focus size.pred &lt;- merge(data.frame(size=seq(70000,220000,15000)), # Variable of interest data.frame(q4=0:1)) # Dummy variable # Append linear prediction and prediction intervals to new data size.pred$pred &lt;- as.data.frame( predict.lm(mod.is, # Model to use for prediction size.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals # Begins plot ggplot(aes(x=size, # levels of &#39;size&#39; for x-axis y=pred$fit, # linear prediction for y-axis color=as.factor(q4)), # different colors for each level of &#39;q4&#39; data=size.pred) + # Draws lines based on predicted values geom_line(size=1) + # Adds confidence interval bands around line geom_ribbon(aes(ymin=pred$lwr, ymax=pred$upr, fill=as.factor(q4)), alpha=0.2) + # Next two commands set colors for lines and CI fill scale_color_manual(name=&quot;Q4&quot;, values=c(&quot;darkred&quot;, &quot;darkblue&quot;)) + scale_fill_manual(name=&quot;Q4&quot;, values=c(&quot;red&quot;, &quot;blue&quot;)) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(70000,220000,30000), minor_breaks=NULL) + scale_y_continuous(limits=c(-5000,25000), expand=c(.025,.025), breaks=seq(-5000,25000,10000), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + # Labels axes and legend labs(x=&quot;Size&quot;, y=&quot;Linear Prediction&quot;) 2.9.15 Figure 2.6 See Figure 2.6 # Create new data for prediction with &#39;size&#39; as focus size.pred &lt;- merge(data.frame(size=seq(70000,220000,15000)), # Variable of interest data.frame(q4=0:1)) # Dummy variable # Append linear prediction and prediction intervals to new data size.pred$pred &lt;- as.data.frame( predict.lm(mod.ss, # Model to use for prediction size.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals # Begins plot ggplot(aes(x=size, # levels of &#39;size&#39; for x-axis y=pred$fit, # linear prediction for y-axis color=as.factor(q4)), # different colors for each level of &#39;q4&#39; data=size.pred) + # Draws lines based on predicted values geom_line(size=1) + # Adds confidence interval bands around line geom_ribbon(aes(ymin=pred$lwr, ymax=pred$upr, fill=as.factor(q4)), alpha=0.2) + # Next two commands set colors for lines and CI fill scale_color_manual(name=&quot;Q4&quot;, values=c(&quot;darkred&quot;, &quot;darkblue&quot;)) + scale_fill_manual(name=&quot;Q4&quot;, values=c(&quot;red&quot;, &quot;blue&quot;)) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(70000,220000,15000), minor_breaks=NULL) + scale_y_continuous(limits=c(-5000,25000), expand=c(.025,.025), breaks=seq(-5000,25000,10000), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + # Labels axes and legend labs(x=&quot;Size&quot;, y=&quot;Linear Prediction&quot;) 2.9.16 Figure 2.7 See Figure 2.7 # Create new data for prediction with &#39;size&#39; as focus size.pred &lt;- merge(data.frame(size=seq(0,220000,20000)), # Variable of interest data.frame(q4=0:1)) # Dummy variable # Append linear prediction and prediction intervals to new data size.pred$pred &lt;- as.data.frame( predict.lm(mod.iss, # Model to use for prediction size.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals ggplot(aes(x=size, # levels of &#39;size&#39; for x-axis y=pred$fit, # linear prediction for y-axis color=as.factor(q4)), # different colors for each level of &#39;q4&#39; data=size.pred) + # Draws lines based on predicted values geom_line(size=1) + # Next two commands set colors for lines and CI fill scale_color_manual(name=&quot;Q4&quot;, values=c(&quot;darkred&quot;, &quot;darkblue&quot;)) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(0,220000,55000), minor_breaks=NULL) + scale_y_continuous(limits=c(-10000,25000), expand=c(.025,.025), breaks=seq(-5000,25000,10000), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + # Labels axes and legend labs(x=&quot;Size&quot;, y=&quot;Linear Prediction&quot;) 2.9.17 Figure 2.8 See Figure 2.8 # Create new data for prediction with &#39;size&#39; as focus size.pred &lt;- merge(data.frame(size=seq(70000,220000,15000)), # Variable of interest data.frame(q4=0:1)) # Dummy variable # Append linear prediction and prediction intervals to new data size.pred$pred &lt;- as.data.frame( predict.lm(mod.iss, # Model to use for prediction size.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals ggplot(aes(x=size, # levels of &#39;size&#39; for x-axis y=pred$fit, # linear prediction for y-axis color=as.factor(q4)), # different colors for each level of &#39;q4&#39; data=size.pred) + # Draws lines based on predicted values geom_line(size=1) + # Adds confidence interval bands around line geom_ribbon(aes(ymin=pred$lwr, ymax=pred$upr, fill=as.factor(q4)), alpha=0.2) + # Next two commands set colors for lines and CI fill scale_color_manual(name=&quot;Q4&quot;, values=c(&quot;darkred&quot;, &quot;darkblue&quot;)) + scale_fill_manual(name=&quot;Q4&quot;, values=c(&quot;red&quot;, &quot;blue&quot;)) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(70000,220000,30000), minor_breaks=NULL) + scale_y_continuous(limits=c(-5000,25000), expand=c(.025,.025), breaks=seq(-5000,25000,10000), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + # Labels axes and legend labs(x=&quot;Size&quot;, y=&quot;Linear Prediction&quot;) 2.9.18 Figure 2.9 See Figure 2.9 # Create new data for prediction with &#39;size&#39; as focus size.pred &lt;- merge(data.frame(size=seq(0,240000,20000)), # Variable of interest data.frame(quarter=1:4)) # Dummy variable # Set &#39;quarter&#39; to be factor variable to match model size.pred$quarter &lt;- as.factor(size.pred$quarter) # Append linear prediction and prediction intervals to new data size.pred$pred &lt;- as.data.frame( predict.lm(mod.mis, # Model to use for prediction size.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals # Begins plot ggplot(aes(x=size, # levels of &#39;size&#39; for x-axis y=pred$fit, # linear prediction for y-axis color=quarter), # different colors for each level of &#39;quarter&#39; data=size.pred) + # Draws lines based on predicted values geom_line(size=1) + # Adds confidence interval bands around line geom_ribbon(aes(ymin=pred$lwr, ymax=pred$upr, fill=quarter), alpha=0.2) + # Next two commands set colors for lines and CI fill scale_color_manual(name=&quot;Quarter&quot;, values=c(&quot;darkred&quot;, &quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;darkorange&quot;)) + scale_fill_manual(name=&quot;Quarter&quot;, values=c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;)) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(0,240000,40000), minor_breaks=NULL) + scale_y_continuous(limits=c(-10000,45000), expand=c(.025,.025), breaks=seq(-10000,40000,10000), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + # Labels axes and legend labs(x=&quot;Size&quot;, y=&quot;Linear Prediction&quot;) 2.9.19 Figure 2.10 See Figure 2.10 # Create new data for prediction with &#39;size&#39; as focus size.pred &lt;- merge(data.frame(size=seq(70000,220000,15000)), # Variable of interest data.frame(quarter=1:4)) # Dummy variable # Set &#39;quarter&#39; to be factor variable to match model size.pred$quarter &lt;- as.factor(size.pred$quarter) # Append linear prediction and prediction intervals to new data size.pred$pred &lt;- as.data.frame( predict.lm(mod.mis, # Model to use for prediction size.pred, # Data set to predict on interval=&quot;confidence&quot;)) # Confidence intervals # Begins plot ggplot(aes(x=size, # levels of &#39;size&#39; for x-axis y=pred$fit, # linear prediction for y-axis color=quarter), # different colors for each level of &#39;q4&#39; data=size.pred) + # Draws lines based on predicted values geom_line(size=1) + geom_point() + # Adds confidence interval bands around line scale_color_manual(name=&quot;Quarter&quot;, values=c(&quot;darkred&quot;, &quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;darkorange&quot;)) + # Next two commands scale x and y axes scale_x_continuous(breaks=seq(70000,220000,30000), minor_breaks=NULL) + scale_y_continuous(limits=c(-5000,40000), expand=c(.025,.025), breaks=seq(-5000,40000,5000), minor_breaks=NULL) + # Position legend at bottom with title over legend; change text size theme(legend.position=&quot;bottom&quot;, text=element_text(size=15)) + # Labels axes and legend labs(x=&quot;Size&quot;, y=&quot;Linear Prediction&quot;) "],["logistic-regression.html", "Topic 3 Logistic Regression 3.1 Motivation 3.2 R Packages and Datasets for Topic 3 3.3 Why not use linear regression? 3.4 Understanding Logistic Regression 3.5 Conducting Logistic Regression 3.6 Logistic Regression Example 3.7 Suggested Readings 3.8 R Code", " Topic 3 Logistic Regression 3.1 Motivation Marketers often observe binary outcomes Did a customer: purchase? subscribe? renew? respond? Using linear regression is not appropriate… …But logistic regression still allows us to: Understand IV/DV relationships Make predictions 3.2 R Packages and Datasets for Topic 3 library(ggplot2) # Advanced graphing capabilities library(cowplot) # Plots in grid library(tidyr) # Easier programming library(flextable) # Better HTML Tables library(htmlTable) # Better HTML Tables library(jtools) # Concise regression results library(dplyr) # Easier programming library(caret) # Create data partitions library(MKT4320BGSU) data(directmktg) 3.3 Why not use linear regression? Want to see how \\(age\\) affects \\(buy\\) \\(buy=\\begin{cases}1\\text{ if yes/true}\\\\0\\text{ if no/false}\\end{cases}\\) Examine relationship with a scatterplot What do we see? Figure 3.1: Scatterplot with binary DV (R code) Try linear regression: \\(buy=\\alpha+\\beta_1 age\\) Table 3.1: Linear Regression Results (R code) F(1,398) 251.7421 R² 0.3874 Adj. R² 0.3859 Est. S.E. t val. p (Intercept) -0.7154 0.0702 -10.1930 0.0000 age 0.0285 0.0018 15.8664 0.0000 Standard errors: OLS Good \\(R^2\\) and \\(age\\) is highly significant So what’s the problem? Predict \\(buy\\) from linear regression results: \\(\\hat{buy}=-.7154+.0285age\\) Prediction line shown in plot Figure 3.2: Predicted Values from Linear Regression (R code) Add \\(age\\) categories and plot mean \\(buy\\) for each category What “shape” does this resemble? Can we use this “shape” to model the relationship? Figure 3.3: Buy for Age Groups (R code) 3.4 Understanding Logistic Regression Uses the logistic function: \\(f(z)=\\frac{e^u}{1+e^u}\\) \\(f(z)\\) is the probability of event happening \\(u\\) is a linear function, such as: \\(\\alpha+\\beta x\\) Ensures predictions are never above 1 or below 0 Figure 3.4: Logistic Function (R code) Probability of event success vs. failure \\(=\\frac{f(z)}{1-f(z)}=\\) Odds Ratio (\\(OR\\)) Suppose probability of success \\(=.01\\), then: \\(OR=\\frac{.01}{1-.01}=.0101\\) Suppose probability of success \\(=.001\\), then: \\(OR=\\frac{.001}{1-.001}=.0010\\) Suppose probability of success \\(=.99\\), then: \\(OR=\\frac{.99}{1-.99}=99\\) Suppose probability of success \\(=.999\\), then: \\(OR=\\frac{.999}{1-.999}=999\\) Suppose probability of success \\(=.5\\), then: \\(OR=\\frac{.5}{1-.5}=1\\) Substituting logistic function for \\(f(z)\\) into Odds Ratio \\(\\Rightarrow\\) \\(OR=e^u=e^{\\alpha+betax}\\) \\(\\frac{f(z)}{1-f(z)}=\\frac{\\frac{e^u}{1+e^u}}{1-\\frac{e^u}{1+e^u}}=\\frac{\\frac{e^u}{1+e^u}}{\\frac{1+e^u}{1+e^u}-\\frac{e^u}{1+e^u}}=\\frac{\\frac{e^u}{1+e^u}}{\\frac{1}{1+e^u}}=e^u\\) Can transform exponential function into linear \\(\\Rightarrow\\) \\(Logit=\\ln(OR)=\\alpha+\\beta x\\) 3.5 Conducting Logistic Regression Model Estimation Assessing Model Fit Goodness of Fit Measures Classification Matrix ROC Curve Interpreting Coefficients Gains and Lift 3.5.1 Model Estimation Best to use training data and holdout data Estimate model on training data (~75% of sample) Check prediction accuracy on holdout data (~25%) Can estimate either (1) \\(OR\\) or (2) \\(Logit\\) formulation \\(OR=e^{\\alpha+\\beta_1x_1+\\cdots+\\beta_kx_k}\\) \\(Logit=\\alpha+\\beta_1x_1+\\cdots+\\beta_kx_k\\) Independent variables: Can be one or more Can be continuous or categorical/factor 3.5.2 Assessing Model Fit 3.5.2.1 Goodness-of-Fit Measures Overall significance based on \\(-2LL\\) Lower (closer to \\(0\\)) \\(-2LL\\) indicates a better fit Compare \\(-2LL\\) of estimated model with “null” model McFadden’s Pseudo-\\(R^2\\) Values range from 0 to 1 like linear regression Interpreted in a similar manner Amount of variation in DV explained by IVs 3.5.2.2 Classification Matrix How does the model do in predicting outcomes? Generate predicted probability of success, \\(p(\\text{SUCCESS})\\), for each observation If \\(p(\\text{SUCCESS})\\ge0.5\\), predict \\(\\text{SUCCESS}=1\\) If \\(p(\\text{SUCCESS})&lt;0.5\\), predict \\(\\text{SUCCESS}=0\\), or \\(\\text{FAILURE}\\) Check predictions against actual outcomes Examine both training and holdout data Figure 3.5: Classification Matrix Three main measures Sensitivity: Predicted success given actual success \\(p(\\hat{+}|+)=a/(a+c)\\) Specificity: Predicted failure given actual failure \\(p(\\hat{-}|-)=d/(b+d)\\) Overall correctly classified \\((a+d)/(a+b+c+d)\\) Sensitivity vs. Specificity Ideally, want both to be high, but… the \\(p(\\text{SUCCESS})\\ge\\pi\\) threshold can be changed Why change \\(\\pi\\)? Avoid false positives or negatives By default: Increasing sensitivity decreases specificity Increasing specificity decreases sensitivity Overall correctly classified Compare with Proportional Chance Criterion (\\(PCC\\)) \\(PCC\\) is the “average” probability of classification based on group sizes \\(PCC=p^2+(1-p)^2\\) where \\(p\\) is the proportion of sample in the \\(\\text{SUCCESS}\\) group Overall correctly classified \\(&gt;PCC\\) considered good fit when examining holdout data 3.5.2.3 ROC Curve Plot sensitivity by \\(1-\\) specificity as \\(\\pi\\) goes from \\(0\\) to \\(1\\) More area under curve means better model Area under Curve Discrimination AUC = .5 None .5 &lt; AUC &lt; .7 Poor .7 ≤ AUC &lt; .8 Acceptable .8 ≤ AUC &lt; .9 Excellent AUC ≥ .9 Outstanding Figure 3.6: Sample ROC Curve 3.5.3 Interpreting Coefficients Relationship between DV and each IV \\(H_0: \\beta_k=0\\) vs. \\(H_a: \\beta_k\\ne0\\) Interpret significant relationships Interpretation depends on \\(OR\\) or \\(Logit\\) estimation Direction of relationship: \\(Logit\\) estimation: \\(\\beta_k&gt;0\\) for positive, \\(\\beta_k&lt;0\\) for negative \\(OR\\) estimation: \\(\\beta_k&gt;1\\) for positive, \\(\\beta_k&lt;1\\) for negative Magnitude of change: \\(Logit\\) estimation: coefficients are not particularly useful \\(OR\\) estimation: Percentage change in odds Compare probabilities between groups Interaction of continuous IV and dummy IV \\(Logit=\\alpha+\\beta_1x_1+\\beta_2D+\\beta_3(x_1\\times D)\\) When \\(D=0\\), \\(Logit=\\alpha+\\beta_1x_1\\), so:\\(OR(x_1)=e^{\\beta_1}\\) When \\(D=1\\), \\(Logit=(\\alpha+\\beta_2)+(\\beta_1+\\beta_3)x_1\\), so:\\(OR(x_1)=e^{(\\beta_1+\\beta_3)}\\) 3.5.4 Gain and Lift Evaluate performance of classification Example: Suppose \\(10\\%\\) of \\(2000\\) customers will accept offer For \\(100\\) random customers, expect \\(10\\) accepted offers Model predicts some customers more likely to accept Instead of contacting \\(100\\) random customers…Contact \\(100\\) most likely to accept based on model Continue doing this in groups of \\(100\\) (or \\(200\\), etc.) Gain and lift provide measures of how much better the model performs vs. no model/random Process Predict $p() for each observation and sort descending Split into 10 (deciles) or 20 (demi-deciles) ordered groups Calculate \\(\\%\\) observations and \\(\\%\\) successes for each group 3.5.4.1 Gain Cumulative successes up to that group divided by total successes across all groups Plot on \\(y\\)-axis, with cumulative percent of observations on \\(x\\)-axis Figure 3.7: Typical Gain Chart Shape 3.5.4.2 Lift Ratio of cumulative success up to that group divided by expected success from no model Plot on \\(y\\)-axis, with cumulative percent of observations on \\(x\\)-axis Figure 3.8: Typical Lift Chart Shape 3.5.5 Sensitivity/Specificity Plots Sensitivity, Specificity, and Accuracy depend on the cutoff value for predicting SUCCESS/ FAILURE While 0.5 is the most common threshold, it might not be the best threshold for prediction Sensitivity/Specificity Plots can show the analyst how each changes with different cutoff values The analyst can try to balance the three depending on the purpose of project Figure 3.9: Sample Sensitivy/Specificity Plot 3.6 Logistic Regression Example 3.6.1 Overview Purchase data for direct marketing campaign 400 observations of individual responses DV: Purchase made, \\(buy\\) (factor: Yes, No) IVs: Age, \\(age\\) Estimated Salary ($000s), \\(salary\\) Gender, \\(gender\\) (factor: Male, Female) Predict likelihood of purchase 3.6.2 Estimation Results Logit formulation results Table 3.2: Logistic Regression Results (Logit Formulation) (R code) χ²(3) 182.2574 Pseudo-R² (Cragg-Uhler) 0.6231 Pseudo-R² (McFadden) 0.4638 AIC 218.6842 BIC 233.5126 Est. S.E. z val. p (Intercept) -13.1661 1.6217 -8.1187 0.0000 age 0.2502 0.0321 7.7961 0.0000 salary 0.0406 0.0067 6.0265 0.0000 genderFemale -0.4069 0.3498 -1.1631 0.2448 Standard errors: MLE Model p-value = 0.0000 Odds Ratio Coefficients Table 3.3: Logistic Regression Odds Ratio Coefficients (R code) .cl-0444e3f2{}.cl-0439bf40{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-043e2120{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-043e2134{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-043e402e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-043e4038{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-043e4039{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-043e403a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-043e4042{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-043e4043{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ParameterOR Estp2.5%97.5%(Intercept)0.00000.00000.00000.0000age1.28430.00001.20601.3677salary1.04150.00001.02781.0553genderFemale0.66570.24480.33541.3215 3.6.3 Overall Model Fit Based on the likelihood ratio \\(\\chi^2\\) test with a \\(p\\text{-value}&lt;.0001\\), the overall model is significant (see Table 3.2) McFadden’s Pseudo-\\(R^2\\) of \\(.464\\) means that the model explains about \\(46\\%\\) of the variation between buyers/non-buyers (see Table 3.2) Classification Matrix for the Training Sample shows: High sensitivity (\\(72.2\\%\\)) High specificity (\\(91.2\\%\\)) Correctly classified (\\(84.4\\%\\)) &gt; PCC (\\(54.0\\%\\)) Table 3.4: Classification Matrix for Training Sample (R code) Confusion Matrix and Statistics Reference Prediction No Yes No 176 30 Yes 17 78 Accuracy : 0.8439 95% CI : (0.7978, 0.883) No Information Rate : 0.6412 P-Value [Acc &gt; NIR] : 4.55e-15 Kappa : 0.6514 Mcnemar&#39;s Test P-Value : 0.08005 Sensitivity : 0.7222 Specificity : 0.9119 Pos Pred Value : 0.8211 Neg Pred Value : 0.8544 Prevalence : 0.3588 Detection Rate : 0.2591 Detection Prevalence : 0.3156 Balanced Accuracy : 0.8171 &#39;Positive&#39; Class : Yes PCC = 53.99% Classification Matrix for the Test/Holdout Sample shows: High sensitivity (\\(77.1\\%\\)) High specificity (\\(90.6\\%\\)) Correctly classified (\\(85.9\\%\\)) &gt; PCC (\\(54.3\\%\\)) Table 3.5: Classification Matrix for Test/Holdout Data (R code) Confusion Matrix and Statistics Reference Prediction No Yes No 58 8 Yes 6 27 Accuracy : 0.8586 95% CI : (0.7741, 0.9205) No Information Rate : 0.6465 P-Value [Acc &gt; NIR] : 2.004e-06 Kappa : 0.6866 Mcnemar&#39;s Test P-Value : 0.7893 Sensitivity : 0.7714 Specificity : 0.9062 Pos Pred Value : 0.8182 Neg Pred Value : 0.8788 Prevalence : 0.3535 Detection Rate : 0.2727 Detection Prevalence : 0.3333 Balanced Accuracy : 0.8388 &#39;Positive&#39; Class : Yes PCC = 54.29% ROC Curve for Training Sample Area \\(&gt;.90\\) suggests an outstanding model fit Figure 3.10: ROC Curve for Training Sample (R code) ROC Curve for Holdout Sample Area \\(&gt;.90\\) suggests an outstanding model fit Figure 3.11: ROC Curve for Test/Holdout Sample (R code) 3.6.4 Interpreting Coefficients \\(age\\) is positive (\\(OR&gt;1\\)) and significant (\\(p&lt;.001\\)) \\(1\\) year increase in \\(age\\) increases odds of buying by a factor of \\(1.28\\) (or odds of buying increase by \\(25\\%\\)) \\(salary\\) is positive (\\(OR&gt;1\\)) and significant (\\(p&lt;.001\\)) \\(\\$1000\\) increase in \\(salary\\) increases odds of buying by a factor of \\(1.04\\) (or odds of buying increase by \\(4\\%\\)) \\(gender\\) is negative (\\(OR&lt;1\\)), but not significant (\\(p=.245\\)) Had it been significant… Being female decreases odds of buying by a factor of \\(.67\\) (or odds of buying decrease by \\(33\\%\\)) Can visually examine how \\(\\Pr(buy)\\) changes with IVs Figure 3.12: Margin Plots for Age, Salary, and Gender (R code) 3.6.5 Gain Can examine gain for both the training and holdout samples… But using holdout is more informative Contacting the top \\(25\\%\\) of predicted buyers yields nearly \\(60\\%\\) of actual buyers Table 3.6: Gain Table for Training and Test/Holdout Samples (R code) # A tibble: 20 × 3 `% Sample` Holdout Training &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.05 0.114 0.130 2 0.1 0.2 0.25 3 0.15 0.343 0.370 4 0.2 0.486 0.481 5 0.25 0.6 0.593 6 0.3 0.743 0.713 7 0.35 0.771 0.778 8 0.4 0.829 0.843 9 0.45 0.914 0.907 10 0.5 0.971 0.935 11 0.55 1 0.981 12 0.6 1 0.991 13 0.65 1 0.991 14 0.7 1 0.991 15 0.75 1 1 16 0.8 1 1 17 0.85 1 1 18 0.9 1 1 19 0.95 1 1 20 1 1 1 Figure 3.13: Gain Chart for Training and Test/Holdout Samples (R code) 3.6.6 Lift Can examine gain for both the training and holdout samples… But using holdout is more informative Contacting the top \\(25\\%\\) of predicted buyers provides lift of nearly 2.5 Table 3.7: Lift Table for Training and Test/Holdout Samples (R code) # A tibble: 20 × 3 `% Sample` Holdout Training &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.05 2.83 2.60 2 0.1 2.2 2.51 3 0.15 2.42 2.48 4 0.2 2.53 2.42 5 0.25 2.48 2.38 6 0.3 2.54 2.38 7 0.35 2.25 2.23 8 0.4 2.10 2.11 9 0.45 2.06 2.02 10 0.5 1.96 1.88 11 0.55 1.83 1.79 12 0.6 1.68 1.66 13 0.65 1.55 1.53 14 0.7 1.43 1.42 15 0.75 1.34 1.34 16 0.8 1.25 1.25 17 0.85 1.18 1.18 18 0.9 1.11 1.11 19 0.95 1.05 1.06 20 1 1 1 Figure 3.14: Lift Chart for Training and Test/Holdout Samples (R code) 3.6.7 Sensitivity/Specificity Plots Examine to see if different thresholds are warranted Looking at the plots for both the training sample (see Figure 3.15) and the test/holdout sample (see Figure 3.16), it might be worthwhile to try a cutoff threshold between 0.35 and 0.40 Doing so will balance specificity and sensitivity without hurting accuracy Figure 3.15: Sensitivity/Specificity Plot for Training Sample (R code) Figure 3.16: Sensitivity/Specificity Plot for Test/Holdout Sample (R code) 3.7 Suggested Readings R for Marketing Research and Analytics. 2nd Edition (2019). Chapman, Chris; McDonnel Feit, Elea BGSU Library Link:http://maurice.bgsu.edu/record=b4966554~S9 eBook through BGSU Library:https://link-springer-com.ezproxy.bgsu.edu/book/10.1007%2F978-3-030-14316-9 Chapter 9.2: Linear Models for Binary Outcomes: Logistic Regression OpenIntro Statistics. 4th Edition (2019). Diez, David; Cetinkaya-Rundel, Mine; Barr, Christopher D. Available at OpenIntro.org:https://www.openintro.org/book/os/ Chapter 9: Multiple and logistic regression Multivariate Data Analysis. Hair, Joseph F.; Black, William C.; Babin, Barry J.; Anderson, Rolph E. 7th Edition: Search for “multivariate data analysis 7th edition hair” Chapter 6: Logistic Regression with a Binary Dependent Variable 5th Edition: Course reserves Chapter 5: Multiple Discriminant Analysis and Logistic Regression (pp. 276-281; 314-321) 3.8 R Code Figure 3.1 directmktg %&gt;% mutate(buy01=as.numeric(buy)-1) %&gt;% # Change &#39;buy&#39; to 0-1 ggplot(aes(x=age, y=buy01)) + geom_point(size=2) + labs(x=&quot;Age&quot;, y=&quot;Buy&quot;) Figure 3.2 directmktg %&gt;% select(age) %&gt;% # Select only the age variable mutate(yhat=predict.lm(model,.)) %&gt;% # Predict y from model # Next line creates variable to highlight negative predictions mutate(neg=as.factor(ifelse(yhat&lt;0,&quot;Yes&quot;,&quot;No&quot;))) %&gt;% ggplot(aes(x=age, y=yhat, color=neg)) + geom_point(size=3) + scale_color_manual(values=c(&quot;Yes&quot;=&quot;red&quot;, # Manually set point colors &quot;No&quot;=&quot;black&quot;), guide=&quot;none&quot;) + labs(x=&quot;Age&quot;, y=&quot;Linear Prediction&quot;) Figure 3.3 # Create data frame grouped by age dmgrp &lt;- directmktg %&gt;% # &#39;cut&#39; breaks a continuous variable into groups of each width # &#39;as.numeric&#39; keeps the new variable as integer (vs. factor) mutate(agegrp = as.numeric(cut(age, 9))) %&gt;% group_by(agegrp) %&gt;% summarise(age=mean(age), buy=mean(as.numeric(buy)-1)) # Run logistic model to create prediction to make s-curve binmod &lt;- glm(buy~age, directmktg, family=&quot;binomial&quot;) # Create dataframe with predicted values dmpred &lt;- directmktg %&gt;% select(age, buy) %&gt;% mutate(yhat=predict(binmod, type=&quot;response&quot;), buy=as.numeric(buy)-1) # Create combined plot; each geom with separate data ggplot() + geom_point(data=directmktg, aes(x=age, y=(as.numeric(buy)-1)), size=3, color=&quot;red&quot;) + geom_line(data=dmgrp, aes(x=age, y=buy), size=1.5, color=&quot;midnightblue&quot;) + geom_line(data=dmpred, aes(x=age, y=yhat), size=1.5, color=&quot;darkorange&quot;) + theme(text=element_text(size=15)) + labs(x=&quot;Age&quot;, y=&quot;Buy&quot;) Figure 3.4 # Create simulated data frame based on logistic function u=seq(-7,7,.05) fz=exp(u)/(1+exp(u)) ufz=data.frame(u=u, fz=fz) # Plot function ufz %&gt;% ggplot(aes(x=u, y=fz)) + geom_line(color=&quot;darkorange&quot;, size=1.5) + theme(text=element_text(size=15), panel.grid.major.x = element_blank()) + scale_x_continuous(breaks=0, minor_breaks=NULL) + scale_y_continuous(breaks=seq(0,1,1), minor_breaks=NULL) + labs(x=&quot;u&quot;, y=&quot;f(z)&quot;) Figure 3.10 # Requires package &#39;pROC&#39; and &#39;ggplot2 logreg_roc(model, # Object with model results train) # Data to use (i.e., training vs. testing) Figure 3.11 # Requires package &#39;pROC&#39; and &#39;ggplot2 logreg_roc(model, # Object with model results test) # Data to use (i.e., training vs. testing) Figure 3.12 # Use &#39;effects&#39; package library(effects) # Want to predict Pr(buy) for different levels of age age.pred &lt;- data.frame(predictorEffect(&quot;age&quot;, # Focal variable model)) # Use &#39;age.pred&#39; for margin plot, assign to &#39;p1&#39; p1 &lt;- age.pred %&gt;% ggplot(aes(x=age, # ad_tv on x-axis y=fit)) + # &#39;sales&#39; prediction on y-axis geom_line(size=1) + # Draw predicted line geom_ribbon(aes(ymin=lower, # Draws the confidence interval bands ymax=upper), alpha=0.2) + # Sets transparency level scale_y_continuous(limits=c(0,1)) + labs(x=&quot;Age&quot;, y=&quot;Pr(Buy)&quot;) # Repeat for other two variables sal.pred &lt;- data.frame(predictorEffect(&quot;salary&quot;, model)) p2 &lt;- sal.pred %&gt;% ggplot(aes(x=salary, # ad_tv on x-axis y=fit)) + # &#39;sales&#39; prediction on y-axis geom_line(size=1) + # Draw predicted line geom_ribbon(aes(ymin=lower, # Draws the confidence interval bands ymax=upper), alpha=0.2) + # Sets transparency level scale_y_continuous(limits=c(0,1)) + labs(x=&quot;Salary&quot;, y=&quot;Pr(Buy)&quot;) g.pred &lt;- data.frame(predictorEffect(&quot;gender&quot;, model)) p3 &lt;- g.pred %&gt;% ggplot(aes(x=gender, y=fit, group=1)) + geom_point(size=4) + geom_line(color=&quot;orange&quot;) + geom_errorbar(aes(ymin=lower, ymax=upper), width=0.2) + scale_y_continuous(limits=c(0,1)) + labs(x=&quot;Gender&quot;, y=&quot;Pr(Buy)&quot;) # Arrange three plots in a grid using package &#39;cowplot&#39; plot_grid(p1,p2,p3, nrow=2) Figure 3.13 # Plot was already returned in the previous call to &#39;gainlift&#39; glresults$gainplot Figure 3.14 # Plot was already returned in the previous call to &#39;gainlift&#39; glresults$gainlift Figure 3.15 # Requires packages &#39;ggplot2&#39; logreg_cut(model, train, &quot;Yes&quot;) Figure 3.16 # Requires packages &#39;ggplot2&#39; logreg_cut(model, test, &quot;Yes&quot;) Table 3.1 model &lt;- directmktg %&gt;% mutate(buy=as.numeric(buy)-1) %&gt;% lm(buy ~ age, .) # NOTE: &#39;summ&#39; uses the &#39;jtools&#39; package summ(model, model.info=FALSE, digits=4) # For virtual environment, use &#39;summary&#39; from Base R, # but manually calculate McFadden&#39;s Pseudo-Rsq summary(model) Mrsq &lt;- 1-model$deviance/model$null.deviance cat(&quot;McFadden&#39;s Pseudo-Rsquared = &quot;, round(Mrsq, digits=4) Table 3.2 # Use &#39;caret&#39; package to create training and test/holdout samples # This will create two separate dataframes: train and test set.seed(4320) inTrain &lt;- createDataPartition(y=directmktg$buy, p=.75, list=FALSE) train &lt;- directmktg[inTrain,] test &lt;- directmktg[-inTrain,] # Estimate the model on the training data model &lt;- glm(buy ~ age + salary + gender, train, family=&quot;binomial&quot;) # NOTE: &#39;summ&#39; uses the &#39;jtools&#39; package summ(model, model.info=FALSE, digits=4) # For virtual environment, use &#39;summary&#39; from Base R Table 3.3 flextable(or_table(model)) Table 3.4 # Requires package &#39;caret&#39; logreg_cm(model, # Object with model results train, # Data to use (i.e., training vs. testing) &quot;Yes&quot;) # Factor level for &quot;True&quot; Table 3.5 # Requires package &#39;caret&#39; logreg_cm(model, # Object with model results test, # Data to use (i.e., training vs. testing) &quot;Yes&quot;) # Factor level for &quot;True&quot; Table 3.6 # Requires packages &#39;ggplot2&#39;, &#39;dplyr&#39;, and &#39;tidyr&#39; # Returns a list of four things: # gainplot, liftplot, gaintable, lifttable glresults &lt;- gainlift(model, train, test, &quot;Yes&quot;) glresults$gaintable Table 3.7 # Table was already returned in the previous call to &#39;gainlift&#39; glresults$lifttable "],["targeting-and-retaining-customers.html", "Topic 4 Targeting and Retaining Customers 4.1 R Packages and Datasets for Topic 4 4.2 Targeting Customers 4.3 Retaining Customers 4.4 Targeting Customers (Linear Regression) Example 4.5 Targeting Customers (Logistic Regression) 4.6 Suggested Readings 4.7 R Code", " Topic 4 Targeting and Retaining Customers 4.1 R Packages and Datasets for Topic 4 library(cowplot) # Arrange plots in grid library(ggplot2) # Advanced graphing capabilities library(tidyr) # Easier programming library(GGally) # Scatterplot matrix library(flextable) # Better HTML Tables library(htmlTable) # Better HTML Tables library(jtools) # Concise regression results library(dplyr) # Easier programming library(caret) # Create data partitions library(MKT4320BGSU) load(&quot;Topic04/bankmktg.rdata&quot;) load(&quot;Topic04/telecom.rdata&quot;) 4.2 Targeting Customers One-to-One Marketing Time consuming Costly Mass Marketing Customer needs not being met Target Marketing Market to those likely to… 4.2.1 Goal Target customers with the highest likelihood of a favorable outcome using explanatory variables Outcome variable could be: Purchase Sales Costs Profitability CLV Explanatory variables could be: Demographics Behaviors Usage Lifestyles The outcome variable will dictate the type of analysis we can perform Continuous outcome variables have a meaningful magnitude Use linear regression Categorical outcome variables do not have a meaningful magnitude Use logistic regression 4.3 Retaining Customers Importance of retention: Reducing defections \\(5\\%\\) boosts profits \\(25\\%\\) to \\(85\\%\\). — Frederick F. Reichheld and W. Earl Sasser, Jr. 4.3.1 Goal Identify factors (i.e., independent variables) that increase the likelihood of retention (or decrease the likelihood of churn) Retention (or Churn) is the outcome or dependent variable DV = Binary, so Method = Logistic Regression 4.4 Targeting Customers (Linear Regression) Example 4.4.1 Overview Customer revenue, usage, and demographics for a cell phone provider DV: Mean monthly revenue (prior 6 months), avg6rev IVs: Mean monthly minutes (prior 6 months), avg6mou Mean monthly customer care calls, cc Mean monthly directory assistance calls, da Mean monthly overage minutes, ovrmou Household income (dollars), income Own home (Yes; No), own 4.4.2 Summarize Data Useful to examine data prior to specifying the model Summary Statistics Table 4.1: Summary Statistics (R code) Variable N Mean Std. Dev. Min Pctl. 25 Pctl. 75 Max avg6rev 2382 57 45 2 33 68 726 avg6mou 2382 467 487 0 143 616 5321 cc 2382 1.5 3.8 0 0 1 63 da 2382 0.8 2.1 0 0 0.74 48 ovrmou 2382 41 105 0 0 36 2239 income 2382 64 36 10 38 83 180 own 2382 … No 709 30% … Yes 1673 70% Scatterplot Matrix (with Correlations) Figure 4.1: Scatterplot Matrix with Correlations (R code) 4.4.3 Model Specification Goal: Determine what behaviors and demographics are associated with high revenue customers IVs are expected to be ones that are related to revenue Expect an interaction between home ownership and overage minutes Model: \\[\\begin{align} avg6rev=\\alpha + &amp;\\beta_1avg6mou + \\beta_2cc + \\\\ &amp;\\beta_3da+\\beta_4income + \\\\ &amp;\\beta_5ovrmou+\\beta_6own + \\beta_7(ovrmou\\times own) \\end{align}\\] 4.4.4 Model Interpretation 4.4.4.1 Results Table 4.2: Linear Regression Results (R code) Call: lm(formula = avg6rev ~ avg6mou + cc + da + income + ovrmou * own, data = telecom) Residuals: Min 1Q Median 3Q Max -114.914 -10.544 -1.583 7.495 286.012 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 33.741596 1.436152 23.494 &lt; 2e-16 *** avg6mou 0.048613 0.001366 35.598 &lt; 2e-16 *** cc -1.331792 0.149055 -8.935 &lt; 2e-16 *** da 1.894769 0.268001 7.070 2.03e-12 *** income -0.011922 0.014251 -0.837 0.403 ovrmou 0.135095 0.008707 15.516 &lt; 2e-16 *** ownYes -7.235480 1.206716 -5.996 2.33e-09 *** ovrmou:ownYes 0.066355 0.010098 6.571 6.11e-11 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 24.53 on 2374 degrees of freedom Multiple R-squared: 0.7053, Adjusted R-squared: 0.7044 F-statistic: 811.5 on 7 and 2374 DF, p-value: &lt; 2.2e-16 \\[\\begin{align} \\hat{avg6rev} = 33.742 + &amp;.049avg6mou-1.332cc + \\\\ &amp;1.895da - .013income + \\\\ &amp;.135ovrmou - 7.235own + 0.066(ovrmou\\times own) \\end{align}\\] 4.4.4.2 Testing Overall Model Significance Relationship between DV and combined effects of IVs \\(H_0: \\text{all }\\beta_k=0\\) vs. \\(H_a: \\text{at least one }\\beta_k\\ne0\\) Use F-statistic to test Conclusion: With a F-statistic of \\(811.5\\) and a \\(p&lt;.001\\), we conclude that at least one \\(\\beta_k\\) is significant 4.4.4.3 Assessing overall model fit How much variation in the DV is explained by the model Use \\(R^2\\) to assess Use Adjusted \\(R^2\\) to compare models Conclusion: Based on the \\(R^2\\), about \\(70\\%\\) of the variance in avg6rev is explained by the model 4.4.4.4 Interpret Individual IVs Relationship between DV and each IV \\(H_0: \\beta_k=0\\) vs. \\(H_a: \\beta_k\\ne0\\) Interpret significant relationships avg6mou With \\(p&lt;.001\\), avg6mou has a significant effect on avg6rev. A one unit increase in avg6mou is predicted to increase avg6rev by \\(.049\\) units. cc With \\(p&lt;.001\\), cc has a significant effect on avg6rev. A one unit increase in cc is predicted to decrease avg6rev by \\(1.332\\) units. da With \\(p&lt;.001\\), da has a significant effect on avg6rev. A one unit increase in da is predicted to increase avg6rev by \\(1.895\\) units. ovrmou and own interaction With \\(p&lt;.001\\), the effect of ovrmou on avg6rev is significantly different based on own. When the customer owns their home, a one unit increase in ovrmou is predicted to increase avg6rev by \\(.201\\) units. When the customer does notown their home, a one unit increase in ovrmou is predicted to increase avg6rev by \\(.135\\) units. Sometimes helps to visually examine the IVs for interpretation Plots can show predicted DV at different levels of IVs Figure 4.2: Margin Plots for Significant IVs (No Interaction) (R code) Figure 4.3: Margin Plots for Interaction (R code) Figure 4.4: Margin Plots for Insignificant IV (R code) Examine deciles of predicted values by the IVs Split sample into 10 groups based on predicted DV Look at mean values of IVs for each decile Table 4.3: IVs by Predicted Deciles (R code) .cl-09c9327e{}.cl-09be99c2{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-09c2a8be{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-09c2bf52{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-09c2bf5c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-09c2bf66{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}yhat.decavg6revavg6mouccdaovrmouincomeown1140.894961,445.205883.49299723.21853993233.662815162.112140.5756303279.25630879.621852.53081231.5976575672.021358568.321700.5672269366.10504658.823532.33193280.9872268945.080882466.210070.5924370454.00000479.504201.13865540.6312289924.701680766.728570.5588235549.58403376.966390.96218490.5324369716.087535065.253830.6680672645.59664291.697480.95798320.3871953810.073529463.878730.6428571739.99160224.222690.97058820.275577734.680322162.539750.6890756836.80252154.697480.70868350.206943282.404411858.185780.7478992932.93724110.351460.56624830.154299161.973152060.233790.98744771028.6610951.280331.28033470.045564850.744769961.958180.9916318 4.4.5 Conclusion Recall our Goal: Determine what behaviors and demographics are associated with high revenue customers What did we learn? We can identify our highest revenue customers by examining avg6mou, cc, da, ovrmou and own Our highest revenue customers consumer over \\(1000\\) minutes per month and have over \\(200\\) overage minutes per month More directory assistance calls and fewer customer care calls are associated with higher revenue 4.5 Targeting Customers (Logistic Regression) 4.5.1 Overview Bank marketing data for customers of a bank DV: Open term deposit account, response IVs: Age, age Average Yearly Balance, balance Housing Loan (Yes, No), housing Personal Loan (Yes, No), loan Married (Yes, No), married Predict current customers likely to buy Use training (75%) and holdout (25%) samples 4.5.2 Estimation Results Table 4.4: Logistic Regression Estimation Results on Training Sample (R code) Call: glm(formula = response ~ age + balance + housing + loan + married, family = &quot;binomial&quot;, data = train) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.42502380 0.28108855 -8.627 &lt; 2e-16 *** age 0.02063143 0.00658090 3.135 0.00172 ** balance -0.00001342 0.00002360 -0.569 0.56962 housingYes -0.57512112 0.13736995 -4.187 0.0000283 *** loanYes -0.67663138 0.22339652 -3.029 0.00245 ** marriedYes -0.57891627 0.14211504 -4.074 0.0000463 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 1629.3 on 2652 degrees of freedom Residual deviance: 1580.0 on 2647 degrees of freedom AIC: 1592 Number of Fisher Scoring iterations: 5 Model p-value = 0.0000 McFadden&#39;s Pseudo-Rsquared = 0.0303 .cl-09e64e18{}.cl-09dc2488{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-09e000a8{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-09e000b2{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-09e014d0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-09e014da{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-09e014e4{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-09e014e5{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-09e014ee{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-09e014ef{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ParameterOR Estp2.5%97.5%(Intercept)0.08850.00000.05100.1535age1.02080.00171.00781.0341balance1.00000.56960.99991.0000housingYes0.56260.00000.42980.7365loanYes0.50830.00250.32810.7876marriedYes0.56050.00000.42420.7405 4.5.3 Overall Model Fit Based on the likelihood ratio test with p-value &lt; .0001, the overall model is significant McFadden’s Pseudo-\\(R^2\\) of .030 means that the model explains only about 3% of variation between buyers/non-buyers Classification Matrix for Training Sample What’s the problem? Table 4.5: Classification Matrix for Training Sample (R code) Confusion Matrix and Statistics Reference Prediction No Yes No 2409 244 Yes 0 0 Accuracy : 0.908 95% CI : (0.8964, 0.9188) No Information Rate : 0.908 P-Value [Acc &gt; NIR] : 0.517 Kappa : 0 Mcnemar&#39;s Test P-Value : &lt;2e-16 Sensitivity : 0.00000 Specificity : 1.00000 Pos Pred Value : NaN Neg Pred Value : 0.90803 Prevalence : 0.09197 Detection Rate : 0.00000 Detection Prevalence : 0.00000 Balanced Accuracy : 0.50000 &#39;Positive&#39; Class : Yes PCC = 83.30% Sensitivity/Specificity Plot Figure 4.5: Sensitivity/Specificity Plot for Training Sample (R code) Classificaiton Matrix for Holdout Sample Table 4.6: Classification Matrix for Training Sample with 0.1 Cutoff (R code) Confusion Matrix and Statistics Reference Prediction No Yes No 1628 125 Yes 781 119 Accuracy : 0.6585 95% CI : (0.6401, 0.6765) No Information Rate : 0.908 P-Value [Acc &gt; NIR] : 1 Kappa : 0.074 Mcnemar&#39;s Test P-Value : &lt;2e-16 Sensitivity : 0.48770 Specificity : 0.67580 Pos Pred Value : 0.13222 Neg Pred Value : 0.92869 Prevalence : 0.09197 Detection Rate : 0.04485 Detection Prevalence : 0.33924 Balanced Accuracy : 0.58175 &#39;Positive&#39; Class : Yes PCC = 83.30% Classification Matrix for Holdout Sample Results very similar for holdout sample Table 4.7: Classification Matrix for Holdout Sample with 0.1 Cutoff (R code) Confusion Matrix and Statistics Reference Prediction No Yes No 544 37 Yes 258 44 Accuracy : 0.6659 95% CI : (0.6337, 0.697) No Information Rate : 0.9083 P-Value [Acc &gt; NIR] : 1 Kappa : 0.0995 Mcnemar&#39;s Test P-Value : &lt;2e-16 Sensitivity : 0.54321 Specificity : 0.67830 Pos Pred Value : 0.14570 Neg Pred Value : 0.93632 Prevalence : 0.09173 Detection Rate : 0.04983 Detection Prevalence : 0.34202 Balanced Accuracy : 0.61076 &#39;Positive&#39; Class : Yes PCC = 83.34% ROC Curve for Holdout Sample Area between \\(.5\\) and \\(.7\\) suggests poor model fit Figure 4.6: ROC Curve for Test/Holdout Sample (R code) 4.5.4 Interpreting Coefficients age is positive (\\(OR&gt;1\\)) and significant (\\(p=.0017\\)) \\(1\\) year increase in age increases odds of buying by a factor of \\(1.021\\) (or odds of buying increase by \\(2.1\\%\\)) married is negative (\\(OR&lt;1\\)) and significant (\\(p&lt;.0001\\)) Being married decreases odds of buying by factor of \\(.56\\) (or odds of buying decrease by \\(44\\%\\)) housing is negative (\\(OR&lt;1\\)) and significant (\\(p&lt;.0001\\)) Having a home loan decreases odds of buying by factor of \\(.56\\) (or odds of buying decrease by \\(44\\%\\)) loan is negative (\\(OR&lt;1\\)) and significant (\\(p=.0025\\)) Having a personal loan decreases odds of buying by factor of \\(.50\\) (or odds of buying decrease by \\(50\\%\\)) 4.5.5 Interpreting Coefficients Visually Figure 4.7: Margin Plots for Significant IVs (R code) 4.5.6 Gain Chart Contacting top \\(20\\%\\) of predicted buyers yields about \\(40\\%\\) of actual buyers Contacting top \\(30\\%\\) of predicted buyers yields about \\(52\\%\\) of actual buyers Figure 4.8: Gain Chart (R code) 4.5.7 Lift Chart Contacting top \\(20\\%\\) of predicted buyers provides a lift of about \\(2\\) Figure 4.9: Lift Chart (R code) 4.5.8 Conclusion Recall our goal: Predict current customers likely to buy What did we learn? We can identify those more likely to buy by examining age, married, housing, and loan By targeting those customers that are more likely to purchase, we can better spend our limited resources 4.6 Suggested Readings Marketing Data Science (2015). Miller, Thomas W. BGSU Library Link:http://maurice.bgsu.edu:2083/record=b41416968~S0 eBook through BGSU Library:https://learning.oreilly.com/library/view/marketing-data-science/9780133887662/?ar=Note: Might need to create an account; select “Not Listed. Click here” from the “Select your institution” drop down box. Use your BGSU email to create the account. Chapter 3: Targeting Current Customers 4.7 R Code Figure 4.1 # Need to detach package &#39;cowplot&#39; to prevent an error detach(&quot;package:cowplot&quot;, unload = TRUE) ggpairs(telecom[,1:6], # Dataset lower=list(continuous= wrap(&quot;smooth&quot;, method=&quot;lm&quot;, se=FALSE, # Add fit line color=&quot;midnightblue&quot;)), # Set dot color diag=list(continuous=&quot;blankDiag&quot;)) # Set diagonals to be blank # Reload &#39;cowplot&#39; library(cowplot) Figure 4.2 # Use package &#39;effects&#39; library(effects) # Create data frame of values to be plotted for &#39;avg6mou&#39; as focal a6.p &lt;- data.frame(predictorEffect(&quot;avg6mou&quot;, target, focal.levels=seq(0,1500,30))) # Create plot and assign to p1 p1 &lt;- a6.p %&gt;% ggplot(aes(x=avg6mou, y=fit)) + geom_line(size=1) + geom_ribbon(aes(ymin=lower, ymax=upper), alpha=0.2) + labs(x=&quot;Mean Monthly Minutes&quot;, y=&quot;Linear Prediction&quot;) + scale_y_continuous(limits=c(30,120), breaks=seq(30,120,15)) # Repeat for other Significant IVs cc.p &lt;- data.frame(predictorEffect(&quot;cc&quot;, target, focal.levels=seq(0,10,.2))) p2 &lt;- cc.p %&gt;% ggplot(aes(x=cc, y=fit)) + geom_line(size=1) + geom_ribbon(aes(ymin=lower, ymax=upper), alpha=0.2) + labs(x=&quot;Mean Monthly Customer Care Calls&quot;, y=&quot;Linear Prediction&quot;) + scale_y_continuous(limits=c(30,120), breaks=seq(30,120,15)) da.p &lt;- data.frame(predictorEffect(&quot;da&quot;, target, focal.levels=seq(0,10,.2))) p3 &lt;- da.p %&gt;% ggplot(aes(x=da, y=fit)) + geom_line(size=1) + geom_ribbon(aes(ymin=lower, ymax=upper), alpha=0.2) + labs(x=&quot;Mean Monthly Directory Assistance Calls&quot;, y=&quot;Linear Prediction&quot;) + scale_y_continuous(limits=c(30,120), breaks=seq(30,120,15)) #Arrange in grid using &#39;cowplot&#39; plot_grid(p1, p2, p3, nrow=2) Figure 4.3 om.p &lt;- data.frame(predictorEffect(&quot;ovrmou&quot;, target, focal.levels=seq(0,300,6))) p4 &lt;- om.p %&gt;% ggplot(aes(x=ovrmou, y=fit, color=own, fill=own)) + geom_line(size=1) + geom_ribbon(aes(ymin=lower, ymax=upper), alpha=0.2) + labs(x=&quot;Mean Monthly Overage Minutes&quot;, y=&quot;Linear Prediction&quot;, color=&quot;Homeowner&quot;, fill=&quot;Homeowner&quot;) + scale_y_continuous(limits=c(30,120), breaks=seq(30,120,15)) + theme(legend.position = &quot;bottom&quot;) own.p &lt;- data.frame(predictorEffect(&quot;own&quot;, target, xlevels=list(ovrmou=seq(0,300,100)))) p5 &lt;- own.p %&gt;% ggplot(aes(x=own, y=fit, group=1)) + geom_point(size=2) + geom_line(color=&quot;orange&quot;) + geom_errorbar(aes(ymin=lower, ymax=upper), width=.5) + facet_wrap(~ovrmou) + labs(x=&quot;Home Ownership&quot;, y=&quot;Linear Prediction&quot;) + scale_y_continuous(limits=c(30,120), breaks=seq(30,120,15)) plot_grid(p4, p5, nrow=1) Figure 4.4 # Create data frame of values to be plotted for &#39;avg6mou&#39; as focal inc.p &lt;- data.frame(predictorEffect(&quot;income&quot;, target)) # Create plot inc.p %&gt;% ggplot(aes(x=income, y=fit)) + geom_line(size=1) + geom_ribbon(aes(ymin=lower, ymax=upper), alpha=0.2) + labs(x=&quot;Income (000s)&quot;, y=&quot;Linear Prediction&quot;) + scale_y_continuous(limits=c(30,120), breaks=seq(30,120,15)) Figure 4.7 age.p &lt;- data.frame(predictorEffect(&quot;age&quot;, tgt.log)) p6 &lt;- age.p %&gt;% ggplot(aes(x=age, y=fit)) + geom_line(size=1) + geom_ribbon(aes(ymin=lower, ymax=upper), alpha=0.2) + labs(x=&quot;Age&quot;, y=&quot;Pr(Response)&quot;) + scale_y_continuous(limits=c(0,.3)) mar.p &lt;- data.frame(predictorEffect(&quot;married&quot;, tgt.log)) p7 &lt;- mar.p %&gt;% ggplot(aes(x=married, y=fit, group=1)) + geom_point(size=4) + geom_line(color=&quot;orange&quot;) + geom_errorbar(aes(ymin=lower, ymax=upper), width=.5) + labs(x=&quot;Married&quot;) + theme(axis.title.y=element_blank()) + scale_y_continuous(limits=c(0,.3)) ln.p &lt;- data.frame(predictorEffect(&quot;loan&quot;, tgt.log)) p8 &lt;- ln.p %&gt;% ggplot(aes(x=loan, y=fit, group=1)) + geom_point(size=4) + geom_line(color=&quot;orange&quot;) + geom_errorbar(aes(ymin=lower, ymax=upper), width=.5) + labs(x=&quot;Personal Loan&quot;, y=&quot;Pr(Response)&quot;) + scale_y_continuous(limits=c(0,.3)) hl.p &lt;- data.frame(predictorEffect(&quot;housing&quot;, tgt.log)) p9 &lt;- hl.p %&gt;% ggplot(aes(x=housing, y=fit, group=1)) + geom_point(size=4) + geom_line(color=&quot;orange&quot;) + geom_errorbar(aes(ymin=lower, ymax=upper), width=.5) + labs(x=&quot;Housing Loan&quot;) + theme(axis.title.y=element_blank()) + scale_y_continuous(limits=c(0,.3)) plot_grid(p6, p7, p8, p9, nrow=2) Figure 4.8 # Call the function and assign to object named &#39;glresults&#39; glresults &lt;- gainlift(tgt.log, # Name of the glm results object train, # Name of the training data frame test, # Name of the testing data frame &quot;Yes&quot;) # Level that represents success/true # Output gainplot glresults$gainplot Figure 4.9 # Output liftplot glresults$liftplot Table 4.1 summary(telecom) Table 4.2 target &lt;- lm(avg6rev ~ avg6mou + cc + da + ovrmou + income + own, data=telecom) summary(target) Table 4.3 telecom %&gt;% cbind(., yhat=fitted(target)) %&gt;% # Append fitted values to data mutate(yhat.dec=11-ntile(yhat, 10), # Create deciles, but reverse order own=as.numeric(own)-1) %&gt;% # Covert own to 1=yes, 0=no group_by(yhat.dec) %&gt;% # Group by decile summarise(across(avg6rev:own, ~mean(.x, na.rm=TRUE))) %&gt;% # Calculate mean of each IV flextable() # Nice table Table 4.4 # Use &#39;caret&#39; package to create training and test/holdout samples library(caret) # This will create two separate dataframes: train and test set.seed(9999) inTrain &lt;- createDataPartition(y=bankmktg$response, p=.75, list=FALSE) train &lt;- bankmktg[inTrain,] test &lt;- bankmktg[-inTrain,] # Estimate using training sample tgt.log &lt;- glm(response ~ age + balance + default + housing + educ + loan + married, data=train, family=&quot;binomial&quot;) summary(tgt.log) # Get p-value p &lt;- with(tgt.log, pchisq(null.deviance - deviance, df.null-df.residual, lower.tail=FALSE)) cat(&quot;Model p-value =&quot;,sprintf(&quot;%.4f&quot;,p),&quot;\\ &quot;) # Calculate McFadden&#39;s R-sq Mrsq &lt;- 1-tgt.log$deviance/tgt.log$null.deviance cat(&quot;McFadden&#39;s Pseudo-Rsquared = &quot;, round(Mrsq, digits=4)) flextable(or_table(tgt.log)) "],["segmentation.html", "Topic 5 Segmentation 5.1 R Packages and Datasets for Topic 5 5.2 Segmentation Overview 5.3 Cluster Analysis 5.4 Segmentation (Cluster Analysis) Example 5.5 Suggested Readings 5.6 R Code", " Topic 5 Segmentation 5.1 R Packages and Datasets for Topic 5 library(ggplot2) # Advanced graphing capabilities library(flextable) # Better HTML Tables library(dplyr) # Easier programming library(dendextend) # Nicer dendrograms library(vtable) # Nicer tables library(MKT4320BGSU) data(ffseg) 5.2 Segmentation Overview 5.2.1 Why segment the market? Markets are heterogeneous Difficult to meet needs with one marketing mix Can create homogeneous segments Each segment offered a different marketing mix 5.2.2 Market Segment vs. Segmentation Market Segment Subgroup of people or organizations sharing one or more characteristics that cause them to have similar product needs. Market Segmentation Process of dividing a market into meaningful, relatively similar, and identifiable segments or groups. 5.2.3 Criteria for Successful Segmentation Identifiable and Measurable Characteristics provide a basis for segmentation Substantial Large and profitable enough to make it worthwhile Accessible Able to receive communications and distributions Responsive Respond to marketing efforts and changes in mix 5.2.4 Bases for Consumer Markets Behavioral Segmentation Benefits sought (e.g., quality, taste, convenience, excitement, etc.) Product usage (e.g., heavy, light, non, former, first-time, etc.) Usage situations (e.g., celebration, emergency, everyday, etc.) Price sensitivity (e.g., value-conscious, status-conscious, etc.) Demographic Segmentation Age Gender Income Occupation Education Ethnicity Generation Psychographic Segmentation Personality (e.g., outgoing, shy, materialistic, controlled, etc.) Lifestyle (e.g., homebody, couch potato, workaholic, etc.) Motives (e.g., safety, status, relaxation, convenience, etc.) Geographic Segmentation Regional City size Population density Block group Climate 5.2.4.1 Which base to use? Using Geographics or Demographics is easy, but it won’t help determine needs. Knowing how a product fits a lifestyle or benefits consumers seek is important, but it won’t help identify specific customers. A combination of bases likely works best, but the bases to be used depend on the purpose of segmentation. 5.3 Cluster Analysis 5.3.1 Overview Cluster analysis classifies objects so that objects are similar to others in the cluster with respect to some predetermined criterion. Resulting clusters exhibit: High within-cluster homogeneity (\\(a\\) in Figure 5.1) High between-cluster heterogeneity (\\(b\\) in Figure5.1) Figure 5.1: Cluster Example Basic steps Choose variables (i.e., bases) Define measure of similarity Develop method for assigning objects 5.3.2 Measures of Similarity Similarity measure depends on data type For continuous data…Use distance-type measures For binary data…Use similarity coefficients For mixed data…Use special measures 5.3.2.1 Continuous data Distance-type measures Euclidean: \\(\\sqrt{(x_{1i}-x_{1j})^2 + \\cdots +(x_{ni}-x_{nj})^2}\\) Squared Euclidean: \\((x_{1i}-x_{1j})^2 + \\cdots +(x_{ni}-x_{nj})^2\\) Absolute: \\(|x_{1i}-x_{1j}|^2 + \\cdots +|x_{ni}-x_{nj}|^2\\) NOTE: If all variables are continuous and measured on different scales, best to standardize them first Euclidean Distance Example Suppose three individuals rated importance of three attributes Importance Person Price Quality Service Alex 6 5 6 Sam 5 1 6 Pat 6 6 5 Calculate Euclidean Distances between each pair Alex/Sam: \\(\\sqrt{6-5)^2+(5-1)^2+(7-2)^2}=\\sqrt{42}=6.48\\) Alex/Pat: \\(\\sqrt{6-6)^2+(5-6)^2+(7-5)^2}=\\sqrt{5}=2.24\\) Sam/Pat: \\(\\sqrt{5-6)^2+(1-6)^2+(2-5)^2}=\\sqrt{35}=5.92\\) Pairwise Similarity Matrix Person Alex Sam Pat Alex 0 NA NA Sam 6.48 0 NA Pat 2.24 5.92 0 5.3.2.2 Binary Data Similarity Coefficients Based on a \\(2×2\\) cross-tab between objects \\(a\\) = number of variables where \\(i\\) and \\(j\\) both had \\(1\\)s \\(d\\) = number of variables where \\(i\\) and \\(j\\) both had \\(0\\)s \\(b\\) = number of variables where \\(i\\) is \\(1\\) and \\(j\\) is \\(0\\) \\(c\\) = number of variables where \\(i\\) is \\(0\\) and \\(j\\) is \\(1\\) Matching coefficient: \\((a+d)/(a+b+c+d)\\) Research Person Price Quality Service Alex 1 1 1 Sam 1 0 0 Pat 0 0 0 Calculate matching coefficient between each pair Alex/Sam: \\((1+0)/(1+2+0+0)=1/3=33.3\\%\\) Alex/Pat: \\((0+0)/(0+3+0+0)=0/3=0.0\\%\\) Sam/Pat: \\((0+2)/(0+1+0+2)=2/3=66.6\\%\\) Pairwise similarity matrix Person Alex Sam Pat Alex 100% NA NA Sam 33.3% 100% NA Pat 0.0% 66.6% 100% 5.3.2.3 Mixed data Special measures Gower coefficent Creates a measure between 0 and 1 for each variable For continuous variables: For each variable, absolute distance divided by range for that variable For binary variables: Matching as before 5.3.3 Assigning Objects Heirarchical: Construct a treelike structure up (agglomerative) or down (divisive) based on similarity of objects Partitioning: Assign objects to one of a specified number of clusters based on similarity of object to the cluster 5.3.3.1 Hierarchical: Agglomerative 5.3.3.1.1 Stages Consider each object as its own cluster Join two closest objects based on similarity measure and algorithm Join next two closest objects (individual or cluster) Repeat step 3 until all items clustered Figure 5.2: Sample Dendrogram 5.3.3.1.2 Linkage Rules How to compute distance between clusters? Four main linkage rules Single Join clusters where minimum distance of any two cases between clusters is smallest Figure 5.3: Single Linkage Visualization \\[\\begin{align} \\left. \\begin{array}{l} d_1 = .55 \\\\ d_2 = .84 \\\\ d_3 = .85 \\end{array}\\right\\} \\text{Red and Blue clusters will be joined} \\end{align}\\] Complete Join clusters where maximum distance of any two cases between clusters is smallest Figure 5.4: Complete Linkage Visualization \\[\\begin{align} \\left. \\begin{array}{l} d_1 = 1.51 \\\\ d_2 = 1.51 \\\\ d_3 = 1.34 \\end{array}\\right\\} \\text{Red and Green clusters will be joined} \\end{align}\\] Average Minimum average distance between all cases in one cluster and another cluster Figure 5.5: Average Linkage Visualization Ward’s Join clusters with a minimum increase in the sum of squared distances within all clusters combined 5.3.3.1.3 Problems Which linkage rule? Ward’s is often a good choice, but may need to look at others based on results How many clusters? With small samples, dendrograms work well With large samples, can use stopping rules Consider managerial implications 5.3.3.1.4 How Many Clusters? Using a dendrogram for small samples Draw horizontal line to visualize clusters easier Figure 5.6: Horizontal Line Showing 4 Clusters Figure 5.7: 3-Cluster Solution Figure 5.8: 4-Cluster Solution Use a stopping rule for large samples The Duda-Hart stopping rule provides an index value for different cluster solutions Large values of the index, combined with small values of pseudo-T-squared values indicate more distinct solutions Num.Clusters Duda.Hart pseudo.t.2 1 0.8091 177.0092 2 0.8867 56.7367 3 0.8599 58.4966 4 0.9090 30.4322 5 0.8423 37.4369 6 0.8787 24.4393 7 0.8562 26.3746 8 0.8852 16.2166 9 0.7997 27.3073 10 0.8550 17.2916 11 0.8116 19.7330 12 0.8173 18.5591 13 0.8495 15.7674 14 0.8112 16.2956 15 0.8576 13.7770 Partitioning: \\(k\\)-means 5.3.3.1.5 Stages Identify number of clusters desired Identify the cluster “seeds” Allocate all objects to cluster based on distance from “seed” Calculate new cluster centers Reallocate items based on distance to cluster centers one at a time Repeat 4 &amp; 5 until within-cluster variability is minimized 5.3.3.1.6 Illustration for 3-cluster solution Ask for 3-cluster solution Randomly assign cluster seeds Allocate all objects based on distance to seeds Calculate new cluster centers Reallocate items based on distance to cluster centers one at a time Repeat Step 4: Calculate new cluster centers Repeat Step 5: Reallocate items based on distance to cluster centers one at a time Repeat Step 4: Calculate new cluster centers At this point, no further reallocation can make the within cluster variance smaller, so this is the best 3-cluster solution 5.3.3.1.7 How many clusters? Create a scree plot based on the within sum of squares (WSS) for about 1 to 15 different solutions Compare solutions around the scree Consider managerial implications Figure 5.9: Example Scree Plot 5.3.4 Describe the Segments Answers the question: How do the clusters differ on relevant dimensions? Create a profile using measures of central tendency, dispersion, etc. for variables used in clustering Profile clusters using data not included in clustering to get a feel for who is in the clusters Can use statistical techiniques to help identify characteristics that predict cluster membership 5.4 Segmentation (Cluster Analysis) Example 5.4.1 Overview Goal Segment students based on importance of attributes when choosing a fast food restaurant Keep number of clusters manageable Bases: Attribute Importance Cleanliness (\\(clean\\)) Variety (\\(variety\\)) Food Quality (\\(quality\\)) Location (\\(location\\)) Service Speed (\\(speed\\)) Price (\\(price\\)) Healthy options (\\(healthy\\)) Friendliness (\\(friendly\\)) Additional Information Frequency of eating out Class level Meal plan Gender Living location College 5.4.2 Examine the Data Table 5.1: Summary Statistics for Cluster Variables (R code) Variable N Mean Std. Dev. Min Pctl. 25 Pctl. 75 Max clean 752 4.1 0.9 1 4 5 5 variety 752 3.3 0.99 1 3 4 5 quality 752 4.2 0.81 1 4 5 5 location 752 3.7 0.97 1 3 4 5 speed 752 3.7 0.9 1 3 4 5 healthy 752 3 1.2 1 2 4 5 price 752 4 0.9 1 3 5 5 friendly 752 3.3 1 1 3 4 5 Nothing out of the ordinary for survey data 5.4.3 Hierarchical Cluster Analysis Distance measure: Euclidean All variables are continuous and on the same scale, but will be standardized anyway Linkage method: Ward’s Over \\(750\\) cases… So dendrogram won’t help Calculate Duda-Hart index for \\(1\\) to \\(15\\) clusters Look at top \\(5\\) index values \\(2\\), \\(3\\), \\(4\\), and \\(6\\) cluster solutions seem like good candidates to explore Table 5.2: Duda-Hart Index (R code) Num.Clusters Duda.Hart pseudo.t.2 1 0.8392 143.7320 2 0.8974 63.8291 3 0.8792 55.2532 4 0.8560 43.4065 5 0.8537 26.3963 6 0.8680 27.6823 7 0.8973 21.7445 8 0.8581 23.4818 9 0.8527 19.1770 10 0.8345 24.7925 Examine dendrogram of top 6 branches only Figure 5.10: Dendrogram with Top 6 Branches Only (R code) Examine cluster sizes for different cluster solutions Overall, 4-cluster solution is chosen All clusters are of decent size and no one cluster is too large Table 5.3: Cluster Sizes for Different \\(k\\) Solutions (R code) Cluster k_2_Count k_3_Count k_4_Count k_6_Count 1 560 404 260 192 2 192 192 192 184 3 NA 156 156 144 4 NA NA 144 127 5 NA NA NA 76 6 NA NA NA 29 Cluster k_2_Percent k_3_Percent k_4_Percent k_6_Percent 1 74.47 53.72 34.57 25.53 2 25.53 25.53 25.53 24.47 3 NA 20.74 20.74 19.15 4 NA NA 19.15 16.89 5 NA NA NA 10.11 6 NA NA NA 3.86 Describe segments using cluster variables (table and chart) Cluster \\(1\\) seems to rate most everything as important, but especially \\(clean\\), \\(quality\\), \\(price\\), and \\(location\\) Cluster \\(4\\) doesn’t seem to think much is important, except maybe \\(price\\), and really doesn’t care about \\(healthy\\), \\(friendly\\), and \\(variety\\) Cluster \\(3\\) rates \\(quality\\), \\(clean\\) and somewhat \\(healthy\\) as important, but not much else Cluster \\(2\\) doesn’t have any attributes that are rated too high or too low compared to the other clusters Table 5.4: Cluster Variable Means by Cluster (R code) Cluster clean variety quality location speed healthy price friendly 1 4.69 3.75 4.81 4.41 4.29 3.84 4.54 4.12 2 4.12 3.30 4.09 3.88 3.71 2.67 4.03 3.36 3 4.51 3.83 4.65 2.99 3.27 4.03 3.83 3.47 4 3.27 2.69 3.38 3.42 3.34 2.17 3.67 2.49 Figure 5.11: Mean Attribute Importance by Cluster (R code) Describe segments using non-cluster variables Only the variable \\(eatin\\) (1 = Prefer to eat in; 5 = Prefer Get to Go) and \\(gender\\) showed significant differences between clusters Cluster \\(3\\) was significantly more likely to want to eat in than Clusters \\(2\\) and \\(4\\) Cluster \\(4\\) was significantly more likely to be male than the other clusters Table 5.5: Eat-In and Gender Means by Cluster (R code) Cluster eatin 1 4.03 2 4.30 3 3.82 4 4.48 Cluster Female 1 0.78 2 0.71 3 0.81 4 0.60 5.4.4 k-Means Cluster Analysis Distance Measure All variables are continuous and on the same scale, but will be standardized anyway Calculate WSS to create a scree plot for \\(1\\) to \\(15\\) cluster solutions Somewhere between \\(3\\) and \\(5\\) clusters looks best Figure 5.12: Scree Plot for Fast Food Data (R code) Quick examination of \\(3\\), \\(4\\), and \\(5\\) cluster solutions Each solution has good sized clusters Use \\(3\\) cluster solution because cluster sizes closest to each other… And we saw a \\(4\\) cluster solution using hierarchical agglomerative clustering Table 5.6: Cluster Sizes 3, 4, and 5 Cluster Solutions (R code) Num_Clusters k_3_Count k_4_Count k_5_Count 1 266 269 252 2 261 174 149 3 225 164 138 4 NA 145 113 5 NA NA 100 &lt;/tr&gt; Examine cluster centers (table and chart) Similar to describing segments using cluster variables Cluster \\(1\\) doesn’t think much is important other than \\(location\\) and \\(price\\) Cluster \\(2\\) thinks most everything is important Cluster \\(3\\) thinks \\(clean\\), \\(quality\\), and \\(healthy\\) are much more important than other attributes Table 5.7: Cluster Centers for 3 Cluster Solution (R code) clean variety quality location speed healthy price friendly Cluster -0.8760 -0.4746 -0.8286 -0.0075 -0.2216 -0.7727 -0.0256 -0.6548 1 0.5076 0.5694 0.5503 0.5832 0.6334 0.4340 0.5615 0.5158 2 0.4161 -0.1227 0.3107 -0.6807 -0.4917 0.3833 -0.6341 0.1497 3 Figure 5.13: Cluster Centers by Cluster (R code) Describe segments using non-cluster variables Only the variables \\(eatin\\) (1 = Prefer to eat in; 5 = Prefer Get to Go), \\(gender\\), and \\(live\\) showed significant differences between clusters Cluster \\(3\\) was significantly more likely to want to eat in than both other clusters Cluster \\(1\\) was significantly more likely to be male than both other clusters Cluster \\(3\\) was significantly more likely to be a commuter than both other clusters Table 5.8: Eat-In, Gender and Live Means by Cluster (R code) Cluster eatin 1 4.40 2 4.29 3 3.85 Cluster Female 1 0.60 2 0.81 3 0.73 Cluster Commuter Off.Campus On.Campus 1 0.08 0.45 0.47 2 0.08 0.46 0.45 3 0.16 0.40 0.44 5.5 Suggested Readings Marketing Data Science (2015). Miller, Thomas W. BGSU Library Link:http://maurice.bgsu.edu:2083/record=b41416968~S0 eBook through BGSU Library:https://learning.oreilly.com/library/view/marketing-data-science/9780133887662/?ar=Note: Might need to create an account; select “Not Listed. Click here” from the “Select your institution” drop down box. Use your BGSU email to create the account. Chapter 4: Finding New Customers Principles of Marketing Engineering and Analytics, 3rd Edition (2017). Lilien, Gary L., Rangaswamy, Arvind, and De Bruyn, Arnaud. Course reserves Pages 75-97 from Chapter 3: Segmentation and Targeting Multivariate Data Analysis. Hair, Joseph F.; Black, William C.; Babin, Barry J.; Anderson, Rolph E. 7th Edition: Search for “multivariate data analysis 7th edition hair” Chapter 8: Cluster Analysis 5th Edition: Course reserves Chapter 9: Cluster Analysis 5.6 R Code Figure 5.10 out &lt;- myhc(segdata, # Dataframe with cluster variables dist=&quot;euc&quot;, # Use &#39;euclidean&#39; distance measure method=&quot;ward&quot;, # Use &#39;Wards&#39; linkage cuts=c(2,3,4,6), # Get data for 4 cut levels clustop=&quot;N&quot;) # Do not get Duda-Hart out$kcount # Table with cluster sizes out$kperc # Table with cluster size proportions Figure 5.11 library(reshape2) # Used to covert data from wide to long # Convert &#39;wide&#39; table to &#39;long&#39; table sumsegl &lt;- melt(outseg$means, # Wide table of summary statistics id.vars=&quot;Cluster&quot;, # ID variable variable.name=&quot;attr&quot;, # Column name identifying attribute value.name=&quot;mean&quot;) # Column name identifying value # Plot data sumsegl %&gt;% ggplot(aes(x=attr, y=mean, fill=Cluster)) + geom_col(position=&quot;dodge&quot;) + coord_cartesian(ylim=c(1,7)) + scale_y_continuous(breaks=seq(1,7,1)) + geom_text(aes(label=round(mean,2)), hjust=1.1, fontface=&quot;bold&quot;, angle=90, position=position_dodge(width=.9)) + theme(legend.position=&quot;bottom&quot;) + labs(x=&quot;Attribute&quot;, y=&quot;Mean Importance&quot;, fill=&quot;Cluster&quot;) Figure 5.12 wssplot(segdata, # Standardized data from earlier 15, # Max clusters seed=4320) # Random number seed Figure 5.13 # outkc was created using the code for thetable outkc$plot Table 5.1 # Create df with attribute importance variables segdata &lt;- ffseg %&gt;% select(clean, variety, quality, location, speed, healthy, price, friendly) # Create summary table with package &#39;vtable&#39; (not availalbe in virtual env.) sumtable(segdata, out=&quot;return&quot;) %&gt;% kable(caption=NULL) Table 5.2 # Stardarize data using &#39;scale()&#39; function segdata &lt;- data.frame(scale(segdata)) # Requires package &#39;NbClust&#39;, which isn&#39;t availabe in virtual env. library(NbClust) dh &lt;- clustop(segdata, # Dataframe to with cluster variables dist=&quot;euc&quot;, # Use &#39;euclidean&#39; distance measure method=&quot;ward&quot;, # Use &#39;Wards&#39; linkage minclust=1, maxclust=10) # Only complete for up to 10 clusters dh Table 5.3 # &#39;out&#39; was a list of two data frame (&#39;kcount&#39; and &#39;kperc&#39;) returned from the # call to the &#39;myhc&#39; user defined function out$kcount # Table with cluster sizes out$kperc # Table with cluster size proportions Table 5.4 ffseg$c4 &lt;- as.factor(cutree(out$hc,4)) # Add cluster membership to original # Create object with segmentation variables listed segvars &lt;- c(&quot;clean&quot;, &quot;variety&quot;, &quot;quality&quot;, &quot;location&quot;, &quot;speed&quot;, &quot;healthy&quot;, &quot;price&quot;, &quot;friendly&quot;) outseg &lt;- cldescr(ffseg, # Orig data with cluster membership added segvars, # Segmentation variables &quot;C&quot;, # Indication that variables are continuous &quot;c4&quot;) # Cluster membership variable name # Table of means outseg$means Table 5.5 outnseg &lt;- cldescr(ffseg, # Orig data with cluster membership added &quot;eatin&quot;, # Prefer to eat in (1) vs. get to go (5) &quot;C&quot;, # Indication that variable is continuous &quot;c4&quot;) # Cluster membership variable name # Table of means outnseg$means outnseg &lt;- cldescr(ffseg, # Orig data with cluster membership added &quot;gender&quot;, # Male or Female &quot;F&quot;, # Indication that variable is factor &quot;c4&quot;) # Cluster membership variable name # Table of means outnseg$means Table 5.6 ks &lt;- ksize(segdata, # Scaled cluster variables from earlier centers=c(3,4,5), # Request for 3, 4, and 5 cluster solutions nstart=25, # Request 25 random starting sets seed=4320) # Set seed to 4320 for reproducible results ks$kcount ks$kperc Table 5.7 set.seed(4320) # Match seed from above k3 &lt;- kmeans(segdata, # Scaled segmentation variables centers=3, # 3 cluster solution nstart=25) # 25 random starting sets outkc &lt;- kcenters(k3) # Call to &#39;kcenters&#39; passing the &#39;k3&#39; cluster object # outkc contains two objects: &#39;table&#39; and &#39;plot&#39; outkc$table # Print table Table 5.8 # Add cluster membership to original data fseg$k3 &lt;- factor(k3$cluster) # Use &#39;cldescr.R&#39; user defined function outk3seg &lt;- cldescr(ffseg, # Orig data with cluster membership added &quot;eatin&quot;, # Prefer to eat in (1) vs. get to go (5) &quot;C&quot;, # Indication that variable is continuous &quot;k3&quot;) # Cluster membership variable name # Table of means outk3seg$means outk3seg &lt;- cldescr(ffseg, # Orig data with cluster membership added &quot;gender&quot;, # Male or Female &quot;F&quot;, # Indication that variable is factor &quot;k3&quot;) # Cluster membership variable name # Table of means outk3seg$means outk3seg &lt;- cldescr(ffseg, # Orig data with cluster membership added &quot;live&quot;, # Where they live &quot;F&quot;, # Indication that variable is factor &quot;k3&quot;) # Cluster membership variable name # Table of means outk3seg$means "],["positioning.html", "Topic 6 Positioning 6.1 R Packages and Datasets for Topic 5 6.2 Positioning Overview 6.3 Factor Analysis 6.4 Principal Components Analysis (PCA) 6.5 Creating a Percptual Map using PCA 6.6 Joint-Space Maps 6.7 Positioning Example 6.8 Suggested Readings 6.9 R Code", " Topic 6 Positioning 6.1 R Packages and Datasets for Topic 5 library(ggplot2) # Advanced graphing capabilities library(flextable) # Better HTML Tables library(dplyr) # Easier programming library(dendextend) # Nicer dendrograms library(vtable) # Nicer tables library(MKT4320BGSU) data(greekbrands) load(&quot;Topic06/ffattrib.rdata&quot;) 6.2 Positioning Overview 6.2.1 The Concept of Positioning Positioning is not what you do to a product Positioning is what you do to the mind of the prospect 6.2.2 Position vs. Positioning Position The place a brand, product line, or organization in general occupies in consumers’ minds relative to competing offerings. Positioning Developing a specific marketing mix to influence potential customers’ overall perception of a brand, product line, or organization in general. 6.2.3 Difficulty of Positioning Positioning is easy when your product is clearly superior in an important way… But few are Most product markets have a lot of parity, so… Positioning is more important and more difficult 6.2.4 Competitive Market Structure Difficulty of positioning depends on level of product competitiveness Easy positioning… Product Superiority: Clearly superior in many important ways “In the middle” positioning… Product Differentiation: One or more specific features superior to the competition (but otherwise similar) Difficult positioning… Product Parity: No essential differences from one product to another 6.2.5 Positioning with Product Parity Remember… Positioning is what you do to the mind of the prospect Thus… Need to create points of difference, whether: They exist or not They are meaningful or not 6.2.6 Requirements for Effective Positioning Uniqueness Desirability Believability Not all three are required… But the more there are, the more likely the positioning will be effective 6.2.6.1 Uniqueness Be different in some way Must get through the clutter Slightly better on an important feature-vs.-Unique on a less important feature More brands = more difficult Find a niche that is not yet occupied 6.2.6.2 Desirability Difference should be on something relatively important to the consumer Example: Is being clear in the cola market important? Don’t look for any gap in the market…Look for gaps that turn prospects into buyers 6.2.6.3 Believability Claims of superiority must be accepted Believability rests on: How reasonable the claim is Objective support for the claim A common error is to strive for too many positions 6.2.7 Steps in Positioning Assess the positions occupied by competing products Determine the dimensions underlying these positions Choose a market position where efforts will have the greatest impact 6.2.8 Positioning Research Methods 6.2.8.1 Image Profile Average ratings of brands/products on a number of attributes Easy to create, but… Difficult to interpret 6.2.8.2 Quadrant Analysis x-axis contains the average ratings of one brand/product on a number of attributes y-axis contains the average importance of each attribute Easily shows what attributes to emphasize, but… Only looks at one brand/product at a time 6.2.8.3 Perceptual Maps Shows the location of competing brands/ products in a “virtual” space Enable marketers to see at a glance how own brand/product relates to the competition 6.2.8.3.1 Types of Perceptual Maps Discriminant Analysis Maps Based on identifying differences between objects with respect to several variables simultaneously Multidimensional Scaling Maps Based only on similarities between objects Factor Analysis Maps Based on the dimensions underlying a set of variables EMPHASIS OF THIS CLASS 6.3 Factor Analysis Generic term for identifying dimensions underlying a set of variables Finds uncorrelated linear dimensions that capture the most variance in the data Main types: exploratory factor analysis principal components analysis 6.4 Principal Components Analysis (PCA) Recomputes a set of variables in terms of linear equations (components) that capture linear relationships in the data * First component captures as much variance as possible from all variables * Second component captures as much variance as possible that remains * Continue until as many components as variables * Analyst retains/analyzes a subset of components 6.4.1 PCA Process Determine number of factors to retain Rotate factors to aid interpretation Interpret factors Use results in further analysis (e.g., perceptual maps) 6.4.1.1 Step 1: Determine number of factors to retain How many factors? Eigenvalues &gt; 1 E.g., Keep two components, accounting for about 85% of variation Component Eigenvalue Difference Proporation Cumulative 1 3.7574 0.7588 0.4697 0.4697 2 2.9986 2.4108 0.3748 0.8445 3 0.5878 0.2561 0.0735 0.9180 4 0.3317 0.1386 0.0415 0.9594 5 0.1931 0.1045 0.0241 0.9836 6 0.0886 0.0616 0.0111 0.9946 7 0.0270 0.0112 0.0034 0.9980 8 0.0158 NA 0.0020 1.0000 Examine Scree Plot Sometimes called an elbow plot Look for bend or kink in the plot Number of components to retain is the number prior to that kink 6.4.1.2 Step 2: Rotate factors to aid interpretation Factor loadings are the correlation between each variable and each factor Higher loadings indicate the variable is representative of the factor Unfortunately, unrotated loadings may not provide a meaningful pattern to understand the factors PC1 PC2 Unexplained perform 0.3327 0.3073 0.3009 leader 0.3116 0.4276 0.0867 lattech -0.3656 0.2875 0.2500 fun -0.4010 -0.2957 0.1335 serious 0.2730 0.4499 0.1131 bargain 0.3722 -0.3160 0.1801 value 0.4229 -0.2843 0.0855 trendy -0.3253 0.4117 0.0941 Rotating the factors redistributes the variance from earlier factors to later factors to make more meaningful patterns Factor/component loading guidelines \\(\\rho&lt;0.4\\Longrightarrow\\) no loading \\(0.4\\le \\rho&lt;0.6\\Longrightarrow\\) low” loading \\(\\rho\\ge0.6\\Longrightarrow\\) “high” loading PC1 PC2 Unexplained perform 0.0490 0.4502 0.3009 leader -0.0459 0.5271 0.0867 lattech -0.4645 -0.0234 0.2500 fun -0.1081 -0.4864 0.1335 serious -0.0897 0.5185 0.1131 bargain 0.4882 0.0062 0.1801 value 0.5057 0.0634 0.0855 trendy -0.5157 0.0967 0.0941 6.4.1.3 Step 3: Interpret factors (not)\\(lattech\\), \\(bargain\\), \\(value\\), (not)\\(trendy\\) describe factor 1 Might label factor 1 as “latest” \\(perform\\), \\(leader\\), (not)\\(fun\\), and \\(serious\\) describe factor 2 Might label factor 2 as “performance” PC1 PC2 Unexplained perform 0.0490 0.4502 0.3009 leader -0.0459 0.5271 0.0867 lattech -0.4645 -0.0234 0.2500 fun -0.1081 -0.4864 0.1335 serious -0.0897 0.5185 0.1131 bargain 0.4882 0.0062 0.1801 value 0.5057 0.0634 0.0855 trendy -0.5157 0.0967 0.0941 6.4.1.4 Step 4: Use results in further analysis Results could be used in linear regerssion, logistic regression, cluster analysis, perceptual mapping, etc. 6.5 Creating a Percptual Map using PCA Steps: Map Brands Map Attributes Interpret Map 6.5.1 Map Brands Using rotated loading matrix and average brand scores for each variable, obtain a score for each brand on each component Attribute Alpha’s Mean Factor 1 Mean x Factor 1 Factor 2 Mean x Factor 2 perform -1.2952 0.0490 -0.0635 0.4502 -0.5831 leader -0.7840 -0.0459 0.0360 0.5271 -0.4133 lattech 0.5696 -0.4645 -0.2646 -0.0234 -0.0133 fun 1.0862 -0.1081 -0.1174 -0.4864 -0.5283 serious -1.2024 -0.0897 0.1079 0.5185 -0.6234 bargain 0.3347 0.4882 0.1634 0.0062 0.0021 value 0.2434 0.5057 0.1231 0.0634 0.0154 trendy -0.7042 -0.5157 0.3632 0.0967 -0.0681 -1.7519 -0.1810 0.3481 1.1523 -2.2120 Use factor scores for each brand as coordinates on the perceptual map brand scr1 scr2 Alpha 0.3480 -2.2120 Beta -0.8181 3.1213 Delta -1.9904 -1.5338 Eta 0.1985 -0.3866 Gamma -0.7339 2.8111 Kappa 2.6707 0.7768 Lambda 3.4634 0.5167 Sigma -2.0978 -0.8910 Theta -1.3777 -0.4682 Zeta 0.3373 -1.7343 6.5.2 Map Attributes Factor loadings serve as the coordinates for the attributes Loadings often need to be scaled to be used on the same map as the brands Figure 6.1: Loadings NOT Scaled Figure 6.2: Loadings Scaled 6.5.3 Interpret Map Length of line represents amount of variance explained for that attribute Lines extend in opposite direction Perpendicular line from brand to attribute vector shows how brand is perceived on that attribute Further along the vector, the higher the association between that brand and attribute Distance between brands shows how similar the market perceives them to be 6.6 Joint-Space Maps Perceptions vs. Preferences Preferences are fundamentally different than perceptions Customers may perceive a brand as safe, but it may not be an determinant attribute Preferences may not change according to the magnitude of an attribute Joint-space maps: Incorporates perceptions and preferences into same map Interpreted in a similar manner to attribute vectors Preference “driven” by attributes vectors most parallel to preference vector 6.7 Positioning Example 6.7.1 Overview Goal: Create a joint-space map of BGSU students perceptions and preferences of fast food restaurants Attributes: All measured on 5-point scale Cleanliness(-), \\(clean\\_neg\\) Convenience, \\(conv\\) Healthy options, \\(healthy\\) Variety(-), \\(variety\\_neg\\) Value, \\(value\\) Taste, \\(taste\\) 6.7.2 Examine the Data Mean (top table) and standard deviation (bottom table) for each restaurant for each attribute Nothing out of the ordinary Some attributes have more variation (\\(conv\\), \\(healthy\\), and \\(taste\\)) Table 6.1: Attribute Means (Top) and Standard Deviations (Bottom) (R code) rest clean_neg conv healthy variety_neg value taste Chipotle 2.025 3.850 3.900 2.650 3.800 4.250 Jimmy John’s 2.050 3.875 3.700 2.700 3.450 3.550 McDonald’s 2.825 4.325 1.975 2.275 3.750 2.925 Mr. Spots 2.475 2.950 2.600 2.675 3.250 3.675 Qdoba 2.150 2.975 3.575 2.550 3.200 3.425 Subway 2.450 3.525 4.175 2.225 3.400 3.650 Taco Bell 2.850 4.350 1.800 2.200 3.650 3.250 Wendy’s 2.625 4.200 2.725 2.075 3.625 3.775 rest clean_neg conv healthy variety_neg value taste Chipotle 0.947 1.099 0.982 1.075 1.159 1.104 Jimmy John’s 0.904 1.244 1.067 1.091 1.154 1.239 McDonald’s 1.035 0.859 0.891 1.012 1.171 1.163 Mr. Spots 0.816 1.176 0.778 0.971 1.006 0.971 Qdoba 0.864 1.097 0.931 0.932 1.067 1.196 Subway 1.037 1.176 1.059 1.050 1.033 1.122 Taco Bell 1.027 0.834 0.853 1.018 1.210 1.296 Wendy’s 0.868 0.883 0.784 0.888 0.897 0.974 Correlation matrix of attributes Some variables have fairly high correlations with others \\(taste\\) and \\(value\\) \\(clean\\_neg\\) and \\(healthy\\) \\(clean\\_neg\\) and \\(taste\\) Table 6.2: Attribute Correlation Matrix (R code) clean_neg conv healthy variety_neg value taste clean_neg 1.0000 0.0209 -0.4050 0.2101 -0.2069 -0.3557 conv 0.0209 1.0000 -0.0486 -0.3334 0.1905 0.1310 healthy -0.4050 -0.0486 1.0000 -0.1391 0.1504 0.3336 variety_neg 0.2101 -0.3334 -0.1391 1.0000 -0.2872 -0.2159 value -0.2069 0.1905 0.1504 -0.2872 1.0000 0.4677 taste -0.3557 0.1310 0.3336 -0.2159 0.4677 1.0000 6.7.3 How many factors How many factors should be retained? Only the first two components have eigenvalues &gt; 1, and they explain nearly 82% of the variation Thus, retaining two components seems appropriate Table 6.3: Eigenvalue Table (R code) Component Eigenvalue Difference Proporation Cumulative 1 3.4763 2.0409 0.5794 0.5794 2 1.4354 0.8174 0.2392 0.8186 3 0.6180 0.2509 0.1030 0.9216 4 0.3671 0.2927 0.0612 0.9828 5 0.0744 0.0455 0.0124 0.9952 6 0.0288 NA 0.0048 1.0000 Figure 6.3: Scree Plot (R code) 6.7.4 Rotate factors Rotate factors to aid in interpretation (after rerunning with 2 components) Cleanliness, Healthy options, and Taste load on the first dimension Convenience and Value load on the second dimension Variety doesn’t load much on either dimension Table 6.4: Rotated Factor Loadings (R code) PC1 PC2 Unexplained clean_neg 0.5398 -0.0824 0.0829 conv 0.0920 -0.6118 0.0803 healthy -0.5103 0.0374 0.2297 variety_neg -0.3231 0.2670 0.3916 value -0.1057 -0.7018 0.0738 taste -0.5694 -0.2316 0.2300 6.7.5 Create perceptual map What two restaurants are most similar? Which restaurant has the least variety? Which restaurant has the highest taste? Which two restaurants have the lowest value? Which attribute is least described by the map? Figure 6.4: Perceptual Map 6.7.6 Create joint-space map What attribute most drives preference? Which attribute least drives preference? Which two restaurants are least preferred? Figure 6.5: Joint Space Map (R code) 6.8 Suggested Readings Principals of Marketing Engineering and Analytics, 3rd Edition (2017). Lilien, Gary L., Rangaswamy, Arvind, and De Bruyn, Arnaud. Course reserves Chapter 4: Positioning Multivariate Data Analysis. Hair, Joseph F.; Black, William C.; Babin, Barry J.; Anderson, Rolph E. 7th Edition: Search for “multivariate data analysis 7th edition hair” Chapter 3: Exploratory Factor Analysis 5th Edition: Course reserves Chapter 9: Factor Analysis 6.9 R Code Figure 6.3 # Call plot from previously store numfact object (from output of call to &#39;pcaex&#39;) numfact$plot Figure 6.5 percmap(ffpos, # Data group=&quot;rest&quot;, # Brand/Group variable pref=&quot;pref&quot;) # Preference variable Table 6.1 # Create dataframe of attributes ffpos &lt;- ffattrib %&gt;% mutate(clean_neg=6-clean, # Reverse code &#39;clean&#39; variety_neg=6-variety) %&gt;% # Reverse code &#39;variety&#39; select(rest, clean_neg, conv, healthy, # Select needed columns variety_neg, value, taste, pref) # Include preference also # Examine data ffpos %&gt;% group_by(rest) %&gt;% select(-pref) %&gt;% # Don&#39;t examine preference summarise_all(mean) # &#39;summarise_all()&#39; summarizes all variables ffpos %&gt;% group_by(rest) %&gt;% select(-pref) %&gt;% # Don&#39;t examine preference summarise_all(sd) Table 6.2 # Correlation matrix on only continuous items minus preference cor(ffpos[,-c(1,8)]) Table 6.3 # Use user defined function and store results numfact &lt;- pcaex(ffpos, # Data group=&quot;rest&quot;, # Group/Brand variable pref=&quot;pref&quot;) # Preference variable (if it exists) numfact$table # Output the table Table 6.4 ff.2comp &lt;- pcaex(ffpos, # Data group=&quot;rest&quot;, # Group/Brand variable pref=&quot;pref&quot;, # Preference variable (if it exists) comp=2) Number of components ff.2comp$rotated # Request rotated factor loading table "],["ab-testing.html", "Topic 7 A/B Testing", " Topic 7 A/B Testing Coming soon… "],["customer-choice.html", "Topic 8 Customer Choice 8.1 R Packages and Datasets for Topic 8 8.2 Motivation 8.3 Understanding Multinomial Logistic 8.4 Conducting Multinomial Logistic 8.5 Standard MNL Regression Example 1 8.6 Standard MNL Regression Example 2 8.7 Alternative Specific MNL Example 8.8 Suggested Readings 8.9 R Code", " Topic 8 Customer Choice 8.1 R Packages and Datasets for Topic 8 library(ggplot2) # Advanced graphing capabilities library(dplyr) # Easier programming library(nnet) # Used for standard MNL library(mlogit) # Used for alternative specific MNL library(MKT4320BGSU) # Course package data(bfast) data(train.yog) data(test.yog) 8.2 Motivation Not all decisions customers make are binary Will a customer purchase Plan A, B, or C? Which product options were chosen? Linear regression even more incorrect, and Binary logistic regression won’t work… …But the binary model can be extended to multiple nominal outcomes to serve the same purposes Understand IV/DV relationships Make predictions 8.3 Understanding Multinomial Logistic Based on Random Utility Theory Given a choice set, consumers assign a level of attractiveness to each alternative \\(\\rightarrow\\) “Utility” Utility for alternative \\(j\\): \\(U_j=x_j\\beta+\\varepsilon_j\\) Assume consumer chooses option with highest utility Probability consumer chooses alternative \\(j\\): \\[\\begin{equation} P(Choice=j)=\\frac{e^{x_j\\beta}}{\\sum_{k=1}^J e^{x_k\\beta}} \\end{equation}\\] Suppose utilities for three choices: \\(U_1=15\\) \\(U_2=25\\) \\(U_3=30\\) Consumer picks choice 3 Now add 5 units to each utility: \\(U_1=20\\) \\(U_2=30\\) \\(U_3=35\\) Consumer still picks choice 3 Thus, absolute values of utilities do not matter Only relative differences So what? To make model work, set one choice as baseline and compare other choices to it 8.4 Conducting Multinomial Logistic Model Estimation Assessing Model Fit Goodness of Fit Measures Classification Matrix Interpreting Coefficients 8.4.1 Model Estimation Data dictate model type Standard multinomial logistic regression Info on customer, but not on choice attributes Alternative specific logistic regression Info on choice attributes (and maybe on customer) Best to use training data and holdout data Independent variables: Can be one or more Can be continuous or nominal 8.4.2 Assessing Model Fit 8.4.2.1 Goodness-of-Fit Measures Overall significance based on \\(-2LL\\) Lower (closer to \\(0\\)) \\(-2LL\\) indicates a better fit Compare \\(-2LL\\) of estimated model with “null” model in the Likelihood Ratio Test Used for both standard and alternative specific MNL McFadden’s Pseudo \\(R^2\\) Measure Values range from 0 to 1 like linear regression Interpreted in a similar manner Amount of variation in DV explained by IVs Used for standard MNL 8.4.2.2 Classification Matrix How does the model do in predicting outcomes? Generate predicted probability for choosing each alternative, \\(p(CHOICE)=j)\\), for each observation Predict \\(CHOICE=j\\) if \\(p(CHOICE=j)&gt;p(All\\ other\\ CHOICE)\\) Check predictions against actual outcomes Examine both training and holdout data Overall correctly classified Compare with Proportional Chance Criterion (\\(PCC\\)) \\(PCC\\) is the “average” probability of classification based on group sizes \\(PCC=\\sum_{k=1}^j p_k^2\\) where \\(p\\) is the proportion of sample in alternative \\(k\\) Overall correctly classified \\(&gt;PCC\\) considered good fit when examining holdout data 8.4.3 Interpreting Coefficients Relationship between DV and each IV \\(H_0: \\beta_k=0\\) vs. \\(H_a: \\beta_k\\ne0\\) Interpret significant relationships Interpretation depends on \\(RRR\\) or \\(Logit\\) estimation \\(RRR\\) is the “relative risk ratio” “Risk” is the probability of occurrence: \\(p(CHOICE=j)\\) “Relative risk ratio” is probability of occurrence compared to probability of some other occurrence happening:\\(\\frac{p(CHOICE=j)}{p(CHOICE=k)}\\) In MNL, compare probability of choice of interest with the probability of the baseline choice Direction of relationship: \\(Logit\\) estimation: \\(\\beta_k&gt;0\\) for positive, \\(\\beta_k&lt;0\\) for negative \\(RRR\\) estimation: \\(\\beta_k&gt;1\\) for positive, \\(\\beta_k&lt;1\\) for negative Magnitude of change: \\(Logit\\) estimation: coefficients are not particularly useful \\(RRR\\) estimation: Relative risk ratio for one unit change in \\(x_k\\) Average predicted probabilities 8.5 Standard MNL Regression Example 1 8.5.1 Data Breakfast food preference data \\(880\\) observations of individual responses DV: Type preferred, \\(bfast\\) (bar, oatmeal, cereal) IVs: Age category, \\(agecat\\) (&lt;31, 31-45, 46-60, 60+) Lifestyle, \\(lifestyle\\) (active, inactive) Predict preference Use training and holdout samples 8.5.2 Estimation Results Table 8.1: Estimation Results (R code) Loading required package: broom LR chi2 (4) = 265.2299; p &lt; 0.0001 McFadden&#39;s Pseudo R-square = 0.1844 y.level term estimate std.error statistic p.value Bar (Intercept) 1.7009 0.2946 1.8031 0.0714 Bar age 0.9727 0.0065 -4.2243 0.0000 Bar maritalUnmarried 1.7966 0.2088 2.8064 0.0050 Oatmeal (Intercept) 0.0130 0.4317 -10.0512 0.0000 Oatmeal age 1.0839 0.0077 10.4435 0.0000 Oatmeal maritalUnmarried 0.6842 0.2343 -1.6195 0.1053 8.5.3 Overall Model Fit Based on the likelihood ratio test with \\(p&lt;.0001\\), the overall model is significant McFadden’s Pseudo \\(R^2\\) of .20 means that the model explains about 20% of the variation in breakfast food preference Classification matrix for training sample Hit ratio \\(&gt;\\) PCC, so model fits well based on training sample Table 8.2: Classification Matrix for Training Sample (R code) 0.5559 = Hit Ratio 0.3413 = PCC .cl-140388ca{}.cl-13f82d5e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-13fc31ba{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-13fc31c4{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-13fc477c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-13fc4786{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-13fc4787{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-13fc4790{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-13fc4791{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-13fc479a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}LevelT.CerealT.BarT.OatmealTotalP.Cereal1319152274P.Bar42593104P.Oatmeal8224178284Total255174233662 Classification matrix for training sample Hit ratio \\(&gt;\\) PCC, so model fits well based on training sample Table 8.3: Classification Matrix for Holdout Sample (R code) 0.5550 = Hit Ratio 0.3416 = PCC .cl-141b63d2{}.cl-14128820{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-14161b20{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-14161b2a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-14162ee4{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-14162ee5{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-14162eee{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-14162eef{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-14162ef8{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-14162ef9{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}LevelT.CerealT.BarT.OatmealTotalP.Cereal48312099P.Bar1517133P.Oatmeal2195686Total845777218 8.5.4 Interpreting Coefficients 46-60 and Over 60 are negative (\\(RRR&lt;1\\)) and significant (\\(p&lt;.05\\)) for choosing Bar over Cereal compared to the Under 31 Relative probability of choosing Bar over Cereal is \\(61\\%\\) lower for 46-60 and \\(68\\%\\) lower for Over 60 compared to Under 31 All age categories are positive (\\(RRR&gt;1\\)) and significant (\\(p&lt;.05\\)) for choosing Oatmeal over Cereal compared to Under 31 Relative probability of choosing Oatmeal over Cereal is \\(299\\%\\), \\(1470\\%\\), and \\(4930\\%\\) higher for 31-45, 46-60, and Over 61, respectively, compared to Under 31 In general, younger people are more likely to choose Bar while older people are more likely to choose Oatmeal versus Cereal Inactive is negative (\\(RRR&lt;1\\)) and signficant (\\(p&lt;.05\\)) for choosing Bar over Cereal, but is not significant (\\(p=.191\\)) for choosing Oatmeal over Cereal Relative probability of choosing Bar over Cereal is \\(52\\%\\) lower for Inactive compared to Active 8.5.5 Average Predicted Probabilities Look at predicted probabilities of selecting each option for levels of each independent variable Rule of thumb says that if confidence intervals of two categories don’t overlap, they are significantly different Table 8.4: Average Predicted Probabilities for agecat (R code) Figure 8.1: agecat Average Predicted Probibility Plot (R code) 8.6 Standard MNL Regression Example 2 8.6.1 Data Same as before, but suppose we replace \\(agecat\\) with actual age for each respondent 8.6.2 Estimation Results Table 8.5: Estimation Results (R code) LR chi2 (4) = 271.9568; p &lt; 0.0001 McFadden&#39;s Pseudo R-square = 0.1891 y.level term estimate std.error statistic p.value Bar (Intercept) 2.8236 0.2953 3.5150 0.0004 Bar age 0.9740 0.0067 -3.9240 0.0001 Bar lifestyleInactive 0.4669 0.2065 -3.6880 0.0002 Oatmeal (Intercept) 0.0124 0.4370 -10.0365 0.0000 Oatmeal age 1.0788 0.0073 10.3624 0.0000 Oatmeal lifestyleInactive 1.3653 0.2146 1.4509 0.1468 8.6.3 Overall Model Fit Based on the likelihood ratio test with \\(p&lt;.0001\\), the overall model is significant McFadden’s Pseudo \\(R^2\\) of .189 means that the model explains about 19% of the variation in breakfast food preference Classification matrix for training sample Hit ratio \\(&gt;\\) PCC, so mone fits well based on training sample Table 8.6: Classification Matrix for Training Sample (R code) 0.5589 = Hit Ratio 0.3413 = PCC .cl-62a6cb5a{}.cl-6296289a{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-629c443c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-629c4446{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-629c5de6{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-629c5de7{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-629c5df0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-629c5df1{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-629c5df2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-629c5dfa{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}LevelT.CerealT.BarT.OatmealTotalP.Cereal1248449257P.Bar57697133P.Oatmeal7421177272Total255174233662 Classification matrix for training sample Hit ratio \\(&gt;\\) PCC, so mone fits well based on training sample Table 8.7: Classification Matrix for Holdout Sample (R code) 0.5413 = Hit Ratio 0.3416 = PCC .cl-62c9f8d2{}.cl-62bf4e82{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-62c372aa{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-62c372ab{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-62c38812{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62c3881c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62c3881d{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62c38826{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62c38827{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62c38830{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}LevelT.CerealT.BarT.OatmealTotalP.Cereal41271886P.Bar2221346P.Oatmeal2195686Total845777218 8.6.4 Interpreting Coefficients \\(age\\) is negative (\\(RRR&lt;1\\)) and significant (\\(p=.0001\\)) for choosing Bar over Cereal For a \\(1\\) unit increase in \\(age\\), we would expect about a \\(2.6\\%\\) decrease in the relative probability of choosing Bar over Cereal \\(age\\) is positive (\\(RRR&gt;1\\)) and significant (\\(p&lt;.0001\\)) for choosing Oatmeal over Cereal For a \\(1\\) unit increase in \\(age\\), we would expect about a \\(7.9\\%\\) increase in the relative probability of choosing Oatmeal over Cereal Inactive is negative (\\(RRR&lt;1\\)) and significant (\\(p=.0002\\)) for choosing Bar over Cereal, but is not significant (\\(p=.1468\\)) for choosing Oatmeal over Cereal Relative probability of choosing Bar over Cereal is \\(53\\%\\) lower for Inactive compared to Active 8.6.5 Average Predicted Probabilities Look at predicted probabilities of selecting each option for levels of each independent variable Rule of thumb says that if confidence intervals of two categories don’t overlap, they are significantly different Table 8.8: Average Predicted Probabilities for age (R code) .cl-62faf81a{}.cl-62efa096{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-62f3c522{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-62f3c536{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-62f3e110{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62f3e11a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62f3e124{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62f3e12e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62f3e138{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-62f3e139{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}agebfastp.problower.CIupper.CI31Cereal0.50500.56100.448931Bar0.41670.47420.361431Oatmeal0.07820.11230.053949Cereal0.47150.51870.424849Bar0.24220.28570.203549Oatmeal0.28630.33320.243567Cereal0.27040.32440.222467Bar0.08650.12150.060967Oatmeal0.64320.69820.5841 Figure 8.2: age Average Predicted Probibility Plot (R code) 8.7 Alternative Specific MNL Example 8.7.1 Data Yogurt choice \\(2412\\) yogurt choice occasions of individuals Observe choice among four brands DV: \\(choice\\) (Yes for brand chosen; No for brand not chosen) Alternative Specific IVs: Feature, \\(feat\\) (\\(1\\) if brand on feature, \\(0\\) otherwise) Price, \\(price\\) Case Specific IVs: Income (000s), \\(income\\) Predict brand chosen Use training and holdout samples Table 8.9: Yogurt Data (First 3 Observations) (R code) .cl-143edbdc{}.cl-1435e220{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-14394ef6{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-14394f00{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-143962ce{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-143962cf{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-143962d8{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-143962d9{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-143962e2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-143962e3{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}idfeatpricechoicebrandincome108.1NoDannon51.50106.1NoHiland51.50107.9YesWeight51.501010.8NoYoplait51.50209.8YesDannon64.75206.4NoHiland64.75207.5NoWeight64.752010.8NoYoplait64.75309.8YesDannon68.75306.1NoHiland68.75308.6NoWeight68.753010.8NoYoplait68.75 8.7.2 Estimation Results Using Odds Ratio formulation Choice specific varaibles assumed to have same effect on all choices Case specific variables assumed to have different effect on choices Table 8.10: Estimation Results (With Classificaition Matrices) (R code) --------- Model Fit --------- Log-Likelihood: -1618.4208 McFadden R^2: 0.2397 Likelihood ratio test: chisq = 1020.5649 (p.value &lt; .0001) --------------------- OR Estimation Results --------------------- term estimate std.error statistic p.value (Intercept):Hiland 2.1355 0.5677 1.3365 0.1814 (Intercept):Weight 0.9740 0.2079 -0.1269 0.8990 (Intercept):Yoplait 0.0185 0.2680 -14.8845 0.0000 feat 1.5267 0.1491 2.8371 0.0046 price 0.6425 0.0296 -14.9691 0.0000 income:Hiland 0.8975 0.0149 -7.2464 0.0000 income:Weight 0.9886 0.0038 -3.0436 0.0023 income:Yoplait 1.0756 0.0040 18.1030 0.0000 --------------------------------------- Classification Matrix for Training Data --------------------------------------- 0.6207 = Hit Ratio 0.3299 = PCC T.Dannon T.Hiland T.Weight T.Yoplait Total P.Dannon 577 39 324 97 1037 P.Hiland 1 12 0 2 15 P.Weight 18 2 38 18 76 P.Yoplait 132 1 53 497 683 Total 728 54 415 614 1811 -------------------------------------- Classification Matrix for Holdout Data -------------------------------------- 0.6073 = Hit Ratio 0.3309 = PCC T.Dannon T.Hiland T.Weight T.Yoplait Total P.Dannon 199 14 104 38 355 P.Hiland 2 2 1 1 6 P.Weight 8 1 12 13 34 P.Yoplait 33 0 21 152 206 Total 242 17 138 204 601 8.7.3 Overall Model Fit Based on the significant test with \\(p&lt;.0001\\), the overall model is significant McFadden’s \\(R^2\\) of \\(.2397\\) suggests that about \\(24\\%\\) of the variance in choice is explained by the model Classification matrix for the Training Sample \\(Hit\\text{ }Ratio=\\frac{577+12+38+497}{1811}=62.07\\%\\) \\(PCC=\\frac{728^2+54^2+415^2+614^2}{1811^2}=32.99\\%\\) \\(Hit\\text{ }Ratio&gt;PCC\\), so model fits well based on training sample Classification matrix for the Holdout Sample \\(Hit\\text{ }Ratio=\\frac{199+2+12+152}{601}=60.73\\%\\) \\(PCC=\\frac{242^2+17^2+138^2+204^2}{601^2}=33.09\\%\\) \\(Hit\\text{ }Ratio&gt;PCC\\), so model fits well based on holdout sample 8.7.4 Interpreting Coefficients 8.7.4.1 Choice specific results \\(feat\\) is significant (\\(p=.0046\\)) in choice of brand. The odds ratio of \\(feat\\) (\\(1.5267&gt;1\\)) means the effect of \\(feat\\) on choice of \\(brand_j\\) is positive. Specifically, when \\(brand_j\\) is on feature, we would expect the odds of choosing \\(brand_j\\) to increase by about \\(53\\%\\). \\(price\\) is significant (\\(p&lt;.0001\\)) in choice of brand. The odds ratio of \\(price\\) (\\(.6425&lt;1\\)) means the effect of \\(price\\) on choice of \\(brand_j\\) is negative. Specifically, for a \\(\\$1\\) increase in the price of \\(brand_j\\), we would expect the odds of choosing \\(brand_j\\) to decrease by about \\(36\\%\\). To further interpret significant variables, we can examine the marginal effects of an increase in a continuous variable at its mean value How much being on \\(feature\\) increases probability of a brand being selected In addition, we can see how one brand being on \\(feature\\) decreases the probability of the other brands being selected How much an increase in \\(price\\) decreases probabiltiy of a brand being selected (at that brand’s mean price) In addition, we can see how an increase in \\(price\\) of one brand increases the probability of the other brands being selected Table 8.11: Marginal Effects (R code) -------------------------------- Predicted Probabilities at Means -------------------------------- Dannon Hiland Weight Yoplait 0.4832 0.0048 0.2571 0.2549 ------------------------- Marginal effects for feat ------------------------- Dannon Hiland Weight Yoplait Dannon 0.10565 -0.00098 -0.05256 -0.05212 Hiland -0.00098 0.00201 -0.00052 -0.00052 Weight -0.05256 -0.00052 0.08080 -0.02773 Yoplait -0.05212 -0.00052 -0.02773 0.08036 -------------------------- Marginal effects for price -------------------------- Dannon Hiland Weight Yoplait Dannon -0.11049 0.00102 0.05496 0.05450 Hiland 0.00102 -0.00211 0.00054 0.00054 Weight 0.05496 0.00054 -0.08450 0.02899 Yoplait 0.05450 0.00054 0.02899 -0.08404 --------------------------- Marginal effects for income --------------------------- Dannon Hiland Weight Yoplait -0.00731 -0.00059 -0.00684 0.01473 Marginal effects for \\(feat\\) When Dannon is on \\(feature\\), we expect a 10.5% increase in the probability of selecting it, and decreases of 5.3% and 5.2% for selecting Weight and Yoplait, respectively When Weight is on \\(feature\\), we expect a 18.1% increase in the probability of selecting it, and decreases of 5.3% and 2.8% for selecting Dannon and Yoplait, respectively When Yoplait is on \\(feature\\), we expect a 8.0% increase in the probability of selecting it, and decreases of 5.2% and 2.8% for selecting Dannon and Weight, respectively Marginal effects for \\(price\\) At mean prices, for a $1 increases, the probability of selecting: Dannon decreases by 11.0%, and the probability of selecting Weight or Yoplait increases by 5.5% each Weight decreases by 8.5%, and the probability of selecting Dannon or Yoplait increases by 5.5% and 2.9%, respectively Yoplait decreases by 2.9%, and the probability of selecting Dannon or Weight increases by 5.5% and 2.9%, respectively 8.7.4.2 Case Specific Results Case specific IVs can have different effects by brand Margin plots can be used to visualize case specific IVs Figure 8.3: Margin Plot for Income (R code) \\(income\\) has a significant effect on the choice of all three brands relative to Dannon Based on \\(OR=.8975\\), for a \\(\\$1000\\) increase in \\(income\\), we expect a \\(10.2\\%\\) decrease in the odds of choosing Hiland over Dannon Based on \\(OR=.9886\\), for a \\(\\$1000\\) increase in \\(income\\), we expect a \\(1.1\\%\\) decrease in the odds of choosing Weight over Dannon Based on \\(OR=1.0756\\), for a \\(\\$1000\\) increase in \\(income\\), we expect a \\(7.6\\%\\) increase in the odds of choosing Yoplait over Dannon Marginal effects for \\(income\\) For a \\(\\$1000\\) increase in \\(income\\) at the mean, we expect the probability of selecting: Dannon to decrease by \\(0.73\\%\\) Hiland to decrease by \\(0.07\\%\\) Weight to decrease by \\(0.68\\%\\) Yoplait to increase by \\(1.47\\%\\) Based on the margin plot, the probability of selecting: Dannon increases with \\(income\\) up to about $43K, but then starts to decrease Hiland decreases with \\(income\\) Weight increases with \\(income\\) up to about $35K, but then starts to decrease Yoplait increases with \\(income\\) 8.8 Suggested Readings Marketing Analytics: Data Driven Techniques with Excel (2014). Winston, Wayne L. BGSU Library Link:http://ezproxy.bgsu.edu/login?url=https://ebookcentral.proquest.com/lib/bowlinggreen-ebooks/detail.action?docID=1629159 Chapter 18: Discrete Choice Analysis Applied Logistic Regression (2013). Hosmer, David W.; Lemeshow, Stanley; Sturdivant, Rodney X. BGSU Library Link:https://ezproxy.bgsu.edu/login?url=http://rave.ohiolink.edu/ebooks/ebc2/9781118548387 Chapter 8.1: The Multinomial Logistic Regression Model Logistic Regression: A Self-Learning Text (2010). Kleinbaum, David G.; Klein, Mitchel. BGSU Library Link:https://ezproxy.bgsu.edu/login?url=https://dx.doi.org/10.1007/978-1-4419-1742-3 Chapter 12: Polytomous Logistic Regression 8.9 R Code Figure 8.1 results.agecat$plot # Object &#39;results.agecat&#39; produced with Table 7.4 Figure ?? results.lifestyle$plot # Object &#39;results.lifestyle&#39; produced with Table 7.5 Figure 8.2 results.age$plot # Object &#39;results.age&#39; produced with Table 7.9 Figure 8.3 asmnl_mp(asmod,&quot;income&quot;, &quot;C&quot;) Table 8.1 # Use &#39;caret&#39; package to create training and test/holdout samples # This will create two separate dataframes: train and test library(caret) set.seed(4320) inTrain &lt;- createDataPartition(y=bfast$bfast, p=.75, list=FALSE) train &lt;- bfast[inTrain,] test &lt;- bfast[-inTrain,] library(nnet) # Library required for multinomial logistic regression st.mod &lt;- multinom(bfast ~ agecat + lifestyle, # Formula data=train, trace=FALSE) # Data # Use &#39;stmnl&#39; user defined function to get clean results stmnl(st.mod) Table ?? # Use &#39;stmnl_cm&#39; user-defined function for classification matrix stmnl_cm(st.mod, train) Table ?? # Use &#39;stmnl_cm&#39; user-defined function for classification matrix stmnl_cm(st.mod, test) Table 8.4 # Use &#39;stmnl_pp&#39; user defined function results.agecat &lt;- stmnl_pp(st.mod, # Model &quot;agecat&quot;, # Focal Variable &quot;Age Category&quot;) # Label for plot results.agecat$table Table ?? results.lifestyle &lt;- stmnl_pp(st.mod, # Model &quot;lifestyle&quot;, # Focal Variable &quot;Lifestyle&quot;) # Label for plot results.lifestyle$table Table 8.5 st.mod2 &lt;- multinom(bfast ~ age + lifestyle, # Formula data=train, trace=FALSE) # Data stmnl(st.mod2) Table ?? # Use &#39;stmnl_cm&#39; user-defined function for classification matrix stmnl_cm(st.mod2, train) Table ?? # Use &#39;stmnl_cm&#39; user-defined function for classification matrix stmnl_cm(st.mod2, test) Table 8.8 # Use &#39;stmnl_pp&#39; user defined function results.age &lt;- stmnl_pp(st.mod2, # Model &quot;age&quot;, # Focal Variable &quot;Age&quot;) # Label for plot results.age$table Table 8.9 # Data already split into training and test samples; # Training data: train.yog # Testing data: test.yog head(train.yog,12) Table 8.10 # Use &#39;asmnl_est&#39; user defined function to get: # Model Fit # OR Coefficient Estimates # Classification matrix results for testing and holdout # A stored model object to use later # To use function, must store formula in object: # IVs before the &#39;|&#39; are alternative specific # IVs after the &#39;|&#39; are case specific asform &lt;- choice ~ feat + price | income asmod &lt;- asmnl_est(formula=asform, # Formula to estimate data=train.yog, # Training sample data id=&quot;id&quot;, # Case ID variable alt=&quot;brand&quot;, # Alternative/brand variable choice=&quot;choice&quot;, # Choice variable (i.e., if alt was chosen) testdata=test.yog) # Holdout sample data Table 8.11 # Use &#39;asmnl_me&#39; user defined function to get marginal effects # for continuous variables asmnl_me(asmod) # Model results saved from above "],["marketing-mix-decisions.html", "Topic 9 Marketing Mix Decisions 9.1 R Packages and Datasets for Topic 9 9.2 Overview 9.3 Elasticity 9.4 Modeling Advertising Response 9.5 Modeling Price Response 9.6 Modeling Advertising Response Example 9.7 Modeling Price Response Example 9.8 Modeling Price and Advertising Response Example 9.9 Suggested Readings 9.10 R Code", " Topic 9 Marketing Mix Decisions 9.1 R Packages and Datasets for Topic 9 library(ggplot2) # Advanced graphing capabilities library(dplyr) # Easier programming library(cowplot) library(MKT4320BGSU) load(&quot;Topic09/adsales.rdata&quot;) load(&quot;Topic09/cereal.rdata&quot;) load(&quot;Topic09/cheese.rdata&quot;) 9.2 Overview 9.2.1 The Marketing Mix Unique blend of product, place, promotion, and pricing strategies designed to produce mutually satisfying exchanges with a target market. 9.2.2 Market Response Models Marketing Inputs \\(\\rightarrow\\) The Market \\(\\rightarrow\\) Marketing Outputs Marketing inputs include: Price Ad Spending Promotion Spending Marketing outputs include: Sales Market share Profit 9.2.3 Marketing Decision Models 9.2.4 Advantages of Response Models Allows marketers to identify how marketing efforts affect outcomes Marketers can answer resources allocation questions Marketers can determine relative impacts to allocate resources optimally Marketers can capture the effects of competitive marketing efforts 9.3 Elasticity 9.3.1 Elasticity of Demand Consumers’ responsiveness or sensitivity to marketing changes. \\(E=\\frac{\\text{% Change in Demand}}{\\text{% Change in Marketing}}\\) Elastic Demand \\(E&gt;1\\) Consumers sensitive to marketing changes Inelastic Demand \\(E&lt;1\\) Consumers insensitive to marketing changes 9.3.2 Inelastic Demand Marketing changes will not significantly affect demand for the product \\(PED=\\frac{\\text{% Change in Demand}}{\\text{% Change in Price}}=\\frac{8.2\\%}{10\\%}=.82&lt;1\\) \\(AED=\\frac{\\text{% Change in Demand}}{\\text{% Change in Ad Spending}}=\\frac{9.3\\%}{10\\%}=.93&lt;1\\) 9.3.3 Elastic Demand Marketing changes will significantly affect demand for the product \\(PED=\\frac{\\text{% Change in Demand}}{\\text{% Change in Price}}=\\frac{15\\%}{10\\%}=1.5&gt;1\\) \\(AED=\\frac{\\text{% Change in Demand}}{\\text{% Change in Ad Spending}}=\\frac{12.9\\%}{10\\%}=1.29&gt;1\\) 9.4 Modeling Advertising Response Problem 1: an additive model (\\(sales=\\alpha+\\beta\\times adv\\)) isn’t appropriate to represent a concave relationship Problem 2: using linear regression on a concave form (\\(sales=e^\\alpha adv^\\beta\\)) won’t work Solution: transform the equation using the natural log (\\(ln(sales)=\\alpha+\\beta\\times ln(adv)\\)) Result: allows the use of linear regression to model the concave relationship \\(ln(sales)=\\alpha+\\beta\\times ln(adv)\\) \\(\\alpha\\) is the \\(ln(sales)\\) when \\(adv\\approx0\\) \\(e^\\alpha\\) is the \\(sales\\) when \\(adv\\approx0\\) \\(\\beta\\) is the effect on sales for a \\(1\\%\\) increase in \\(adv\\) In other words, \\(\\beta\\) is the advertising elasticity of demand \\(\\beta\\) should be positive To calculate the effect on \\(sales\\) for an \\(x\\%\\) increase, calculate \\(1.x^\\beta -1\\) 9.5 Modeling Price Response Problem 1: an additive model (\\(sales=\\alpha+\\beta\\times price\\)) isn’t appropriate to represent a concave relationship Problem 2: using linear regression on a concave form (\\(sales=e^\\alpha price^\\beta\\)) won’t work Solution: transform the equation using the natural log (\\(ln(sales)=\\alpha+\\beta\\times ln(price)\\)) Result: allows the use of linear regression to model the concave relationship \\(ln(sales)=\\alpha+\\beta\\times ln(price)\\) \\(\\alpha\\) is the \\(ln(sales)\\) when \\(price\\approx0\\) \\(e^\\alpha\\) is the \\(sales\\) when \\(price\\approx0\\) \\(\\beta\\) is the effect on sales for a \\(1\\%\\) increase in \\(price\\) In other words, \\(\\beta\\) is the price elasticity of demand \\(\\beta\\) should be negative To calculate the effect on \\(sales\\) for an \\(x\\%\\) increase, calculate \\(1.x^\\beta -1\\) 9.6 Modeling Advertising Response Example 9.6.1 Data Advertising and Sales data for 200 firms DV: Sales (in millions), \\(sales\\) IVs: TV Advertising (in 000s), \\(tv\\) Radio Advertising (in 000s), \\(radio\\) Paper Advertising (in 000s), \\(paper\\) Model: \\(sales=e^\\alpha tv^{\\beta_1} radio^{\\beta_2} paper^{\\beta_3}\\) 9.6.2 Scatterplots Fit lines show the sales and advertising relationships are not linear Figure 9.1: Advertising-Sales Scatterplots (R code) 9.6.3 Estimation Transform data to use the multiplicative model \\(sales=e^\\alpha tv^{\\beta_1} radio^{\\beta_2} paper^{\\beta_3}\\)\\(\\rightarrow\\) Take natural log of both sides \\(ln(sales)=ln(e^\\alpha tv^{\\beta_1} radio^{\\beta_2} paper^{\\beta_3})\\)\\(ln(sales)=\\alpha+\\beta_1ln(tv)+\\beta_2ln(radio)+\\beta_3ln(paper)\\) NOTE: \\(ln(x^y)=y\\times ln(x)\\) \\(ln(x\\times y)=ln(x)+ln(y)\\) \\(ln(e)=1\\) 9.6.4 Results Table 9.1: Advertising Response Results (R code) F(3,196) 903.5021 R² 0.9326 Adj. R² 0.9315 Est. S.E. t val. p (Intercept) 0.4013 0.0458 8.7591 0.0000 log(tv) 0.3498 0.0077 45.6892 0.0000 log(radio) 0.1718 0.0076 22.7162 0.0000 log(paper) 0.0159 0.0080 1.9824 0.0488 Standard errors: OLS Model is significant (\\(p&lt;.0001\\)) and explains about \\(93\\%\\) of the variance All three types of advertising are significant and in the expected direction Advertising elasticities: Television: \\(.350\\) (inelastic) Radio: \\(.172\\) (inelastic) Paper: \\(.016\\) (inelastic) 9.7 Modeling Price Response Example 9.7.1 Data Price and Sales data for one cereal over 380 weeks DV: Units sold (in 000s), \\(sold\\) IV: Price (in dollars), \\(price\\) Model: \\(sold=e^\\alpha price^\\beta\\) 9.7.2 Scatterplot Fit line show the units sold and price relationship is not linear Figure 9.2: Price-Units Sold Scatterplot (R code) 9.7.3 Estimation Transform data to use the multiplicative model \\(sold=e^\\alpha price^\\beta\\)\\(\\rightarrow\\) Take natural log of both sides \\(ln(sold)=ln(e^\\alpha price^\\beta)\\)\\(ln(sold)=\\alpha+\\beta ln(price)\\) NOTE: \\(ln(x^y)=y\\times ln(x)\\) \\(ln(x\\times y)=ln(x)+ln(y)\\) \\(ln(e)=1\\) 9.7.4 Results Table 9.2: Price Response Results (R code) F(1,378) 344.7064 R² 0.4770 Adj. R² 0.4756 Est. S.E. t val. p (Intercept) 8.3844 0.0600 139.7515 0.0000 log(price) -1.6960 0.0913 -18.5663 0.0000 Standard errors: OLS Model is significant (\\(p&lt;.0001\\)) and explains about \\(48\\%\\) of the variance Price is significant and in the expected direction Price elasticity: \\(-1.696\\) (elastic) 9.8 Modeling Price and Advertising Response Example 9.8.1 Data Price, Advertising, and Sales data for a cheese product across 10 cities over about 5 years DV: Sales volume, \\(vol\\) IVs: Price (in dollars), \\(price\\) Advertising spending index, \\(adv\\) Model: \\(vol=e^\\alpha price^{\\beta_1} adv^{\\beta_2}\\) 9.8.2 Scatterplot Fit lines show the volume relationships with price and advertising are not linear Figure 9.3: Price and Advertising Scatterplots with Sales Volume (R code) 9.8.3 Estimation Transform data to use the multiplicative model \\(vol=e^\\alpha price^{\\beta_1} adv^{\\beta_2}\\)$ Take natural log of both sides \\(ln(vol)=ln(e^\\alpha price^{\\beta_1} adv^{\\beta_2})\\)\\(ln(sold)=\\alpha+\\beta_1 ln(price)+\\beta_2 ln(adv)\\) NOTE: \\(ln(x^y)=y\\times ln(x)\\) \\(ln(x\\times y)=ln(x)+ln(y)\\) \\(ln(e)=1\\) 9.8.4 Results Table 9.3: Price and Advertising Response Results (R code) F(2,632) 178.0760 R² 0.3604 Adj. R² 0.3584 Est. S.E. t val. p (Intercept) 9.0455 0.1356 66.7247 0.0000 log(price) -1.3361 0.1157 -11.5472 0.0000 log(adv) 0.2103 0.0169 12.4244 0.0000 Standard errors: OLS Model is significant (\\(p&lt;.0001\\)) and explains about \\(36\\%\\) of the variance Both advertising and price are significant and in the expected direction Elasticities: Price: \\(-1.336\\) (elastic) Advertising: \\(.210\\) (inelastic) 9.9 Suggested Readings Principles of Marketing Engineering and Analytics, 3rd Edition (2017). Lilien, Gary L., Rangaswamy, Arvind, and De Bruyn, Arnaud. Course reserves Chapter 7: The Marketing Mix 9.10 R Code Figure 9.1 load(&quot;Topic09/adsales.rdata&quot;) # Load data library(effects) # Used to create a log-log fit line library(cowplot) # Used to create a grid for the three plots # Run simple log-log models for each IV logtv &lt;- lm(log(sales)~log(tv), data=adsales) logradio &lt;- lm(log(sales)~log(radio), data=adsales) logpaper &lt;- lm(log(sales)~log(paper), data=adsales) # Use results from log-log models to get a &quot;fit line&quot; for the scatter plots tvpred &lt;- data.frame(predictorEffect(&quot;tv&quot;, logtv)) radiopred &lt;- data.frame(predictorEffect(&quot;radio&quot;, logradio)) paperpred &lt;- data.frame(predictorEffect(&quot;paper&quot;, logpaper)) # Create first plot p1 &lt;- adsales %&gt;% ggplot(aes(x=tv, y=sales)) + # Call to ggplot geom_point() + #Plot points geom_line(aes(x=tv, y=exp(fit)), # Plot &#39;log-log&#39; fit line data=tvpred, color=&quot;orange&quot;, size=1) + geom_smooth(method=&quot;lm&quot;, se=FALSE, # Plot linear fit line size=1, color=&quot;red&quot;, linetype=&quot;dashed&quot;) + labs(x=&quot;Television Advertising&quot;, y=&quot;Sales&quot;) # Repeat for second and third plots p2 &lt;- adsales %&gt;% ggplot(aes(x=radio, y=sales)) + geom_point() + geom_line(aes(x=radio, y=exp(fit)), data=radiopred, color=&quot;orange&quot;, size=1) + geom_smooth(method=&quot;lm&quot;, se=FALSE, size=1, color=&quot;red&quot;, linetype=&quot;dashed&quot;) + labs(x=&quot;Radio Advertising&quot;, y=&quot;Sales&quot;) p3 &lt;- adsales %&gt;% ggplot(aes(x=paper, y=sales)) + geom_point() + geom_line(aes(x=paper, y=exp(fit)), data=paperpred, color=&quot;orange&quot;, size=1) + geom_smooth(method=&quot;lm&quot;, se=FALSE, size=1, color=&quot;red&quot;, linetype=&quot;dashed&quot;) + labs(x=&quot;Newspaper Advertising&quot;, y=&quot;Sales&quot;) # Plot three scatter plots together using cowplot package plot_grid(p1,p2,p3, nrow=2) Figure 9.2 load(&quot;Topic09/cereal.rdata&quot;) # Load data library(effects) # Used to create a log-log fit line logprice &lt;- lm(log(sold)~log(price), data=cereal) # Run simple log-log model # Use results from log-log model to get a &quot;fit line&quot; for the scatter plot pricepred &lt;- data.frame(predictorEffect(&quot;price&quot;, logprice)) # Create plot cereal %&gt;% ggplot(aes(x=price, y=sold)) + geom_point() + geom_line(aes(x=price, y=exp(fit)), data=pricepred, color=&quot;orange&quot;, size=1) + geom_segment(aes(x=1, y=3413, xend=(5325/1912), yend=0), # Manually produce size=1, color=&quot;red&quot;, linetype=&quot;dashed&quot;) + # linear fit line labs(x=&quot;Price ($)&quot;, y=&quot;Units Sold (000s)&quot;) + scale_y_continuous(limits=c(0,11500)) Figure 9.3 load(&quot;Topic09/cheese.rdata&quot;) # Load Data library(effects) library(cowplot) logpr2 &lt;- lm(log(vol)~log(price), data=cheese) logad2 &lt;- lm(log(vol)~log(adv), data=cheese) pr2pred &lt;- data.frame(predictorEffect(&quot;price&quot;, logpr2)) ad2pred &lt;- data.frame(predictorEffect(&quot;adv&quot;, logad2)) p1 &lt;- cheese %&gt;% ggplot(aes(x=price, y=vol)) + geom_point() + geom_line(aes(x=price, y=exp(fit)), data=pr2pred, color=&quot;orange&quot;, size=1) + geom_smooth(method=&quot;lm&quot;, se=FALSE, size=1, color=&quot;red&quot;, linetype=&quot;dashed&quot;) + labs(x=&quot;Price&quot;, y=&quot;Sales Volume&quot;) + scale_y_continuous(limits=c(0,15000)) p2 &lt;- cheese %&gt;% ggplot(aes(x=adv, y=vol)) + geom_point() + geom_line(aes(x=adv, y=exp(fit)), data=ad2pred, color=&quot;orange&quot;, size=1) + geom_smooth(method=&quot;lm&quot;, se=FALSE, size=1, color=&quot;red&quot;, linetype=&quot;dashed&quot;) + labs(x=&quot;Advertising&quot;, y=&quot;Sales Volume&quot;) + scale_y_continuous(limits=c(0,15000)) plot_grid(p1,p2, ncol=2) Table ?? library(jtools) adresp &lt;- lm(log(sales)~log(tv)+log(radio)+log(paper), data=adsales) summ(adresp, digits=4, model.info=FALSE) Table ?? library(jtools) priceresp &lt;- lm(log(sold)~log(price), data=cereal) summ(priceresp, digits=4, model.info=FALSE) Table ?? library(jtools) pradresp &lt;- lm(log(vol)~log(price)+log(adv), data=cheese) summ(pradresp, digits=4, model.info=FALSE) "],["forecasting-i.html", "Topic 10 Forecasting I 10.1 R Packages and Datasets for Topic 10 10.2 Overview 10.3 Forecasting Methods 10.4 Forecasting - Time Series 10.5 Naïve Methods 10.6 Smoothing Techniques 10.7 Regression-based 10.8 Forecast Accuracy 10.9 Forecasting Example 1 10.10 Forecasting Example 2 10.11 Suggested Readings 10.12 R Code", " Topic 10 Forecasting I 10.1 R Packages and Datasets for Topic 10 library(ggplot2) # Advanced graphing capabilities library(dplyr) # Easier programming library(fpp3) library(slider) library(cowplot) library(MKT4320BGSU) load(&quot;Topic10/monthlysales.rdata&quot;) 10.2 Overview 10.2.1 The Concept of Forecasting Firms must make forecasts, but……Forecasing is difficult and challenging Marketing strategy focuses on the long term……Making the task more difficult and challenging Forecasting is not about “getting the future right” Forecasting is about helping manage uncertainty Question to Ask What are we forecasting? Why do we need the forecast? 10.3 Forecasting Methods Judgmental Sales force estimates Jury of experts Structured processes Market/Product Analysis Surveys Market tests ATAR Time Series Naïve methods Smoothing techniques Box-Jenkins methods Regression/Econometric Linear Non-Linear 10.4 Forecasting - Time Series Decomposition: splitting data into components Trend Seasonality Reaminder 10.5 Naïve Methods 10.5.1 Naïve Forecast future values to be same as the last observation\\(\\hat{y_t}=y_{t-1}\\) Easy to use, but… What about… Seasonality? Trend? 10.5.2 Seasonal Naïve Forecast future values to be same as the last observation of the same period\\(\\hat{y_t}=y_{t-m}\\) Easy to use, but… What about… Trend? 10.5.3 Naïve with Drift Forecast future values to be same as the last observation plus the average change per period\\(\\hat{y_t}=y_{t-1}+(\\frac{y_{t-1}-y_1}{T-1})\\) Easy to use, but… What about… Seasonality? 10.5.4 Seasonal Naïve with Drift Forecast future values to be same as the last observation of the same time period plus the average change per period 10.5.5 Summary Naïve models may work, but… Usually serve as a benchmark rather than preferred method 10.6 Smoothing Techniques 10.6.1 Moving Average Classic method to smooth randomness \\(\\begin{aligned} \\hat{y_t}=\\frac{1}{m}\\sum_{j=1}^{m}y_{t-j} \\end{aligned}\\) Short-range forecast Best for one-step ahead forecast Doesn’t account for trend or seasonality 10.6.2 Exponential Smoothing Weighted moving average approach with geometrically declining weights Uses smoothing parameter A larger parameter puts more weight on recent periods Parameter can be manually input… Better to optimize parameter by minimizing the error Doesn’t really account for trend and seasonality 10.6.3 Holt-Winters Smoothing Extends exponential smoothing Adds additional smoothing parameters for: Trend Seasonality Better if forecasting multiple periods Two methods: Additive and Multiplicative Additive Use when seasonal effects are roughly constant throughout Multiplicative Use when seasonal effects respond proportionally to the trend 10.7 Regression-based Can be used to model simple trend and seasonality Linear model: \\(y_t=\\alpha+\\beta x_t\\) To model trend, substitute \\(t\\) for \\(x_t\\)\\(y_t=\\alpha+\\beta t\\) To model seasonality, substitute \\(s\\) dummies for \\(x_t\\):\\(y_t=\\alpha+\\gamma_1 S_1+\\dots+\\gamma_{s-1}S_{s-1}\\) Combine to model trend and seasonality:\\(y_t=\\alpha+\\beta t+\\gamma_1 S_1+\\dots+\\gamma_{s-1}S_{s-1}\\) 10.7.1 Trend only \\(y_t=\\alpha+\\beta t\\) Forecast is simply a straight line 10.7.2 Trend and Seasonality \\(y_t=\\alpha+\\beta t+\\gamma_1 S_1+\\dots+\\gamma_{s-1}S_{s-1}\\) Forecast follows pattern 10.8 Forecast Accuracy Important to evaluate forecast accuracy Predicted values of test data time periods 10.8.1 Measures Calculate error in each test data time period:\\(e_{T+h}=y_{T+h}-\\hat{y}_{T+h}\\) Use Mean Absolute Error, Root Mean Squared Error, and Mean Absolute Percentage Error to compare models Mean Absolute Error (MAE): \\(\\begin{aligned} \\frac{\\sum_{t=T+1}^{T+h}|e_t|}{n} \\end{aligned}\\) Root Mean Squared Error (RMSE): \\(\\begin{aligned} \\sqrt\\frac{\\sum_{t=T+1}^{T+h}e_t^2}{n} \\end{aligned}\\) Mean Absolute Percentage Error (MAPE): \\(\\begin{aligned} \\frac{\\sum_{t=T+1}^{T+h}|e_t/y_t|}{n}\\times 100 \\end{aligned}\\) Lower values indicate better forecast accuracy 10.8.2 Visual Visually examine forecast errors for test data Forecast errors closer to 0 indicate better forecast 10.9 Forecasting Example 1 10.9.1 Data Quarterly sales data from 1990 through 2012 Variables: Time period (\\(t\\)) Sales from 1990 through 2010 (\\(sales\\)) Goal: Forecast quarterly sales for the next two years 10.9.2 Examine Data Figure 10.1: Time Series Plot (R code) Trend? Yes Seasonlity? No Potential Models? Naïve with Drift Moving Average Holt-Winters Regression Trend 10.9.3 Naïve Methods Run all methods Plot dynamic forecasts Naïve underestimates Naïve with Drift over estimates Seasonal methods not appropriate Figure 10.2: Naive Methods Plot (R code) Examine accuracy Surprisingly, Naïve performs slightly better, than Naïve with Drift Lowest RMSE, MAE, and MAPE Table 10.1: Naive Methods Accuracy (R code) .cl-2392710c{}.cl-23888e08{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-238c435e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-238c435f{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-238c5bd2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-238c5bdc{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-238c5bdd{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-238c5bde{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-238c5be6{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-238c5be7{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEMAEMAPENaive47.56143.9330.570Naive.Drift52.60644.5150.577Seas.Naive63.07561.1380.793Seas.Naive.Drift62.95457.3810.744 Examine forecast residuals Naïve and Naïve with Drift appear similar Figure 10.3: Naive Methods Forecast Residuals (R code) 10.9.4 Smoothing Methods Run all methods Plot dynamic forecasts Moving Average underestimates Both Holt-Winters overestimate, but not by much Exponential Smoothing a poor fit Figure 10.4: Smoothing Methods Plot (R code) Examine accuracy Holt-Winters Additive is clearly the best model Much lower RMSE, MAE, and MAPE Table 10.2: Smoothing Methods Accuracy (R code) .cl-24a6e06e{}.cl-249cdd30{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-24a0c1d4{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-24a0c1de{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-24a0d782{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-24a0d78c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-24a0d796{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-24a0d797{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-24a0d798{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-24a0d7a0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEMAEMAPEExp.Smooth47.56243.9330.570H-W.Add29.72124.6600.320H-W.Mult49.57842.3390.549Mov.Ave67.93467.7740.880 Examine forecast residuals Holt-Winters Additive consistently closer to 0 Figure 10.5: Smoothing Methods Forecast Residuals (R code) 10.9.5 Regression Methods Run all methods Plot dynamic forecasts Both methods nearly identical… But we know there isn’t seasonality Figure 10.6: Regression Methods Plot (R code) Examine accuracy Both methods nearly identical… But we know there isn’t seasonality Table 10.3: Regression Methods Accuracy (R code) .cl-2562d792{}.cl-2558e43a{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-255ccff0{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-255ccffa{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-255ce530{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-255ce531{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-255ce53a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-255ce544{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-255ce545{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-255ce546{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEMAEMAPELin.Reg.Seas.Trend84.11879.0901.026Lin.Reg.Trend84.20478.9721.024 Examine forecast residuals Both methods nearly identical… But we know there isn’t seasonality Figure 10.7: Regression Methods Forecast Residuals (R code) 10.9.6 Compare Best Models Examine RMSE, MAE, MAPE, and Forecast Error Plot Figure 10.8: Forecase Residuals Comparison (R code) Table 10.4: Accuracy Comparison (R code) .cl-25e63092{}.cl-25dc0d6a{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-25e011d0{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-25e011e4{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-25e028f0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-25e028fa{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-25e028fb{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-25e02904{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-25e02905{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-25e02906{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEMAEMAPENaive47.56143.9330.570H-W.Add29.72124.6600.320Lin.Reg.Trend84.20478.9721.024 None of the models do a great job, especially farther out Holt-Winters and Naïve with Drift seem to do the best 10.10 Forecasting Example 2 10.10.1 Data Monthly Sales from 2005 through 2016 Variables: Time period (\\(t\\)) Sales per period (\\(sales\\)) Goal: Forecast quarterly sales for the next 12 months 10.10.2 Examine Data Figure 10.9: Time Series Plot (R code) Trend? Yes Seasonlity? Yes Potential Models? Seasonal Naïve with Drift Holt-Winters Regression Seasonal Trend 10.10.3 Naïve Methods Run all methods Plot dynamic forecasts Naïve and Naïve with Drift mostly under estimate due to seasonality and drift Seasonal Naïve and Seasonal Naïve with Drift seem pretty good Figure 10.10: Naive Methods Plot (R code) Examine accuracy Seasonal Naïve with Drift is by far the best model Lowest RMSE, MAE, and MAPE Table 10.5: Naive Methods Accuracy (R code) .cl-26ad1d56{}.cl-26a2f826{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-26a6db58{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-26a6db62{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-26a6f016{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-26a6f017{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-26a6f020{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-26a6f021{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-26a6f02a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-26a6f02b{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEMAEMAPENaive323.496263.9170.945Naive.Drift253.103185.8680.665Seas.Naive133.710127.4170.459Seas.Naive.Drift45.76941.4170.149 Examine forecast residuals Seasonal Naïve and Seasonal Naïve with Drift appear similar, but the latter hugs the \\(0\\) line very closely Figure 10.11: Naive Methods Forecast Residuals (R code) 10.10.4 Smoothing Methods Run all methods Plot dynamic forecasts Moving Average underestimates Both Holt-Winters seem very good fits Exponential Smoothing a poor fit Figure 10.12: Smoothing Methods Plot (R code) Examine accuracy Both Holt-Winters are good, but Multiplicative is slightly better Lower RMSE, MAE, and MAPE Table 10.6: Smoothing Methods Accuracy (R code) .cl-27d9826e{}.cl-27cf8ec6{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-27d354f2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-27d354fc{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-27d36866{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-27d36870{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-27d36871{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-27d3687a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-27d3687b{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-27d3687c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEMAEMAPEExp.Smooth323.495263.9150.945H-W.Add54.46946.7540.168H-W.Mult43.85740.6260.146Mov.Ave225.685191.2450.686 Examine forecast residuals Both Holt-Winters hug the \\(0\\) line Figure 10.13: Smoothing Methods Forecast Residuals (R code) 10.10.5 Regression Methods Run all methods Plot dynamic forecasts Seasonal method very good forecast Trend alone not effective Figure 10.14: Regression Methods Plot (R code) Examine accuracy Seasonal method much better Lower RMSE, MAE, and MAPE Table 10.7: Regression Methods Accuracy (R code) .cl-289665be{}.cl-288be86e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-288fe28e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-288fe298{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-288ff88c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-288ff896{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-288ff897{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-288ff8a0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-288ff8a1{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-288ff8a2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEMAEMAPELin.Reg.Seas.Trend55.21247.8980.172Lin.Reg.Trend199.198168.9850.607 Examine forecast residuals Seasonal method hugs the \\(0\\) line Figure 10.15: Regression Methods Forecast Residuals (R code) 10.10.6 Compare Best Models Examine RMSE, MAE, MAPE, and Forecast Error Plot Figure 10.16: Forecase Residuals Comparison (R code) Table 10.8: Accuracy Comparison (R code) .cl-291e5e06{}.cl-29147da0{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-291851fa{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-29185204{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-291867a8{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-291867b2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-291867b3{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-291867b4{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-291867bc{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-291867bd{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEMAEMAPESeas.Naive.Drift45.76941.4170.149H-W.Add54.46946.7540.168H-W.Mult43.85740.6260.146Lin.Reg.Seas.Trend55.21247.8980.172 Holt-Winters Multiplicative does slightly better than Seasonal Naïve with Drift Both are better than the other two options 10.11 Suggested Readings Principles of Marketing Engineering and Analytics, 3rd Edition (2017). Lilien, Gary L., Rangaswamy, Arvind, and De Bruyn, Arnaud. Course reserves Chapter 5: Forecasting Forecasting: Principles and Practice, 3rd Edition (2021). Hyndman, Rob J., and Athanasopoulos, George. Online Textshttps://otexts.com/fpp3/ 10.12 R Code Figure 10.1 load(&quot;qsales.rdata&quot;) # Load Data tsplot(data=qsales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype=&quot;yq&quot;, # Date type h=8) # Number of holdout periods Figure 10.2 # Save results to object results &lt;- naivefc(qsales, # Data frame &quot;t&quot;, # Date variable &quot;sales&quot;, # Measure variable &quot;yq&quot;, # Date type 8) # Number of holdout periods results$plot # Request plot Figure 10.3 results$fcresplot # Request forecast residual plot from previous saved results Figure 10.4 results2 &lt;- smoothfc(qsales, &quot;t&quot;, &quot;sales&quot;, &quot;yq&quot;, 8) # Save results to object results2$plot # Request plot Figure 10.5 results2$fcresplot # Request forecast residual plot from previous saved results Figure 10.6 results3 &lt;- linregfc(qsales, &quot;t&quot;, &quot;sales&quot;, &quot;yq&quot;, 8) # Save results to object results3$plot # Request plot Figure 10.7 results3$fcresplot # Request forecast residual plot from previous saved results Figure 10.8 resultslist &lt;- list(results, results2, results3) # Create list of result objects models &lt;- c(&quot;Naive&quot;, &quot;H-W.Add&quot;, &quot;Lin.Reg.Trend&quot;) # Create object with requested models compare &lt;- fccompare(resultslist, # List of results created above models) # Models we want to compare compare$fcresplot # Request plot Figure 10.9 load(&quot;msales.rdata&quot;) # Load Data tsplot(data=msales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype=&quot;ym&quot;, # Date type h=12) # Number of holdout periods Figure 10.10 # Save results to object na.results &lt;- naivefc(msales, &quot;t&quot;, &quot;sales&quot;, &quot;ym&quot;, 12) na.results$plot # Request plot Figure 10.11 na.results$fcresplot # Request forecast residual plot from previous saved results Figure 10.12 sm.results &lt;- smoothfc(msales, &quot;t&quot;, &quot;sales&quot;, &quot;ym&quot;, 12) # Save results to object sm.results$plot # Request plot Figure 10.13 sm.results$fcresplot # Request forecast residual plot from previous saved results Figure 10.14 lr.results &lt;- linregfc(msales, &quot;t&quot;, &quot;sales&quot;, &quot;ym&quot;, 12) # Save results to object lr.results$plot # Request plot Figure 10.15 lr.results$fcresplot # Request forecast residual plot from previous saved results Figure 10.16 resultslist &lt;- list(na.results, sm.results, lr.results) # Create list of result objects models &lt;- c(&quot;Seas.Naive.Drift&quot;, &quot;H-W.Add&quot;, &quot;H-W.Mult&quot;, &quot;Lin.Reg.Seas.Trend&quot;) # Create object with requested models compare &lt;- fccompare(resultslist, # List of results created above models) # Models we want to compare compare$fcresplot # Request plot Table ?? results$acc # Request accuracy table from previous saved results Table ?? results2$acc # Request accuracy table from previous saved results Table ?? results3$acc # Request accuracy table from previous saved results Table ?? compare$acc # Request accuracy table Table ?? na.results$acc # Request accuracy table from previous saved results Table ?? sm.results$acc # Request accuracy table from previous saved results Table ?? lr.results$acc # Request accuracy table from previous saved results Table ?? compare$acc # Request accuracy table "],["conjoint-analysis-not-covered.html", "Topic 11 Conjoint Analysis (Not Covered) 11.1 R Packages and Datasets for Topic 11 11.2 Why Conjoint? 11.3 How Conjoint Analysis Works 11.4 Part-Worth Utilities and Importances 11.5 Traditional Conjoint Example 11.6 Suggested Readings 11.7 R Code", " Topic 11 Conjoint Analysis (Not Covered) 11.1 R Packages and Datasets for Topic 11 library(ggplot2) # Advanced graphing capabilities library(dplyr) # Easier programming data(airlineca) 11.2 Why Conjoint? 11.2.1 Buyers versus Sellers Buyers want to Maximize Utility Most desirable features Lowest possible price Sellers want to Maximize Profits Lower costs Greater value 11.2.2 Breaking Down Products Buyers value products based on the sum of their parts If we can learn how buyers value components, we can design better positioned products, and improve profitability 11.2.3 Learning What Customers Want Ask direct questions about preference Unfortunately, responses are often unenlightening No information about sensitivities within ranges Ask direct questions about importance Typically results if everything being important Problem of low discrimination 11.2.4 How is Conjoint Analysis Different? Present consumers with realistic tradeoffs For example: Which would you prefer in your next automobile? 210 Horsepower, 18 MPG 140 Horsepower, 28 MPG If “a” is selected, we infer that they prefer power If “b” is selected, we infer that they prefer fuel economy Doesn’t ask directly about preferences or importance Preferences and importance inferred from tradeoffs made 11.2.5 Why Does Conjoint Analysis Work? Consumers: Make tradeoffs similar to those in the real market Are discouraged from saying all features are equally desirable Must make difficult tradeoffs, which provides more valuable info 11.2.6 Practical Guidelines Should be able to specify product as a bundle of attributes Need to know the determinant attributes Respondents should be familiar with category and attributes Firm should be able to act on results 11.3 How Conjoint Analysis Works Break down product into component parts or features Vary features to “build” many product concepts Ask respondents to rank/rate or choose between concepts Use statistics to derive each feature’s unique value \\(\\leftarrow\\) Marketing analytics 11.3.1 Rules for Creating the Attribute List Each attribute has varying levels EXAMPLE Attribute: BrandLevels: Apple; Samsung; Moto Attribute: Screen SizeLevels: 5”; 5.5”; 6” Attribute: BatteryLevels: 12 hrs.; 18 hrs.; 24 hrs. Attribute: PriceLevels: $400; $700; $1000 Attribute: StorageLevels: 32GB; 64GB; 128GB Try to balance number of levels across attributes Reduces the number of levels bias: Holding all else constant, attributes with more levels than others will be biased upwards in importance EXAMPLE A PRICE attribute with levels of $10, $12, $14,$16, $18, $20 will have a higher relative importance compared to other attributes than if the PRICE attribute levels were $10, $15, $20 Attribute levels should be mutually exclusive EXAMPLE Attribute: AccessoriesLevels: Screen protector; Phone case; Protection plan We could not determine the value of providing these features at the same time 11.3.2 Build Concepts Suppose three attributes, each with three levels There would be \\(3\\times 3\\times 3=27\\) different profiles 11.3.3 Rate/Rank/Choose Concepts EXAMPLE Rate: On a scale of 1 to 100, how likely would you be to purchase the following product combination? Rank: Please rate the following combinations from most likely to least likely to purchase. Choose: Which of the following product combinations would you choose? Traditional Conjoint Rate or rank profiles Good for small number of attributes Can work for small samples Adaptive Conjoint Rate attributes and choose profiles Good for high number of attributes Bad for pricing research Choice-Based Conjoint Choose profiles Best mirrors real-world purchase Requires larger sample sizes 11.3.4 Use Statistics to Derive Unique Value Traditional Conjoint Uses regression-based methods Adaptive Conjoint Use hierarchical Bayesian methods Choice-Based Conjoint Uses multinomial choice models with hierarchical Bayesian methods QUESTION: What type of data output do we get from conjoint analysis? ANSWER: Part-worth utilities Numerical values that reflect how desirable different features are How consumers value each level of each attribute Can be created for each respondent 11.4 Part-Worth Utilities and Importances 11.4.1 Characteristics of Part-Worth Utilities EXAMPLE: Suppose we studied burritos and used two attributes: Price and Meat We might end up with the following part-worth utilities for a single respondent Price Level Utility $6 48 $8 32 $10 24 Meat Level Utility Chicken 29 Steak 13 Higher is more preferred EXAMPLE: $6 preferred to $8 $8 preferred to $10 Chicken preferred to Steak Can be positive or negative Higher is still more preferred Estimated on an interval scale Addition/subtraction allowed Multiplication/division not allowed EXAMPLE: Can’t say $6 is “twice as preferred” as $10 CANNOT compare level from one attribute with level from another attribute EXAMPLE: Can’t say $6 is more preferred than Chicken CAN compare differences between levels of one attribute with levels of another attribute EXAMPLE: Preference difference between $6 and $8 is same as between Chicken and Steak Total utility for a profile is additive EXAMPLE: \\(3\\times 2=6\\) product combinations with the following utilities: $6 – Chicken: 48 + 29 = 77 $8 – Chicken: 32 + 29 = 61 $6 – Steak: 48 + 13 = 61 $10 – Chicken: 24 + 29 = 53 $8 – Steak: 32 + 13 = 45 $10 – Steak: 24 + 13 = 37 11.4.2 Calculating Part-Worth Utilities (traditional conjoint) Each respondent provided ratings on many product profiles For each respondent, run a linear regression: \\(profilerating_i=\\alpha+\\beta_kattr_{ik}\\) Works if: profiles rated &gt; attribute levels in the regression equation Regression coefficients become the part-worth utilities Not concerned with significance 11.4.3 From Utilities to Importances Part-worth utilities DO… directly provide a measure of preference within attributes DON’T… directly provide a measure of each attributes importance Conjoint Importances Measures how much influence each attribute has on choice Considers the difference each attribute could make in total utility 11.4.4 Calculating Conjoint Importances Calculate utility range for each attribute Sum up ranges Calculate percent of “total range” each attribute contributes EXAMPLE: Suppose we add a third attribute to our burrito example: Guacamole We might end up with the following part-worth utilities for a single respondent Price Level Utility $6 48 $8 32 $10 24 Meat Level Utility Chicken 29 Steak 13 Guacamole Level Utility Yes 47 No 15 Step 1: Calculate range for each attirbute Price: $6 (high) - $10 (low) = 48 - 24 = 24 Meat: Chicken (high) - Steak (low) = 29 - 13 = 16 Guacamole: Yes (high) - No (low) = 47 - 15 = 24 Step 2: Sum up the ranges 24 + 16 + 32 = 72 Step 3: Calculate percent of “total range” each attribute contributes Price: 24/72 = 33.4% Meat: 16/72 = 22.2% Guacamole: 32/72 = 44.4% 11.4.5 Interpreting Conjoint Importances Higher is more important Guac is most important, followed by Price, then Meat Ratio scaled Can say that Guac is twice as important as Meat Computing average importances Compute for each respondent and then average 11.4.6 Further Analysis If respondent characteristics are available, further analysis can be performed to understand relationships between characteristics and part-worths and importances Correlation analysis between part-worths/importances and continuous respondent characteristics Regression analysis with importances as the DV and characteristics as the IVs 11.4.7 Prediction Can predict value of different profiles to see which is preferred Can be done overall or on a case-by-case basis 11.5 Traditional Conjoint Example 11.5.1 Data Conjoint ratings from 40 people (\\(caseid\\)) on nine bundles Value (\\(value\\)): (0 = Not at all; 100 = Very much) Attributes: \\(airline\\) 1: Delta 2: Spirit 3: SW \\(connect\\) 1: None (No connections) 2: One (One connection) $price 1: $300 2: $450 3: $600 Goal: Generate individual and average part-worths and importances 11.5.2 Summarize Average value ratings by attribute levels Table 11.1: Average Value Ratings by Attribute Levels (R code) .cl-29a91406{}.cl-299f48fe{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-29a30b88{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-29a30b92{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-29a31ee8{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29a31ef2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29a31ef3{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29a31ef4{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29a31efc{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29a31f06{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}airlinemean2: Spirit39.3001: Delta52.3753: SW50.900 .cl-29bad678{}.cl-29b0ef6e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-29b4f762{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-29b4f776{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-29b50d4c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29b50d56{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29b50d60{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29b50d61{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29b50d62{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29b50d6a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}connectmean2: One45.00001: None48.7875 .cl-29ce3dda{}.cl-29c49618{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-29c84f7e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-29c84f88{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-29c86a7c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29c86a86{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29c86a87{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29c86a88{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29c86a90{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29c86a91{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}pricemean3: $60020.2502: $45047.2751: $30075.050 11.5.3 Regression results for one respondent For \\(caseid=13\\) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.6666667 1.993818 0.3343669 0.76011403398 airline1: Delta 16.6666667 1.845916 9.0289390 0.00286884539 airline3: SW 13.3333333 1.845916 7.2231512 0.00547151485 connect1: None 5.0000000 1.598611 3.1277162 0.05215791724 price2: $450 47.0000000 1.845916 25.4616079 0.00013286386 price1: $300 81.0000000 1.845916 43.8806434 0.00002605196 Part-Worth Utilities Airline SW = 13.33Delta = 16.67Spirit = 0 Connections None = 5One = 0 Price $300 = 81$450 = 47$600 = 0 11.5.4 Importances for one respondent Sum of ranges: \\(16.67+5+81=102.37\\) Airline Importance: \\(\\frac{16.67}{102.37}=16.28\\%\\) Connections Importance: \\(\\frac{5}{102.37}=4.88\\%\\) Price Importance: \\(\\frac{81}{102.37}=79.12\\%\\) 11.5.5 Average Part-Worth Utilities and Importances Figure 11.1: Part-Worth Plots (R code) Figure 11.2: Importance Plot (R code) 11.5.6 Explain Importances by Other Variables Table 11.2: Explanation by Other Variables (R code) Correlation Matrix for airline_ inc airline_1: Delta 0.36* airline_2: Spirit 0.17 airline_3: SW 0.36* Correlation Matrix for connect_ inc connect_1: None -0.01 connect_2: One 0.20 Correlation Matrix for price_ inc price_1: $300 -0.35* price_2: $450 -0.17 price_3: $600 -0.09 Correlation Matrix for Imp inc Imp_airline 0.47* Imp_connect 0.17 Imp_price -0.48* Regression Results for Imp_airline Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 10.1131 7.5592 1.3379 0.1891 inc 0.3325 0.0946 3.5161 0.0012 typePleasure -9.4936 5.3744 -1.7664 0.0856 Regression Results for Imp_connect Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 10.2144 5.3877 1.8959 0.0658 inc 0.0787 0.0674 1.1672 0.2506 typePleasure -5.0497 3.8305 -1.3183 0.1955 Regression Results for Imp_price Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 79.6701 8.7485 9.1068 0.0000 inc -0.4111 0.1094 -3.7567 0.0006 typePleasure 14.5439 6.2200 2.3382 0.0249 \\(income\\) has significant positive correlations with Delta and SW part-worths \\(income\\) has a significant negative correlation with $300 price \\(income\\) has a significant positive correlation with the importance of \\(airline\\) and a significant negative correlation with the importance of \\(price\\) For a \\(\\$1000\\) increase in \\(income\\), we would expect a \\(.33\\%\\) increase in the importance of \\(airline\\) For a \\(\\$1000\\) increase in \\(income\\), we would expect a \\(.41\\%\\) decrease in the importance of \\(price\\) We would expect the importance of \\(price\\) to be \\(14.5\\%\\) greater for Pleasure travelers than for Business travelers 11.5.7 Predict Part-Worths for New Profiles Table 11.3: Profile Predictions (R code) Profile 1 = airline_1: Delta / connect_1: None / price_2: $450 Mean Utility = 57.195 95% CI = (48.119,66.271) Profile 2 = airline_2: Spirit / connect_1: None / price_1: $300 Mean Utility = 71.895 95% CI = (61.793,81.996) On average, would prefer a cheaper flight on Spirit versus a more expensive flight on Delta Although the difference is not significant (\\(95\\%\\text{ CIs}\\) overlap) 11.6 Suggested Readings Multivariate Data Analysis. Hair, Joseph F.; Black, William C.; Babin, Barry J.; Anderson, Rolph E. 7th Edition: Search for “multivariate data analysis 7th edition hair” Chapter 7: Conjoint Analysis 5th Edition: Course reserves Chapter 7: Conjoint Analysis Techincal papers on general conjoint analysis https://sawtoothsoftware.com/resources/technical-papers/categories/general-conjoint-analysis 11.7 R Code Figure 11.1 caform &lt;- value ~ airline + connect + price # Set formula results &lt;- tradca(formula=caform, # Pass formula data=airlineca, # Pass data set idvar=&quot;caseid&quot;) # Pass ID variable results$pwplot # Request plot Figure 11.2 results$impplot # Request importance plot from previous results Table 11.1 airlineca %&gt;% group_by(airline) %&gt;% summarise(mean=mean(value)) airlineca %&gt;% group_by(connect) %&gt;% summarise(mean=mean(value)) airlineca %&gt;% group_by(price) %&gt;% summarise(mean=mean(value)) Table 11.2 # Create new dataframe with demographic variables attached to case-level # part-worths and importances demos &lt;- airlineca %&gt;% group_by(caseid) %&gt;% summarise(inc=first(inc), type=first(type)) %&gt;% bind_cols(.,results$casetable[,2:12]) cademo(caform, # Pass formula demos, # Pass demos dataframe c(&quot;inc&quot;, &quot;type&quot;)) # Provide demographic variable names Table 11.3 prof1 &lt;- c(&quot;airline_1&quot;, &quot;connect_1&quot;, &quot;price_2&quot;) # Set first profile prof2 &lt;- c(&quot;airline_2&quot;, &quot;connect_1&quot;, &quot;price_1&quot;) # Set second profile capred(caform, # Pass formula results$casetable, # Pass case-by-case part-worths and importances prof1, # Pass profile 1 prof2) # Pass Profile 2 "],["forecasting-ii-not-covered.html", "Topic 12 Forecasting II (NOT COVERED) 12.1 R Packages and Datasets for Topic 12 12.2 Forecasting Methods 12.3 The Box-Jenkins Method 12.4 Box-Jenkins Method Overview 12.5 ARIMA Forecasting Example 1 12.6 ARIMA Forecasting Example 2 12.7 Suggested Readings 12.8 R Code", " Topic 12 Forecasting II (NOT COVERED) 12.1 R Packages and Datasets for Topic 12 library(ggplot2) # Advanced graphing capabilities library(dplyr) # Easier programming library(fpp3) library(slider) library(ggfortify) load(&quot;Topic12/monthlysales.rdata&quot;) 12.2 Forecasting Methods Judgmental Sales force estimates Jury of experts Structured processes Market/Product Analysis Surveys Market tests ATAR Time Series Naïve methods Smoothing techniques Box-Jenkins methods Regression/Econometric Linear Non-Linear 12.3 The Box-Jenkins Method An overall philosophy or method of identifying, fitting, and checking Provides adequate model for most any data pattern Appropriate for medium to long length (at least 50 observations) 12.3.1 Stationary Time Series Box-Jenkins requires the time series to be stationary Properties of the time series do not depend on the time at which the series is observed Why do we need the stationary assumption? To make statements about autocorrelation What does this plot tell us? Seasonality (12 months peak to peak) Trend (increasing sales over time) Heteroscedasticity (increasing variance over time) 12.3.2 Autocorrelation Correlation is a measure of the linear relationship between two variables \\(\\begin{aligned} r=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})\\times (y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}} \\end{aligned}\\) Autocorrelation is a measure of the linear relationship between lagged values of a time series \\(\\begin{aligned} r_k=\\frac{\\sum_{t=k+1}^{T}(y_t-\\bar{y})\\times (y_{t-k}-\\bar{y})}{\\sum_{t=1}^{T}(y_t-\\bar{y})^2} \\end{aligned}\\) \\(r_1\\) measures the relationship between \\(y_t\\) and \\(y_{t-1}\\) \\(r_2\\) measures the relationship between \\(y_t\\) and \\(y_{t-2}\\) 12.3.3 Autocorrelation Function Autocorrelation used to determining the right model for the series Specifically, use the autocorrelation function Graphs autocorrelation for successive lag values 12.3.4 Partial Autocorrelation Function Partial autocorrelation can also be used to determining the right model for the series PACF removes the influence of intermediate lags E.G., PACF of lag three is the correlation between \\(y_t\\) and \\(y_{t-3}\\) after removing the influence of \\(y_{t-1}\\) and \\(y_{t-2}\\) 12.3.5 Forecasting ARMA \\(AR(p)\\): AutoregressiveVariable of interest is a linear combination of the past \\(p\\) values of the variable\\(y_t=c+\\phi_1 y_{t-1}+\\phi_2 y_{t-2}+\\dots+\\phi_p y_{t-p}+\\varepsilon_t\\) A series displays “autoregressive” behavior if it tends to get pulled back toward its mean. The larger the sum of the \\(AR\\) coefficients, the more slowly the series returns to its mean. \\(MA(q)\\): Moving AverageVariable of interest is a weighted linear combination of the past \\(q\\) forecast errors\\(y_t=c+\\varepsilon_t+\\theta_1 \\varepsilon_{t-1}+\\theta_2 \\varepsilon_{t-2}+\\dots+\\theta_q \\varepsilon_{t-q}\\) A series displays “moving average” behavior if it seems to have random shocks felt in later periods. The \\(MA(q)\\) coefficients represent the fraction of shock from \\(q\\) periods ago that is still being felt. \\(ARMA(p,q)\\): \\(y_t=c+\\phi_1 y_{t-1}+\\dots+\\phi_p y_{t-p}+\\theta_1 \\varepsilon_{t-1}+\\dots+\\theta_q \\varepsilon_{t-q}\\varepsilon_t+\\varepsilon_t\\) 12.3.6 Nonstationary Time Series Many time series exhibit nonstationary behavior Box-Jenkins methodology is for stationary series… Must make an adjustment before modeling Differencing Can take care of nonstationarity Instead of \\(y_t=\\text{...}\\)Use \\(y_t^\\prime=\\text{...}\\)where \\(y_t^\\prime=y_t - y_{t-1}\\) 12.3.7 Forecasting ARIMA \\(I(d)\\): IntegratedRaw observations of the variable of interest are differenced \\(d\\) times to make the series stationary \\(ARMA(p,q)\\) * \\(y_t=c+\\phi_1 y_{t-1}+\\dots+\\phi_p y_{t-p}+\\theta_1 \\varepsilon_{t-1}+\\dots+\\theta_q \\varepsilon_{t-q}\\varepsilon_t+\\varepsilon_t\\) \\(ARIMA(p,d,q)\\) * \\(y_t=c+\\phi_1 y_{t-1}^\\prime+\\dots+\\phi_p y_{t-p}^\\prime +\\theta_1 \\varepsilon_{t-1}+\\dots+\\theta_q \\varepsilon_{t-q}\\varepsilon_t+\\varepsilon_t\\)where \\(y_t^\\prime\\) is the differenced series 12.3.8 ARIMA with Seasonal Time Series Many time series exhibit seasonal fluctuations Box-Jenkins methods can incorporate seasonality by including additional seasonal terms \\(ARIMA(p,d,q)\\) becomes \\(ARIMA(p,d,q)(P,D,Q)_S\\), where: \\((p,d,q)\\) is the non-seasonal part of the model \\((P,D,Q)_S\\) is the seasonal part of the model \\(S\\) is th enumber of observations per overall period E.G., \\(S=12\\) for monthly data, \\(S=4\\) for quarterly data Differencing with seasonal data Original Data Non-Seasonal Difference Data Seasonal and Non-seasonal Differenced Data 12.4 Box-Jenkins Method Overview IdentificationUsing plots, ACFs, PACFs, and other information, a class of simple ARIMA models is selected EstimationEstimate the models using appropriate methods DiagnosticsCheck fitted models for inadequacies 12.4.1 ARIMA 12.4.1.1 Model Identification Start by identifying the order of differencing If the ACF decays very slowly, it is likely a nonstationary series Try a first order differencing to make the series stationary If autocorrelations die out quickly, the appropriate value of \\(d\\) has been found Be careful not to overdifference the series Next, identify the \\(p\\) and/or \\(q\\) terms For \\(p\\): Use PACF Identify how many significant lags before it “cuts off” The last significant lag is the estimated value of \\(p\\) In this example, two significant lags For \\(p\\): Run model with estimated \\(p\\) term Generate residuals from predicted values Plot ACF and PACF residuals to see if significant auto-correlations still exist * Use white noise test to see if the data come from a &quot;white noise&quot; process (i.e., uncorrelated with constant mean and variance) * $H_0$ is white noise For \\(q\\): Use ACF If ACF has a sharp cutoff, consider the last lag before the cutoff for \\(q\\) If ACF has negative autocorrelation at lag 1, consider 1 for \\(q\\) In most cases, the best model is usually one with ONLY \\(AR\\) terms or ONLY \\(MA\\) terms Mixed models (i.e., both \\(AR\\) and \\(MA\\) terms) may be the best fitting model, but caution should be used E.g.; \\(ARIMA(2,1,0)\\) vs. \\(ARIMA(2,2,1)\\) 12.4.1.2 Model Estimation Estimate model with maximum likelihood estimation Finds values of parameters that maximize probability of obtaining observed data Similar to least squares by minimizing squared residuals Manual way to select final model Run potential model using previous guidelines Vary \\(p\\) and \\(q\\) from that model \\(\\pm1\\), and run models Compare AIC values to see which is lowest Automatic selection Many programs/packages have automatic searching procedures 12.4.1.3 Diagnostic Checking Once final model is selected, ensure that the autocorrelation plots of the residuals represent “white noise” I.e., no further structure exists If further structure exists: Check other potential models to eliminate the additional structure If further structure doesn’t exist: Check forecast accuracy on holdout periods 12.4.2 Seasonal ARIMA 12.4.2.1 Model Identification Start by identifying the order of differencing If the data is clearly seasonal, examine the ACF plot from a first order seasonal difference, \\(D\\) If the ACF shows a slow decay, try a first order difference also If autocorrelations die out quickly, the appropriate value of \\(d\\) has been found Identify the \\(P\\) and/or \\(Q\\) terms Follow same general procedure as used for \\(p\\) and \\(q\\) If positive ACF of the differenced series at lag \\(s\\), consider a \\(P\\) term More likely to occur if seasonal difference not used If negative ACF of the differenced series at lag \\(s\\), consider a \\(Q\\) term More likely to occur if seasonal difference is used Try to avoid more than one or two seasonal parameters E.g. \\(Q=2\\) is two parameters, as is \\(P=1\\) and \\(Q=1\\) 12.5 ARIMA Forecasting Example 1 12.5.1 Data Quarterly sales data from 1990 through 2012 Variables: Time period (\\(t\\)) Sales from 1990 through 2010 (\\(sales\\)) Goal: Forecast quarterly sales for the next two years 12.5.2 Examine Data Figure 12.1: Time Series Plot (R code) Trend? Yes Seasonlity? No Models? ARIMA 12.5.3 Deterime \\(d\\) ACF of non-differenced sales shows very slow decay Figure 12.2: ACF of Non-Differenced Sales (R code) Try \\(d=1\\) ACF of first-differenced sales shows no significant lags early on Use \\(d=1\\) in modeling Figure 12.3: ACF of First-Order Differenced Sales (R code) 12.5.4 Determine \\(p\\) PACF of first-differenced sales shows some significant lags, but appear random \\(p=0\\) probably appropriate Figure 12.4: PACF of First=Order Differenced Sales (R code) 12.5.5 Determine \\(q\\) ACF of first-differenced sales shows: No sharp cutoff No significant negative correlation at lag 1 \\(q=0\\) probably appropriate Figure 12.5: ACF of First-Order Differenced Sales (R code) 12.5.6 Check White Noise Run Model \\(ARIMA(0,1,0)\\) and check white noise test Test results suggest model would be appropriate Figure 12.6: White Noise Test for ARIMA(0,1,0) (R code) 12.5.7 Model Selection Use \\(ARIMA(0,1,0)\\) as “starting” model Use automatic selection algorithm to fine “best” model Model automatically selected — \\(ARIMA(1,1,2)\\) — is considerably better than “starting model” Lower forecast accuracy measures and lower AIC Table 12.1: ARIMA Model Accuracy (R code) .cl-44388bc6{}.cl-442e0124{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-4432128c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-44321296{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-443228b2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-443228bc{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-443228bd{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-443228be{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-443228c6{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-443228c7{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEMAEMAPEAICcAuto: A(1,1,2)44.37136.3910.472630.896Self: A(0,1,0)52.60644.5150.577634.913 Check diagnostics for \\(ARIMA(1,1,2)\\) (i.e., “Auto”) ACF and PACF look good with no worrisome significant correlations White noise test looks good Figure 12.7: ACF/PACF of Model Residuals (R code) Figure 12.8: White Noise Test of Models (R code) 12.5.8 Model Forecast Use dynamic forecast for \\(ARIMA(1,1,2)\\) Compare with Holt-Winters Additive Holt-Winters Additive predicting better Table 12.2: ARIMA Forecast Residual Comparison with Holt-Winters Additive (R code) .cl-4ba59962{}.cl-4b9ba434{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-4b9fb178{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-4b9fb182{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-4b9fc604{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4b9fc60e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4b9fc60f{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4b9fc610{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4b9fc618{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4b9fc619{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEMAEMAPEAuto: A(1,1,2)44.37136.3910.472H-W.Add29.72124.6600.320 Figure 12.9: ARIMA Forecast Comparison with Holt-Winters Additive (R code) 12.6 ARIMA Forecasting Example 2 12.6.1 Data Monthly Sales from 2005 through 2016 Variables: Time period (\\(t\\)) Sales per period (\\(sales\\)) Goal: Forecast quarterly sales for the next 12 months 12.6.2 Examine Data Figure 12.10: Time Series Plot (R code) Trend? Yes Seasonlity? Yes Models? Seasonal ARIMA 12.6.3 Deterime \\(d\\) and \\(D\\) ACF of non-differenced sales shows very slow decay Try \\(d=1\\) Wave pattern suggest seasonality Figure 12.11: ACF of Non-Differenced Sales (R code) Try \\(d=1\\) ACF of first-differenced sales still shows significant correlations at intervals of 12 lags Try \\(D=1\\) Figure 12.12: ACF of First-Order Non-Seasonal Differenced Sales (R code) ACF of first-differenced non-seasonal and seasonal sales Sharp cutoff suggests trend and seasonality accounted for Figure 12.13: ACF of First-Order Non Seasonal and Seasonal Differenced Sales (R code) 12.6.4 Determine \\(p\\) PACF of differenced sales shows cutoff afer 1 lag Begin with \\(p=1\\) Figure 12.14: PACF of Differenced Sales (R code) 12.6.5 Determine \\(q\\) ACF of differenced sales shows negative correlation at first lag Begin with \\(q=1\\) Figure 12.15: ACF of Differenced Sales (R code) 12.6.6 Determine \\(P\\) and \\(Q\\) ACF of differenced sales shows negative correlation at lage \\(s\\) Begin with \\(Q=1\\) No \\(P\\) term 12.6.7 Check White Noise Run Model \\(ARIMA(1,1,1)(0,1,1)_{12}\\) and check white noise test Test results suggest model would be appropriate Figure 12.16: White Noise Test for ARIMA(1,1,1)(0,1,1)12 (R code) 12.6.8 Model Selection Use \\(ARIMA(1,1,1)(0,1,1)_{12}\\) as “starting” model Use automatic selection algorithm to fine “best” model Model automatically selected — \\(ARIMA(1,0,1)(0,1,1)_{12})\\) — is not better than the starting model Two models measures are fairly equal Table 12.3: ARIMA Model Accuracy (R code) .cl-9c3c4bd2{}.cl-9c322c4c{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9c35f868{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9c35f869{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9c360da8{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c360da9{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c360db2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c360db3{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c360dbc{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c360dbd{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEMAEMAPEAICcAuto: A(1,0,1)S(0,1,1)[12]50.53145.3930.1631,347.42Self: A(1,1,1)S(0,1,1)[12]47.65841.8610.1501,338.05 Check diagnostics for both models\\(ARIMA(1,1,2)\\) (i.e., “Auto”) ACF and PACF look good for both models White noise test looks good for both models Figure 12.17: ACF/PACF of Model Residuals (R code) Figure 12.18: White Noise Test of Models (R code) 12.6.9 Model Forecast Use dynamic forecast for both models Compare with Linear Regression with Seasonal Trend All three models perform about the same Table 12.4: ARIMA Forecast Residual Comparison with Linear Regression Seasonal Trend (R code) .cl-a3c466d2{}.cl-a3ba3342{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a3be30f0{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a3be3104{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a3be464e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a3be4658{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a3be4659{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a3be465a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a3be4662{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a3be4663{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEMAEMAPEAuto: A(1,0,1)S(0,1,1)[12]50.53145.3930.163Self: A(1,1,1)S(0,1,1)[12]47.65841.8610.150Lin.Reg.Seas.Trend55.21247.8980.172 Figure 12.19: ARIMA Forecast Comparison with Linear Regression Seasonal Trend (R code) 12.7 Suggested Readings Principles of Marketing Engineering and Analytics, 3rd Edition (2017). Lilien, Gary L., Rangaswamy, Arvind, and De Bruyn, Arnaud. Course reserves Chapter 5: Forecasting Forecasting: Principles and Practice, 3rd Edition (2021). Hyndman, Rob J., and Athanasopoulos, George. Online Textshttps://otexts.com/fpp3/ 12.8 R Code Figure 12.1 load(&quot;Topic12/qsales.rdata&quot;) # Load Data tsplot(data=qsales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype=&quot;yq&quot;, # Date type h=8) # Number of holdout periods Figure 12.2 acplots(data=qsales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;yq&quot;, # Date type quarterly h=8, # Number of holdout periods lags=25, # Number of lags to show d=0, # No differencing D=0)$acf # No seasonal differencing; request ACF plot Figure 12.3 acplots(data=qsales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;yq&quot;, # Date type quarterly h=8, # Number of holdout periods lags=25, # Number of lags to show d=1, # 1st order difference D=0)$acf # No seasonal differencing; request ACF plot Figure 12.4 acplots(data=qsales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;yq&quot;, # Date type quarterly h=8, # Number of holdout periods lags=25, # Number of lags to show d=1, # 1st order difference D=0)$pacf # No seasonal differencing; request PACF plot Figure 12.5 acplots(data=qsales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;yq&quot;, # Date type quarterly h=8, # Number of holdout periods lags=25, # Number of lags to show d=1, # 1st order difference D=0)$acf # No seasonal differencing; request ACF plot Figure 12.6 tswh.noise(data=qsales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;yq&quot;, # Date type quarterly h=8, # Number of holdout periods c(0,1,0), # ARIMA terms (p,d,q) c(0,0,0)) # SARIMA terms (P,D,Q) Figure 12.7 arima$acresid Figure 12.8 arima$wn Figure 12.9 compare$fcresplot Figure 12.10 load(&quot;Topic12/msales.rdata&quot;) # Load Data tsplot(data=msales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype=&quot;ym&quot;, # Date type h=12) # Number of holdout periods Figure 12.11 acplots(data=msales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;ym&quot;, # Date type quarterly h=12, # Number of holdout periods lags=25, # Number of lags to show d=0, # No differencing D=0)$acf # No seasonal differencing; request ACF plot Figure 12.12 acplots(data=msales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;ym&quot;, # Date type quarterly h=12, # Number of holdout periods lags=25, # Number of lags to show d=1, # 1st order difference D=0)$acf # No seasonal differencing; request ACF plot Figure 12.13 acplots(data=msales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;ym&quot;, # Date type quarterly h=12, # Number of holdout periods lags=25, # Number of lags to show d=1, # 1st order difference D=1)$acf # 1st order seasonal differencing; request ACF plot Figure 12.14 acplots(data=msales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;ym&quot;, # Date type quarterly h=12, # Number of holdout periods lags=25, # Number of lags to show d=1, # 1st order difference D=1)$pacf # 1st order seasonal differencing; request PACF plot Figure 12.15 acplots(data=msales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;ym&quot;, # Date type quarterly h=12, # Number of holdout periods lags=25, # Number of lags to show d=1, # 1st order difference D=1)$acf # 1st order seasonal differencing; request ACF plot Figure 12.16 tswh.noise(data=msales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;ym&quot;, # Date type quarterly h=12, # Number of holdout periods c(1,1,1), # ARIMA terms (p,d,q) c(0,1,1)) # SARIMA terms (P,D,Q) Figure 12.17 sarima$acresid Figure 12.18 sarima$wn Figure 12.19 compare$fcresplot Table 12.1 # Save results to object called &#39;arima&#39; arima &lt;- autoarima(data=qsales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;yq&quot;, # Date type quarterly h=8, # Number of holdout periods c(0,1,0), # ARIMA terms (p,d,q) c(0,0,0), # SARIMA terms (P,D,Q) auto=&quot;Y&quot;) # Request automatic search arima$acc # Request accuracy table Table 12.2 smooth &lt;- smoothfc(qsales, &quot;t&quot;, &quot;sales&quot;, &quot;yq&quot;, 8) results &lt;- list(arima, smooth) models &lt;- c(&quot;Auto&quot;, &quot;H-W.Add&quot;) compare &lt;- fccompare(results, models) compare$acc Table 12.3 # Save results to object called &#39;sarima&#39; sarima &lt;- autoarima(data=msales, # Data frame tvar=&quot;t&quot;, # Date variable obs=&quot;sales&quot;, # Measure variable datetype = &quot;ym&quot;, # Date type quarterly h=12, # Number of holdout periods c(1,1,1), # ARIMA terms (p,d,q) c(0,1,1), # SARIMA terms (P,D,Q) auto=&quot;Y&quot;) # Request automatic search sarima$acc # Request accuracy table Table 12.4 linreg &lt;- linregfc(msales, &quot;t&quot;, &quot;sales&quot;, &quot;ym&quot;, 12) results &lt;- list(sarima, linreg) models &lt;- c(&quot;Auto&quot;, &quot;Self&quot;, &quot;Lin.Reg.Seas.Trend&quot;) compare &lt;- fccompare(results, models) compare$acc "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
